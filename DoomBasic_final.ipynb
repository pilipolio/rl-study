{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import ppaquette_gym_doom\n",
    "from ppaquette_gym_doom.wrappers import SetResolution, ToDiscrete\n",
    "from gym.wrappers import SkipWrapper\n",
    "from gym import wrappers\n",
    "\n",
    "# (see https://github.com/ppaquette/gym-doom/blob/master/ppaquette_gym_doom/doom_basic.py)\n",
    "def create_env(seed=None):\n",
    "    env_spec = gym.spec('ppaquette/DoomBasic-v0')\n",
    "    env_spec.id = 'DoomBasic-v0'\n",
    "    env = env_spec.make()\n",
    "\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "\n",
    "    return SetResolution('200x150')(\n",
    "        SkipWrapper(repeat_count=4)(\n",
    "        ToDiscrete('minimal')(env)))\n",
    "\n",
    "env = create_env()\n",
    "WIDTH, HEIGHT = env.screen_width, env.screen_height\n",
    "\n",
    "NOOP, SHOOT, RIGHT, LEFT = 0, 1, 2, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import operator\n",
    "\n",
    "SARE = namedtuple('SAR', ['state', 'action', 'reward', 'end'])\n",
    "\n",
    "\n",
    "def generate_sares(env, agent, episode_count=100):\n",
    "    reward = 0\n",
    "    done = False\n",
    "\n",
    "    for i in range(episode_count):\n",
    "        observation = env.reset()\n",
    "        while True:\n",
    "            action = agent.act(observation, reward, done)\n",
    "            new_observation, reward, done, _ = env.step(action)\n",
    "            yield SARE(observation, action, reward, done)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            else:\n",
    "                observation = new_observation\n",
    "\n",
    "def episode_sares(env, agent, episode_count=100):\n",
    "    sares = list(generate_sares(env, agent, episode_count))\n",
    "    print('average reward per episode = {}'.format(\n",
    "        sum(r for _, _, r, _ in sares) / float(sum(e for _, _, _, e in sares))))\n",
    "    return sares\n",
    "\n",
    "        \n",
    "def to_experiences(sares, only_n_misses=100):\n",
    "    experiences = [\n",
    "        (previous_s, a, r, next_s, end)\n",
    "        for (previous_s, a, r, end), (next_s, _, _, _) in zip(sares[:-1], sares[1:])\n",
    "    ]\n",
    "\n",
    "    # simplistic experience prioritization\n",
    "    shuffled_exps = experiences if only_n_misses is None\\\n",
    "        else random.choices(experiences, k=only_n_misses) + [e for e in experiences if e[2] > 0]\n",
    "    random.shuffle(shuffled_exps)\n",
    "\n",
    "    prev_frames, actions, rewards, next_frames, is_ends = zip(*shuffled_exps)\n",
    "    prev_frames = np.asarray(prev_frames)\n",
    "    next_frames = np.asarray(next_frames)\n",
    "    actions = np.asarray(actions)\n",
    "    rewards = np.asarray(rewards)\n",
    "    is_ends = np.asarray(is_ends)\n",
    "    \n",
    "    print('Training on {}/{} positive/total out of {} 1-step experiences with actions distribution {}'.format(\n",
    "        np.sum(rewards>=0),\n",
    "        len(rewards),\n",
    "        len(experiences),\n",
    "        np.bincount(actions)))\n",
    "    \n",
    "    return (prev_frames, next_frames, actions, rewards, is_ends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.layers import Dense, Convolution2D, Flatten, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "K.set_session(sess)\n",
    "\n",
    "def create_q_model(conv1_weights=None, conv2_weights=None, dense1_weights=None, dense2_weights=None):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(\n",
    "        Convolution2D(nb_filter=2, nb_row=6, nb_col=6, subsample=(1, 1), border_mode='valid', weights=conv1_weights,\n",
    "            input_shape=[HEIGHT, WIDTH, 3], dim_ordering='tf'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(\n",
    "        Convolution2D(nb_filter=4, nb_row=2, nb_col=2, subsample=(1, 1), weights=conv2_weights))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, init='normal', weights=dense1_weights))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(4, init='normal', weights=dense2_weights))\n",
    "    model.compile(loss='mse', optimizer=Adam())\n",
    "    \n",
    "    return model\n",
    "\n",
    "acting_model = create_q_model()\n",
    "target_model = create_q_model()\n",
    "\n",
    "def copy_model(model):\n",
    "    conv1_weights =  [w.eval() for w in model.layers[0].weights]\n",
    "    conv2_weights = [w.eval() for w in model.layers[2].weights]\n",
    "    dense1_weights = [w.eval() for w in model.layers[5].weights]\n",
    "    dense2_weights = [w.eval() for w in model.layers[7].weights]\n",
    "    return create_q_model(conv1_weights, conv2_weights, dense1_weights, dense2_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sares_to_input_targets(model, sares, gamma = .99, reward_clip=5, only_n_misses=100):\n",
    "    \n",
    "    prev_frames, next_frames, actions, rewards, is_ends = to_experiences(sares, only_n_misses)\n",
    "    \n",
    "    n_samples = len(actions)\n",
    "    clipped_rewards = np.clip(rewards, -np.inf, reward_clip)\n",
    "    \n",
    "    # Transcription of the Q-learning target formula\n",
    "    targets = clipped_rewards + gamma * (1 - is_ends) * model.predict(next_frames).max(axis=1)\n",
    "\n",
    "    target_action_rewards = model.predict(prev_frames)\n",
    "    target_action_rewards[np.arange(n_samples), actions] = targets\n",
    "\n",
    "    return prev_frames, target_action_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 17:54:06,666] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-03-04 17:54:06,668] Clearing 26 monitor files from previous run (because force=True was provided)\n",
      "[2017-03-04 17:54:07,044] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/q_learning/openaigym.video.1.52235.video000000.mp4\n",
      "[2017-03-04 17:54:13,501] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/q_learning/openaigym.video.1.52235.video000001.mp4\n",
      "[2017-03-04 17:54:15,328] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/q_learning/openaigym.video.1.52235.video000008.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = -80.0\n",
      "Training on 13/207 positive/total out of 243 1-step experiences with actions distribution [  8 188   9   2]\n",
      "average reward per episode = -322.4\n",
      "Training on 1/201 positive/total out of 644 1-step experiences with actions distribution [ 28   3   5 165]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 17:54:46,825] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/q_learning/openaigym.video.1.52235.video000027.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = -319.4\n",
      "Training on 1/201 positive/total out of 647 1-step experiences with actions distribution [ 12   4   3 182]\n",
      "average reward per episode = -362.5\n",
      "Training on 0/200 positive/total out of 699 1-step experiences with actions distribution [172   6   5  17]\n",
      "average reward per episode = -335.9\n",
      "Training on 1/201 positive/total out of 677 1-step experiences with actions distribution [  7   7   1 186]\n",
      "average reward per episode = -357.0\n",
      "Training on 0/200 positive/total out of 699 1-step experiences with actions distribution [ 17   4   9 170]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 17:55:50,210] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/q_learning/openaigym.video.1.52235.video000064.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = -358.5\n",
      "Training on 0/200 positive/total out of 699 1-step experiences with actions distribution [163   3   8  26]\n",
      "average reward per episode = -323.4\n",
      "Training on 1/201 positive/total out of 658 1-step experiences with actions distribution [ 29   7   3 162]\n",
      "average reward per episode = -354.0\n",
      "Training on 0/200 positive/total out of 699 1-step experiences with actions distribution [102   2   8  88]\n",
      "average reward per episode = -323.4\n",
      "Training on 1/201 positive/total out of 650 1-step experiences with actions distribution [176   8   3  14]\n",
      "average reward per episode = -341.4\n",
      "Training on 0/200 positive/total out of 683 1-step experiences with actions distribution [  7   6   4 183]\n",
      "average reward per episode = -358.5\n",
      "Training on 0/200 positive/total out of 699 1-step experiences with actions distribution [ 10   4   7 179]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 17:57:32,382] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/q_learning/openaigym.video.1.52235.video000125.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = -314.3\n",
      "Training on 2/202 positive/total out of 648 1-step experiences with actions distribution [176  10   9   7]\n",
      "average reward per episode = -322.9\n",
      "Training on 2/201 positive/total out of 654 1-step experiences with actions distribution [168   6   4  23]\n",
      "average reward per episode = -326.9\n",
      "Training on 1/201 positive/total out of 652 1-step experiences with actions distribution [  9  10   4 178]\n",
      "average reward per episode = -357.5\n",
      "Training on 0/200 positive/total out of 699 1-step experiences with actions distribution [ 35   3  32 130]\n",
      "average reward per episode = -269.2\n",
      "Training on 3/203 positive/total out of 589 1-step experiences with actions distribution [102   5  46  50]\n",
      "average reward per episode = -331.3\n",
      "Training on 3/202 positive/total out of 681 1-step experiences with actions distribution [48 11 76 67]\n",
      "average reward per episode = -265.7\n",
      "Training on 3/202 positive/total out of 570 1-step experiences with actions distribution [87 10 56 49]\n",
      "average reward per episode = -223.6\n",
      "Training on 6/203 positive/total out of 479 1-step experiences with actions distribution [49 24 43 87]\n",
      "average reward per episode = -166.3\n",
      "Training on 10/205 positive/total out of 371 1-step experiences with actions distribution [64 64 36 41]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 18:00:06,719] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/q_learning/openaigym.video.1.52235.video000216.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = -320.2\n",
      "Training on 4/204 positive/total out of 606 1-step experiences with actions distribution [74 51 48 31]\n",
      "average reward per episode = -342.7\n",
      "Training on 3/203 positive/total out of 640 1-step experiences with actions distribution [18 43 57 85]\n",
      "average reward per episode = -331.8\n",
      "Training on 2/201 positive/total out of 682 1-step experiences with actions distribution [58  8 64 71]\n",
      "average reward per episode = -91.8\n",
      "Training on 14/208 positive/total out of 276 1-step experiences with actions distribution [ 10 180  15   3]\n",
      "average reward per episode = -289.3\n",
      "Training on 5/202 positive/total out of 597 1-step experiences with actions distribution [46 12 85 59]\n",
      "average reward per episode = -183.8\n",
      "Training on 6/205 positive/total out of 409 1-step experiences with actions distribution [92 40 23 50]\n",
      "average reward per episode = -232.1\n",
      "Training on 7/205 positive/total out of 443 1-step experiences with actions distribution [  8 147  18  32]\n",
      "average reward per episode = -316.4\n",
      "Training on 0/200 positive/total out of 634 1-step experiences with actions distribution [  9   6 149  36]\n",
      "average reward per episode = -329.9\n",
      "Training on 1/201 positive/total out of 671 1-step experiences with actions distribution [188   4   4   5]\n",
      "average reward per episode = -277.3\n",
      "Training on 1/201 positive/total out of 567 1-step experiences with actions distribution [  5  12 152  32]\n",
      "average reward per episode = -171.4\n",
      "Training on 7/206 positive/total out of 367 1-step experiences with actions distribution [11 92 22 81]\n",
      "average reward per episode = -232.6\n",
      "Training on 3/203 positive/total out of 516 1-step experiences with actions distribution [31 16 87 69]\n",
      "average reward per episode = -361.0\n",
      "Training on 0/200 positive/total out of 699 1-step experiences with actions distribution [145   5  36  14]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 18:03:32,704] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/q_learning/openaigym.video.1.52235.video000343.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = -139.5\n",
      "Training on 11/206 positive/total out of 335 1-step experiences with actions distribution [ 3 99 56 48]\n",
      "average reward per episode = -215.1\n",
      "Training on 3/203 positive/total out of 478 1-step experiences with actions distribution [ 2 14 90 97]\n",
      "average reward per episode = -258.4\n",
      "Training on 4/203 positive/total out of 465 1-step experiences with actions distribution [70 94 27 12]\n",
      "average reward per episode = -131.4\n",
      "Training on 10/207 positive/total out of 346 1-step experiences with actions distribution [52 74 45 36]\n",
      "average reward per episode = -347.4\n",
      "Training on 1/201 positive/total out of 688 1-step experiences with actions distribution [  8  13  62 118]\n",
      "average reward per episode = -301.8\n",
      "Training on 3/201 positive/total out of 607 1-step experiences with actions distribution [141  21  28  11]\n",
      "average reward per episode = -238.0\n",
      "Training on 5/204 positive/total out of 503 1-step experiences with actions distribution [  5  37  31 131]\n",
      "average reward per episode = -319.4\n",
      "Training on 1/201 positive/total out of 636 1-step experiences with actions distribution [  7  16 169   9]\n",
      "average reward per episode = -211.1\n",
      "Training on 6/203 positive/total out of 450 1-step experiences with actions distribution [ 26  33   3 141]\n",
      "average reward per episode = -331.3\n",
      "Training on 3/202 positive/total out of 569 1-step experiences with actions distribution [94 99  5  4]\n",
      "average reward per episode = -361.0\n",
      "Training on 0/200 positive/total out of 699 1-step experiences with actions distribution [ 12   8  28 152]\n",
      "average reward per episode = -197.5\n",
      "Training on 5/204 positive/total out of 465 1-step experiences with actions distribution [20 14 95 75]\n",
      "average reward per episode = -321.6\n",
      "Training on 5/203 positive/total out of 561 1-step experiences with actions distribution [ 60 128   7   8]\n",
      "average reward per episode = -313.3\n",
      "Training on 2/202 positive/total out of 649 1-step experiences with actions distribution [173   7   9  13]\n",
      "average reward per episode = -154.1\n",
      "Training on 10/206 positive/total out of 350 1-step experiences with actions distribution [  1 131  31  43]\n",
      "average reward per episode = -208.2\n",
      "Training on 5/205 positive/total out of 428 1-step experiences with actions distribution [ 8 60 52 85]\n",
      "average reward per episode = -248.7\n",
      "Training on 3/203 positive/total out of 526 1-step experiences with actions distribution [  6  21 109  67]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 18:07:28,185] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/q_learning/openaigym.video.1.52235.video000512.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = -328.3\n",
      "Training on 3/202 positive/total out of 593 1-step experiences with actions distribution [145  46   6   5]\n",
      "average reward per episode = -312.9\n",
      "Training on 2/201 positive/total out of 634 1-step experiences with actions distribution [  9   5 174  13]\n",
      "average reward per episode = -316.4\n",
      "Training on 0/200 positive/total out of 639 1-step experiences with actions distribution [  6   4   5 185]\n",
      "average reward per episode = -382.5\n",
      "Training on 0/200 positive/total out of 699 1-step experiences with actions distribution [  3  28 158  11]\n",
      "average reward per episode = -226.6\n",
      "Training on 4/204 positive/total out of 482 1-step experiences with actions distribution [137  35  15  17]\n",
      "average reward per episode = -272.2\n",
      "Training on 6/203 positive/total out of 509 1-step experiences with actions distribution [  5  53 112  33]\n",
      "average reward per episode = -189.4\n",
      "Training on 8/206 positive/total out of 424 1-step experiences with actions distribution [ 8 51 88 59]\n",
      "average reward per episode = -273.2\n",
      "Training on 3/203 positive/total out of 478 1-step experiences with actions distribution [ 26 164   8   5]\n",
      "average reward per episode = -294.3\n",
      "Training on 3/202 positive/total out of 612 1-step experiences with actions distribution [115  10  48  29]\n",
      "average reward per episode = -183.2\n",
      "Training on 10/205 positive/total out of 363 1-step experiences with actions distribution [  6 104  49  46]\n",
      "average reward per episode = -310.8\n",
      "Training on 5/204 positive/total out of 523 1-step experiences with actions distribution [  9 177   6  12]\n",
      "average reward per episode = -322.4\n",
      "Training on 1/201 positive/total out of 645 1-step experiences with actions distribution [  9   4  75 113]\n",
      "average reward per episode = -263.1\n",
      "Training on 5/204 positive/total out of 592 1-step experiences with actions distribution [ 13   8 101  82]\n",
      "average reward per episode = -204.1\n",
      "Training on 3/203 positive/total out of 453 1-step experiences with actions distribution [167  19   6  11]\n",
      "average reward per episode = -184.1\n",
      "Training on 13/208 positive/total out of 425 1-step experiences with actions distribution [  6 161  23  18]\n",
      "average reward per episode = -272.8\n",
      "Training on 2/202 positive/total out of 563 1-step experiences with actions distribution [60 10 53 79]\n",
      "average reward per episode = -284.8\n",
      "Training on 4/202 positive/total out of 596 1-step experiences with actions distribution [188   9   3   2]\n",
      "average reward per episode = -50.2\n",
      "Training on 10/207 positive/total out of 215 1-step experiences with actions distribution [ 0 64 85 58]\n",
      "average reward per episode = -169.5\n",
      "Training on 8/204 positive/total out of 378 1-step experiences with actions distribution [  8  49  39 108]\n",
      "average reward per episode = -302.8\n",
      "Training on 2/202 positive/total out of 570 1-step experiences with actions distribution [  1  29 144  28]\n",
      "average reward per episode = -210.1\n",
      "Training on 4/203 positive/total out of 462 1-step experiences with actions distribution [160  20  11  12]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 18:12:23,331] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/q_learning/openaigym.video.1.52235.video000729.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = -262.6\n",
      "Training on 5/203 positive/total out of 510 1-step experiences with actions distribution [  3  34 129  37]\n",
      "average reward per episode = -36.3\n",
      "Training on 12/209 positive/total out of 225 1-step experiences with actions distribution [ 8 81 55 65]\n",
      "average reward per episode = -265.7\n",
      "Training on 5/203 positive/total out of 509 1-step experiences with actions distribution [41 43 94 25]\n",
      "average reward per episode = -204.6\n",
      "Training on 5/204 positive/total out of 441 1-step experiences with actions distribution [29 26 92 57]\n",
      "average reward per episode = -157.7\n",
      "Training on 9/206 positive/total out of 365 1-step experiences with actions distribution [88 90 17 11]\n",
      "average reward per episode = -215.3\n",
      "Training on 6/205 positive/total out of 438 1-step experiences with actions distribution [ 7 75 82 41]\n",
      "average reward per episode = -153.4\n",
      "Training on 9/206 positive/total out of 397 1-step experiences with actions distribution [96 25 41 44]\n",
      "average reward per episode = -144.1\n",
      "Training on 10/205 positive/total out of 333 1-step experiences with actions distribution [20 87 24 74]\n",
      "average reward per episode = -166.5\n",
      "Training on 6/204 positive/total out of 402 1-step experiences with actions distribution [115  19  40  30]\n",
      "average reward per episode = -172.4\n",
      "Training on 7/205 positive/total out of 386 1-step experiences with actions distribution [  0  48 110  47]\n",
      "average reward per episode = -245.7\n",
      "Training on 5/203 positive/total out of 518 1-step experiences with actions distribution [152  20  10  21]\n",
      "average reward per episode = -33.7\n",
      "Training on 12/207 positive/total out of 192 1-step experiences with actions distribution [ 3 63 44 97]\n",
      "average reward per episode = -304.7\n",
      "Training on 2/202 positive/total out of 632 1-step experiences with actions distribution [164  16  11  11]\n",
      "average reward per episode = -129.8\n",
      "Training on 8/205 positive/total out of 324 1-step experiences with actions distribution [ 6 46 62 91]\n",
      "average reward per episode = -137.4\n",
      "Training on 10/206 positive/total out of 349 1-step experiences with actions distribution [116  52  13  25]\n",
      "average reward per episode = -157.6\n",
      "Training on 8/205 positive/total out of 360 1-step experiences with actions distribution [103  62  31   9]\n",
      "average reward per episode = -155.9\n",
      "Training on 5/204 positive/total out of 363 1-step experiences with actions distribution [ 6 31 74 93]\n",
      "average reward per episode = -103.6\n",
      "Training on 9/206 positive/total out of 310 1-step experiences with actions distribution [  8  24  66 108]\n",
      "average reward per episode = -144.4\n",
      "Training on 9/205 positive/total out of 331 1-step experiences with actions distribution [60 98 36 11]\n",
      "average reward per episode = -61.7\n",
      "Training on 10/206 positive/total out of 251 1-step experiences with actions distribution [  4  23 128  51]\n",
      "average reward per episode = -148.4\n",
      "Training on 11/205 positive/total out of 315 1-step experiences with actions distribution [ 14 181   3   7]\n",
      "average reward per episode = -152.5\n",
      "Training on 10/206 positive/total out of 331 1-step experiences with actions distribution [ 66 132   4   4]\n",
      "average reward per episode = -40.0\n",
      "Training on 21/208 positive/total out of 219 1-step experiences with actions distribution [12 47 79 70]\n",
      "average reward per episode = -74.8\n",
      "Training on 8/206 positive/total out of 256 1-step experiences with actions distribution [ 10  63 114  19]\n",
      "average reward per episode = -95.8\n",
      "Training on 12/206 positive/total out of 295 1-step experiences with actions distribution [113  22  18  53]\n",
      "average reward per episode = -294.4\n",
      "Training on 4/203 positive/total out of 559 1-step experiences with actions distribution [17 48 72 66]\n",
      "average reward per episode = -156.5\n",
      "Training on 7/205 positive/total out of 397 1-step experiences with actions distribution [156  24   2  23]\n",
      "average reward per episode = -161.7\n",
      "Training on 7/205 positive/total out of 379 1-step experiences with actions distribution [  7  34  60 104]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 18:19:17,957] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/q_learning/openaigym.video.1.52235.video001000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = -205.1\n",
      "Training on 5/204 positive/total out of 479 1-step experiences with actions distribution [173  12  18   1]\n",
      "average reward per episode = -71.6\n",
      "Training on 13/207 positive/total out of 259 1-step experiences with actions distribution [38 37 80 52]\n",
      "average reward per episode = -138.5\n",
      "Training on 8/205 positive/total out of 367 1-step experiences with actions distribution [123  14  46  22]\n",
      "average reward per episode = -147.9\n",
      "Training on 5/205 positive/total out of 372 1-step experiences with actions distribution [101  21  38  45]\n",
      "average reward per episode = -99.7\n",
      "Training on 11/206 positive/total out of 311 1-step experiences with actions distribution [ 18  18 159  11]\n",
      "average reward per episode = -13.3\n",
      "Training on 17/208 positive/total out of 175 1-step experiences with actions distribution [111  43  17  37]\n",
      "average reward per episode = -96.2\n",
      "Training on 10/206 positive/total out of 298 1-step experiences with actions distribution [ 22  16 150  18]\n",
      "average reward per episode = -171.1\n",
      "Training on 5/204 positive/total out of 380 1-step experiences with actions distribution [53 54 70 27]\n",
      "average reward per episode = -57.1\n",
      "Training on 13/206 positive/total out of 240 1-step experiences with actions distribution [ 54  27 103  22]\n",
      "average reward per episode = -56.8\n",
      "Training on 12/206 positive/total out of 234 1-step experiences with actions distribution [44 35 90 37]\n",
      "average reward per episode = -106.4\n",
      "Training on 10/206 positive/total out of 316 1-step experiences with actions distribution [ 12  26 146  22]\n",
      "average reward per episode = -37.1\n",
      "Training on 14/208 positive/total out of 216 1-step experiences with actions distribution [41 80 54 33]\n",
      "average reward per episode = -93.9\n",
      "Training on 7/205 positive/total out of 294 1-step experiences with actions distribution [ 22  21 139  23]\n",
      "average reward per episode = -34.7\n",
      "Training on 17/208 positive/total out of 231 1-step experiences with actions distribution [79 42 53 34]\n",
      "average reward per episode = -187.0\n",
      "Training on 5/203 positive/total out of 436 1-step experiences with actions distribution [  8  12 150  33]\n",
      "average reward per episode = -239.8\n",
      "Training on 8/205 positive/total out of 517 1-step experiences with actions distribution [76 38 47 44]\n",
      "average reward per episode = -57.8\n",
      "Training on 11/206 positive/total out of 248 1-step experiences with actions distribution [ 12  21 170   3]\n",
      "average reward per episode = -145.0\n",
      "Training on 10/205 positive/total out of 369 1-step experiences with actions distribution [ 29  22 104  50]\n",
      "average reward per episode = -56.2\n",
      "Training on 16/207 positive/total out of 244 1-step experiences with actions distribution [142  21  34  10]\n",
      "average reward per episode = -149.3\n",
      "Training on 8/205 positive/total out of 371 1-step experiences with actions distribution [  6  34 116  49]\n",
      "average reward per episode = -139.8\n",
      "Training on 12/206 positive/total out of 394 1-step experiences with actions distribution [165  17  14  10]\n",
      "average reward per episode = -22.6\n",
      "Training on 14/207 positive/total out of 196 1-step experiences with actions distribution [35 29 90 53]\n",
      "average reward per episode = -146.5\n",
      "Training on 7/205 positive/total out of 384 1-step experiences with actions distribution [55 12 88 50]\n",
      "average reward per episode = 32.7\n",
      "Training on 28/209 positive/total out of 130 1-step experiences with actions distribution [68 34 85 22]\n",
      "average reward per episode = -8.6\n",
      "Training on 13/207 positive/total out of 175 1-step experiences with actions distribution [  4  17 137  49]\n",
      "average reward per episode = -99.3\n",
      "Training on 9/206 positive/total out of 304 1-step experiences with actions distribution [  6  19 117  64]\n",
      "average reward per episode = -119.9\n",
      "Training on 7/205 positive/total out of 350 1-step experiences with actions distribution [ 30  13 102  60]\n",
      "average reward per episode = -89.8\n",
      "Training on 9/206 positive/total out of 311 1-step experiences with actions distribution [119  15  46  26]\n",
      "average reward per episode = -113.7\n",
      "Training on 10/206 positive/total out of 338 1-step experiences with actions distribution [  1  18 114  73]\n",
      "average reward per episode = -91.9\n",
      "Training on 10/207 positive/total out of 321 1-step experiences with actions distribution [11 29 96 71]\n",
      "average reward per episode = -56.0\n",
      "Training on 12/207 positive/total out of 262 1-step experiences with actions distribution [19 21 99 68]\n",
      "average reward per episode = -86.4\n",
      "Training on 10/207 positive/total out of 322 1-step experiences with actions distribution [93 23 45 46]\n",
      "average reward per episode = -20.9\n",
      "Training on 15/207 positive/total out of 195 1-step experiences with actions distribution [ 11  25 105  66]\n",
      "average reward per episode = 38.6\n",
      "Training on 27/209 positive/total out of 102 1-step experiences with actions distribution [ 23  32 102  52]\n",
      "average reward per episode = -99.9\n",
      "Training on 6/205 positive/total out of 310 1-step experiences with actions distribution [  5  18 134  48]\n",
      "average reward per episode = 48.8\n",
      "Training on 26/209 positive/total out of 98 1-step experiences with actions distribution [  3  44  51 111]\n",
      "average reward per episode = -66.5\n",
      "Training on 14/207 positive/total out of 262 1-step experiences with actions distribution [ 15  21  47 124]\n",
      "average reward per episode = -118.0\n",
      "Training on 13/207 positive/total out of 347 1-step experiences with actions distribution [33 32 79 63]\n",
      "average reward per episode = -21.7\n",
      "Training on 14/207 positive/total out of 199 1-step experiences with actions distribution [ 32  16  57 102]\n",
      "average reward per episode = -25.1\n",
      "Training on 11/207 positive/total out of 206 1-step experiences with actions distribution [100  19  30  58]\n",
      "average reward per episode = -103.8\n",
      "Training on 11/206 positive/total out of 312 1-step experiences with actions distribution [  4  22  56 124]\n",
      "average reward per episode = 75.3\n",
      "Training on 43/209 positive/total out of 47 1-step experiences with actions distribution [ 5 72 80 52]\n",
      "average reward per episode = -140.5\n",
      "Training on 6/205 positive/total out of 371 1-step experiences with actions distribution [  3  12  29 161]\n",
      "average reward per episode = 28.9\n",
      "Training on 15/208 positive/total out of 117 1-step experiences with actions distribution [  5  28  65 110]\n",
      "average reward per episode = -87.3\n",
      "Training on 9/205 positive/total out of 291 1-step experiences with actions distribution [ 10  11  38 146]\n",
      "average reward per episode = 69.7\n",
      "Training on 36/209 positive/total out of 56 1-step experiences with actions distribution [ 0 97 13 99]\n",
      "average reward per episode = -51.8\n",
      "Training on 9/206 positive/total out of 239 1-step experiences with actions distribution [  6  13  11 176]\n",
      "average reward per episode = -54.1\n",
      "Training on 14/207 positive/total out of 244 1-step experiences with actions distribution [  1  18  15 173]\n",
      "average reward per episode = -54.1\n",
      "Training on 10/206 positive/total out of 242 1-step experiences with actions distribution [  6  14  28 158]\n",
      "average reward per episode = -51.3\n",
      "Training on 9/206 positive/total out of 236 1-step experiences with actions distribution [  6  23  65 112]\n",
      "average reward per episode = -97.9\n",
      "Training on 10/205 positive/total out of 307 1-step experiences with actions distribution [  7  20   5 173]\n",
      "average reward per episode = -56.1\n",
      "Training on 10/206 positive/total out of 247 1-step experiences with actions distribution [  7  11  55 133]\n",
      "average reward per episode = 29.5\n",
      "Training on 22/208 positive/total out of 116 1-step experiences with actions distribution [  0  36  31 141]\n",
      "average reward per episode = 39.0\n",
      "Training on 23/208 positive/total out of 101 1-step experiences with actions distribution [  2  29   3 174]\n",
      "average reward per episode = -5.2\n",
      "Training on 20/208 positive/total out of 167 1-step experiences with actions distribution [  7  25  49 127]\n",
      "average reward per episode = -11.2\n",
      "Training on 14/208 positive/total out of 174 1-step experiences with actions distribution [47 30 41 90]\n",
      "average reward per episode = 64.1\n",
      "Training on 36/209 positive/total out of 69 1-step experiences with actions distribution [ 9 52 73 75]\n",
      "average reward per episode = 51.0\n",
      "Training on 27/209 positive/total out of 96 1-step experiences with actions distribution [ 4 38 70 97]\n",
      "average reward per episode = -30.8\n",
      "Training on 13/207 positive/total out of 213 1-step experiences with actions distribution [  4  29  33 141]\n",
      "average reward per episode = 42.1\n",
      "Training on 22/209 positive/total out of 116 1-step experiences with actions distribution [11 23 96 79]\n",
      "average reward per episode = 39.0\n",
      "Training on 24/209 positive/total out of 119 1-step experiences with actions distribution [ 6 30 80 93]\n",
      "average reward per episode = 75.7\n",
      "Training on 54/209 positive/total out of 48 1-step experiences with actions distribution [ 5 62 69 73]\n",
      "average reward per episode = 61.1\n",
      "Training on 34/209 positive/total out of 75 1-step experiences with actions distribution [ 4 49 97 59]\n",
      "average reward per episode = 36.5\n",
      "Training on 30/208 positive/total out of 105 1-step experiences with actions distribution [  4  36 152  16]\n",
      "average reward per episode = 70.0\n",
      "Training on 42/209 positive/total out of 60 1-step experiences with actions distribution [ 6 48 72 83]\n",
      "average reward per episode = 71.5\n",
      "Training on 36/209 positive/total out of 58 1-step experiences with actions distribution [ 3 36 87 83]\n",
      "average reward per episode = 33.4\n",
      "Training on 21/208 positive/total out of 112 1-step experiences with actions distribution [  2  24 170  12]\n",
      "average reward per episode = 72.5\n",
      "Training on 40/209 positive/total out of 56 1-step experiences with actions distribution [  8  40 114  47]\n",
      "average reward per episode = 69.1\n",
      "Training on 44/209 positive/total out of 61 1-step experiences with actions distribution [  5  52 104  48]\n",
      "average reward per episode = 78.5\n",
      "Training on 44/209 positive/total out of 43 1-step experiences with actions distribution [ 0 50 83 76]\n",
      "average reward per episode = 76.2\n",
      "Training on 48/209 positive/total out of 45 1-step experiences with actions distribution [ 7 79 42 81]\n",
      "average reward per episode = 66.6\n",
      "Training on 28/209 positive/total out of 65 1-step experiences with actions distribution [ 23  30  21 135]\n",
      "average reward per episode = 74.0\n",
      "Training on 38/209 positive/total out of 53 1-step experiences with actions distribution [  4  38 111  56]\n",
      "average reward per episode = 81.5\n",
      "Training on 64/209 positive/total out of 38 1-step experiences with actions distribution [ 0 64 52 93]\n",
      "average reward per episode = 76.5\n",
      "Training on 40/209 positive/total out of 48 1-step experiences with actions distribution [ 2 40 72 95]\n",
      "average reward per episode = 74.1\n",
      "Training on 44/209 positive/total out of 51 1-step experiences with actions distribution [ 0 59 58 92]\n",
      "average reward per episode = 65.2\n",
      "Training on 43/209 positive/total out of 67 1-step experiences with actions distribution [  3  57  49 100]\n",
      "average reward per episode = 27.4\n",
      "Training on 18/208 positive/total out of 122 1-step experiences with actions distribution [  3  22  47 136]\n",
      "average reward per episode = 69.6\n",
      "Training on 36/209 positive/total out of 60 1-step experiences with actions distribution [ 9 41 69 90]\n",
      "average reward per episode = 84.5\n",
      "Training on 68/209 positive/total out of 32 1-step experiences with actions distribution [  0  68  39 102]\n",
      "average reward per episode = -11.1\n",
      "Training on 14/207 positive/total out of 180 1-step experiences with actions distribution [  8  18  22 159]\n",
      "average reward per episode = -19.1\n",
      "Training on 11/207 positive/total out of 192 1-step experiences with actions distribution [  3  21  22 161]\n",
      "average reward per episode = 73.0\n",
      "Training on 54/209 positive/total out of 55 1-step experiences with actions distribution [  4  54  38 113]\n",
      "average reward per episode = 78.5\n",
      "Training on 50/209 positive/total out of 44 1-step experiences with actions distribution [25 50 64 70]\n",
      "average reward per episode = 28.9\n",
      "Training on 21/208 positive/total out of 118 1-step experiences with actions distribution [ 21  31  26 130]\n",
      "average reward per episode = 78.5\n",
      "Training on 53/209 positive/total out of 44 1-step experiences with actions distribution [ 0 53 71 85]\n",
      "average reward per episode = 31.0\n",
      "Training on 18/208 positive/total out of 114 1-step experiences with actions distribution [  0  27  40 141]\n",
      "average reward per episode = 31.9\n",
      "Training on 18/208 positive/total out of 116 1-step experiences with actions distribution [  2  19  35 152]\n",
      "average reward per episode = -56.8\n",
      "Training on 17/207 positive/total out of 247 1-step experiences with actions distribution [  5  22   6 174]\n",
      "average reward per episode = 79.1\n",
      "Training on 52/209 positive/total out of 42 1-step experiences with actions distribution [ 0 61 82 66]\n",
      "average reward per episode = 31.4\n",
      "Training on 23/208 positive/total out of 116 1-step experiences with actions distribution [  3  27  43 135]\n",
      "average reward per episode = 73.5\n",
      "Training on 38/209 positive/total out of 53 1-step experiences with actions distribution [  0  43 110  56]\n",
      "average reward per episode = 78.5\n",
      "Training on 50/209 positive/total out of 43 1-step experiences with actions distribution [17 55 84 53]\n",
      "average reward per episode = 75.0\n",
      "Training on 51/209 positive/total out of 51 1-step experiences with actions distribution [ 0 51 71 87]\n",
      "average reward per episode = 70.2\n",
      "Training on 54/209 positive/total out of 58 1-step experiences with actions distribution [ 0 70 92 47]\n",
      "average reward per episode = 78.5\n",
      "Training on 45/209 positive/total out of 44 1-step experiences with actions distribution [  0  45  43 121]\n",
      "average reward per episode = 79.0\n",
      "Training on 42/209 positive/total out of 42 1-step experiences with actions distribution [11 49 75 74]\n",
      "average reward per episode = 37.9\n",
      "Training on 25/208 positive/total out of 103 1-step experiences with actions distribution [ 12  27  23 146]\n",
      "average reward per episode = 79.6\n",
      "Training on 55/209 positive/total out of 41 1-step experiences with actions distribution [ 6 69 44 90]\n",
      "average reward per episode = 72.6\n",
      "Training on 41/209 positive/total out of 55 1-step experiences with actions distribution [ 5 48 90 66]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 18:43:50,765] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/q_learning/openaigym.video.1.52235.video002000.mp4\n",
      "[2017-03-04 18:45:09,355] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/gui/Dev/rl-study/tmp/q_learning')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = 72.067\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class EpsilonGreedyQAgent(object):\n",
    "    def __init__(self, model, epsilon=.1):\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        if random.uniform(0, 1) <= self.epsilon:\n",
    "            return random.choice([NOOP, SHOOT, LEFT, RIGHT])\n",
    "        else:\n",
    "            return self.model.predict(observation[np.newaxis])[0].argmax()\n",
    "\n",
    "\n",
    "N_BATCHES = 100\n",
    "N_BATCHED_EPISODES = 10\n",
    "UPDATE_TARGET_EVERY_N_BACTHES = 2\n",
    "MINI_BATCH_SIZE = 32\n",
    "REWARD_CLIP = 5\n",
    "ONLY_N_MISSES = 200\n",
    "\n",
    "env = create_env()\n",
    "env = wrappers.Monitor(env, directory='tmp/q_learning', force=True, mode='training')\n",
    "\n",
    "for _ in range(N_BATCHES):\n",
    "    for _ in range(UPDATE_TARGET_EVERY_N_BACTHES):\n",
    "        sares = episode_sares(env, EpsilonGreedyQAgent(acting_model, epsilon=.1), N_BATCHED_EPISODES)\n",
    "        prev_frames, target_action_rewards = sares_to_input_targets(target_model, sares, reward_clip=REWARD_CLIP, only_n_misses=ONLY_N_MISSES)\n",
    "        acting_model.fit(x=prev_frames, y=target_action_rewards, batch_size=MINI_BATCH_SIZE, nb_epoch=1, verbose=0)\n",
    "    \n",
    "    target_model = copy_model(acting_model)\n",
    "\n",
    "\n",
    "# final greedy episodes\n",
    "sares = episode_sares(env, EpsilonGreedyQAgent(acting_model, epsilon=0), episode_count=1000)\n",
    "\n",
    "env.close()\n",
    "#gym.upload('tmp/q_learning', api_key='sk_bNZUvCfkTfabQCoKoKbjFA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Embedding viz\n",
    "\n",
    "See http://projector.tensorflow.org/?config=https://raw.githubusercontent.com/pilipolio/rl-study/master/projectors/doom_v1_projector_config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "prev_frames, target_action_rewards = sares_to_input_targets(target_model, sares, n_misses=None)\n",
    "\n",
    "thumbnail_size = 150\n",
    "n_frames = 1000\n",
    "frames = prev_frames[:n_frames, :, :, :]\n",
    "thumbnails = frames[:, :, 25:-25, :]\n",
    "frame_embeddings = Sequential(acting_model.layers[:-1]).predict(frames)\n",
    "\n",
    "\n",
    "frame_action_rewards = acting_model.predict(frames)\n",
    "frame_metadata = pd.DataFrame.from_dict({\n",
    "    'best_action': np.array(['NOOP', 'SHOOT', 'LEFT', 'RIGHT'])[frame_action_rewards.argmax(1)], \n",
    "    'value': frame_action_rewards.max(1)})\\\n",
    "    .assign(value_quantile=lambda df: np.digitize(df.value, bins=np.percentile(df.value, q=[25, 50, 75])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.misc\n",
    "\n",
    "def images_to_sprite(data):\n",
    "    \"\"\"Creates the sprite image along with any necessary padding\n",
    "    From https://github.com/tensorflow/tensorflow/issues/6322\n",
    "    Args:\n",
    "      data: NxHxW[x3] tensor containing the images.\n",
    "\n",
    "    Returns:\n",
    "      data: Properly shaped HxWx3 image with any necessary padding.\n",
    "    \"\"\"\n",
    "    if len(data.shape) == 3:\n",
    "        data = np.tile(data[...,np.newaxis], (1,1,1,3))\n",
    "    data = data.astype(np.float32)\n",
    "    min = np.min(data.reshape((data.shape[0], -1)), axis=1)\n",
    "    data = (data.transpose(1,2,3,0) - min).transpose(3,0,1,2)\n",
    "    max = np.max(data.reshape((data.shape[0], -1)), axis=1)\n",
    "    data = (data.transpose(1,2,3,0) / max).transpose(3,0,1,2)\n",
    "    # Inverting the colors seems to look better for MNIST\n",
    "    #data = 1 - data\n",
    "\n",
    "    n = int(np.ceil(np.sqrt(data.shape[0])))\n",
    "    padding = ((0, n ** 2 - data.shape[0]), (0, 0),\n",
    "            (0, 0)) + ((0, 0),) * (data.ndim - 3)\n",
    "    data = np.pad(data, padding, mode='constant',\n",
    "            constant_values=0)\n",
    "    # Tile the individual thumbnails into an image.\n",
    "    data = data.reshape((n, n) + data.shape[1:]).transpose((0, 2, 1, 3)\n",
    "            + tuple(range(4, data.ndim + 1)))\n",
    "    data = data.reshape((n * data.shape[1], n * data.shape[3]) + data.shape[4:])\n",
    "    data = (data * 255).astype(np.uint8)\n",
    "    return data\n",
    "\n",
    "def save_projector_config(frame_embeddings, frame_metadata, thumbnails=None):\n",
    "    gh_root = 'https://raw.githubusercontent.com/pilipolio/rl-study/master'\n",
    "    projector_dir = 'projectors'\n",
    "    embedding_name = 'doom_v1'\n",
    "    \n",
    "    projector_config = {\n",
    "        'embeddings': [\n",
    "        {\n",
    "            'metadataPath': os.path.join(gh_root, projector_dir, embedding_name + '_metadata.tsv'),\n",
    "            'tensorName': 'Frames',\n",
    "            'tensorShape': frame_embeddings.shape,\n",
    "            'tensorPath': os.path.join(gh_root, projector_dir, embedding_name + '.tsv')\n",
    "        }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    if thumbnails is not None:\n",
    "        projector_config['embeddings'][0]['sprite'] = {\n",
    "                'imagePath': os.path.join(gh_root, projector_dir, embedding_name + '_sprite.png'),\n",
    "                'singleImageDim': thumbnails.shape}\n",
    "        sprite = images_to_sprite(thumbnails)\n",
    "        scipy.misc.imsave(os.path.join(projector_dir, embedding_name + '_sprite.png'), sprite)\n",
    "        \n",
    "    pd.DataFrame(frame_embeddings).to_csv(os.path.join(projector_dir, embedding_name + '.tsv'),\n",
    "                            sep='\\t', index=None, header=None)\n",
    "    frame_metadata.to_csv(os.path.join(projector_dir, embedding_name + '_metadata.tsv'), sep='\\t', index=None)\n",
    "\n",
    "    with open(os.path.join(projector_dir, embedding_name + ('_with_sprite' if thumbnails is not None else '') + '_projector_config.json'), 'w+') as f:\n",
    "        json.dump(projector_config, f)\n",
    "\n",
    "save_projector_config(frame_embeddings, frame_metadata)\n",
    "\n",
    "save_projector_config(frame_embeddings, frame_metadata, thumbnails)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
