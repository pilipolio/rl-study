{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import ppaquette_gym_doom\n",
    "from ppaquette_gym_doom.wrappers import SetResolution, ToDiscrete\n",
    "from gym.wrappers import SkipWrapper\n",
    "from gym import wrappers\n",
    "\n",
    "from scipy.misc import imresize\n",
    "import types\n",
    "\n",
    "def resize_observation_patch(env, size):\n",
    "    \"\"\" Because unfortunately SetResolution also impacts the recorded videos...\n",
    "    \"\"\"\n",
    "    original_reset = env.reset\n",
    "    def reset_with_resized_obs(self):\n",
    "        observation = original_reset()\n",
    "        return imresize(observation, size)\n",
    "\n",
    "    original_step = env.step\n",
    "    def step_with_resized_obs(self, action):\n",
    "        observation, reward, done, end = original_step(action)\n",
    "        return imresize(observation, size), reward, done, end\n",
    "\n",
    "    env.step = types.MethodType(step_with_resized_obs, env)\n",
    "    env.reset = types.MethodType(reset_with_resized_obs, env)\n",
    "    return env\n",
    "\n",
    "WIDTH, HEIGHT = 84, 84\n",
    "\n",
    "# (see https://github.com/ppaquette/gym-doom/blob/master/ppaquette_gym_doom/doom_basic.py)\n",
    "def create_env(seed=None, monitor_directory=None, size=(WIDTH, HEIGHT)):\n",
    "    env_spec = gym.spec('ppaquette/DoomDefendCenter-v0')\n",
    "    env_spec.id = 'ppaquette/DoomDefendCenter-v0'\n",
    "    env = env_spec.make()\n",
    "\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "\n",
    "    if monitor_directory is not None:\n",
    "        env = wrappers.Monitor(env, monitor_directory, force=True, mode='training')\n",
    "\n",
    "    return resize_observation_patch(size=size,\n",
    "                                    env=SkipWrapper(repeat_count=2)(ToDiscrete('minimal')(env)))\n",
    "\n",
    "env = create_env()\n",
    "\n",
    "\n",
    "NOOP, SHOOT, RIGHT, LEFT = 0, 1, 2, 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO4AAADuCAYAAAA+7jsiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBoj/C41eyFWnYqhOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10f395d30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "obs = env.reset()\n",
    "obs, _, _, _ = env.step(action=1)\n",
    "\n",
    "print(obs.shape)\n",
    "def show(observation):\n",
    "    plt.imshow(observation, interpolation=None)\n",
    "    plt.xticks([]); plt.yticks([]);\n",
    "    \n",
    "show(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO4AAADuCAYAAAA+7jsiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAAvRJREFUeJzt2rFtw0AQRcE9QX24/7KUq4dVCTYBE8IDZuIfbPJwDHh2\nd4CWx7cPAK4TLgQJF4KEC0HChSDhQpBwIUi4ECRcCHpeGZ9z/GYFN9vd89vGiwtBwoUg4UKQcCFI\nuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReC\nhAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAh\nSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkX\ngoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBw\nIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJ\nF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQ\ncCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4E\nCReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFC\nkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDhQpBwIUi4ECRcCBIu\nBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReChAtBwoUg4UKQcCFIuBAkXAgSLgQJF4KEC0HChSDh\nQpBwIUi4ECRcCBIuBAkXgoQLQcKFIOFCkHAhSLgQJFwIEi4ECReCnhf375l53XEIMDMzP38Znd29\n+xDgn/lUhiDhQpBwIUi4ECRcCBIuBAkXgoQLQcKFoA9tHg3FnbuYaQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fb54a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "obs, _, _, _ = env.step(action=SHOOT)\n",
    "show(obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import operator\n",
    "\n",
    "SARE = namedtuple('SAR', ['state', 'action', 'reward', 'end'])\n",
    "\n",
    "\n",
    "def generate_sares(env, agent, episode_count=100):\n",
    "    reward = 0\n",
    "    done = False\n",
    "\n",
    "    for i in range(episode_count):\n",
    "        observation = env.reset()\n",
    "        while True:\n",
    "            action = agent.act(observation, reward, done)\n",
    "            new_observation, reward, done, _ = env.step(action)\n",
    "            yield SARE(observation, action, reward, done)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            else:\n",
    "                observation = new_observation\n",
    "\n",
    "def episode_sares(env, agent, episode_count=100):\n",
    "    sares = list(generate_sares(env, agent, episode_count))\n",
    "    print('average reward per episode = {}'.format(\n",
    "        sum(r for _, _, r, _ in sares) / float(sum(e for _, _, _, e in sares))))\n",
    "    return sares\n",
    "\n",
    "\n",
    "def to_experiences(sares, only_n_misses=100):\n",
    "    experiences = [\n",
    "        (previous_s, a, r, next_s, end)\n",
    "        for (previous_s, a, r, end), (next_s, _, _, _) in zip(sares[:-1], sares[1:])\n",
    "    ]\n",
    "\n",
    "    # simplistic experience prioritization\n",
    "    shuffled_exps = experiences if only_n_misses is None\\\n",
    "        else random.choices(experiences, k=only_n_misses) + [e for e in experiences if e[2] > 0]\n",
    "    random.shuffle(shuffled_exps)\n",
    "\n",
    "    prev_frames, actions, rewards, next_frames, is_ends = zip(*shuffled_exps)\n",
    "    prev_frames = np.asarray(prev_frames)\n",
    "    next_frames = np.asarray(next_frames)\n",
    "    actions = np.asarray(actions)\n",
    "    rewards = np.asarray(rewards)\n",
    "    is_ends = np.asarray(is_ends)\n",
    "\n",
    "    print('Training on {}/{} positive/total out of {} 1-step experiences with actions distribution {}'.format(\n",
    "        np.sum(rewards>=0),\n",
    "        len(rewards),\n",
    "        len(experiences),\n",
    "        np.bincount(actions)))\n",
    "    \n",
    "    return (prev_frames, next_frames, actions, rewards, is_ends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.layers import Dense, Convolution2D, Flatten, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "K.set_session(sess)\n",
    "\n",
    "def create_q_model(conv1_weights=None, conv2_weights=None, dense1_weights=None, dense2_weights=None):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(\n",
    "        Convolution2D(nb_filter=8, nb_row=3, nb_col=3, subsample=(2, 2), border_mode='valid', weights=conv1_weights,\n",
    "            input_shape=[HEIGHT, WIDTH, 3], dim_ordering='tf'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(\n",
    "        Convolution2D(nb_filter=8, nb_row=2, nb_col=2, weights=conv2_weights))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, init='normal', weights=dense1_weights))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(4, init='normal', weights=dense2_weights))\n",
    "    model.compile(loss='mse', optimizer=Adam())\n",
    "    \n",
    "    return model\n",
    "\n",
    "acting_model = create_q_model()\n",
    "target_model = create_q_model()\n",
    "\n",
    "def copy_model(model):\n",
    "    conv1_weights =  [w.eval() for w in model.layers[0].weights]\n",
    "    conv2_weights = [w.eval() for w in model.layers[2].weights]\n",
    "    dense1_weights = [w.eval() for w in model.layers[5].weights]\n",
    "    dense2_weights = [w.eval() for w in model.layers[7].weights]\n",
    "    return create_q_model(conv1_weights, conv2_weights, dense1_weights, dense2_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DeepQNetworkTrainer:\n",
    "    \n",
    "    MINI_BATCH_SIZE = 32\n",
    "    UPDATE_TARGET_EVERY_N_BACTHES = 2\n",
    "    \n",
    "    def __init__(self, acting_model, target_model, double_q=False, gamma=.99, reward_clip=5, only_n_negative_experiences=100):\n",
    "        self.acting_model = acting_model\n",
    "        self.target_model = target_model\n",
    "        self.double_q = double_q\n",
    "        \n",
    "        self.gamma = gamma\n",
    "        self.reward_clip = reward_clip\n",
    "        # basic experience prioritization \n",
    "        self.only_n_negative_experiences = only_n_negative_experiences\n",
    "\n",
    "        # counter to periodically set target_model <- acting_model\n",
    "        self.trained_epochs = 0\n",
    "    \n",
    "    def fit_episodes(self, sares):\n",
    "        prev_frames, target_action_rewards = self._sares_to_input_targets(sares, double_q=self.double_q)\n",
    "        self.acting_model.fit(x=prev_frames, y=target_action_rewards, batch_size=self.MINI_BATCH_SIZE, nb_epoch=1, verbose=0)\n",
    "        self.trained_epochs += 1\n",
    "\n",
    "        if (self.trained_epochs % self.UPDATE_TARGET_EVERY_N_BACTHES) == 0:\n",
    "            self.target_model = copy_model(self.acting_model)\n",
    "\n",
    "    def _sares_to_input_targets(self, sares, double_q=False):\n",
    "        \n",
    "        prev_frames, next_frames, actions, rewards, is_ends = to_experiences(sares, self.only_n_negative_experiences)\n",
    "        n_samples = prev_frames.shape[0]\n",
    "\n",
    "        clipped_rewards = np.clip(rewards, -np.inf, self.reward_clip)\n",
    "    \n",
    "        if double_q:\n",
    "            greedy_actions = self.acting_model.predict(next_frames).argmax(axis=1)\n",
    "            actions_target_values = self.target_model.predict(next_frames)[np.arange(n_samples), greedy_actions]            \n",
    "            targets = clipped_rewards + self.gamma * (1 - is_ends) * actions_target_values\n",
    "        else:\n",
    "            # Transcription of the Q-learning target formula\n",
    "            targets = clipped_rewards + self.gamma * (1 - is_ends) * self.target_model.predict(next_frames).max(axis=1)\n",
    "\n",
    "        target_action_rewards = self.target_model.predict(prev_frames)\n",
    "        target_action_rewards[np.arange(n_samples), actions] = targets\n",
    "\n",
    "        return prev_frames, target_action_rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-11 12:47:43,509] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-03-11 12:47:43,874] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/DoomDefendLine_q_learning/openaigym.video.2.17465.video000000.mp4\n",
      "Exception ignored in: <bound method Env.__del__ of <SkipWrapper<ToDiscreteWrapper<_Monitor<TimeLimit<DoomDefendCenterEnv instance>>>>>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/core.py\", line 252, in __del__\n",
      "    self.close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/core.py\", line 190, in close\n",
      "    self._close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/core.py\", line 346, in _close\n",
      "    return self.env.close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/core.py\", line 190, in close\n",
      "    self._close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/core.py\", line 346, in _close\n",
      "    return self.env.close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/core.py\", line 190, in close\n",
      "    self._close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/wrappers/monitoring.py\", line 38, in _close\n",
      "    self._monitor.close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 193, in close\n",
      "    self._flush(force=True)\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 183, in _flush\n",
      "    'env_info': self._env_info(),\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 300, in _env_info\n",
      "    if self.env.spec:\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 98, in env\n",
      "    raise error.Error(\"env has been garbage collected. To keep using a monitor, you must keep around a reference to the env object. (HINT: try assigning the env to a variable in your code.)\")\n",
      "gym.error.Error: env has been garbage collected. To keep using a monitor, you must keep around a reference to the env object. (HINT: try assigning the env to a variable in your code.)\n",
      "Exception ignored in: <bound method Env.__del__ of <ToDiscreteWrapper<_Monitor<TimeLimit<DoomDefendCenterEnv instance>>>>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/core.py\", line 252, in __del__\n",
      "    self.close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/core.py\", line 190, in close\n",
      "    self._close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/core.py\", line 346, in _close\n",
      "    return self.env.close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/core.py\", line 190, in close\n",
      "    self._close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/wrappers/monitoring.py\", line 38, in _close\n",
      "    self._monitor.close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 193, in close\n",
      "    self._flush(force=True)\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 183, in _flush\n",
      "    'env_info': self._env_info(),\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 300, in _env_info\n",
      "    if self.env.spec:\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 98, in env\n",
      "    raise error.Error(\"env has been garbage collected. To keep using a monitor, you must keep around a reference to the env object. (HINT: try assigning the env to a variable in your code.)\")\n",
      "gym.error.Error: env has been garbage collected. To keep using a monitor, you must keep around a reference to the env object. (HINT: try assigning the env to a variable in your code.)\n",
      "Exception ignored in: <bound method Env.__del__ of <_Monitor<TimeLimit<DoomDefendCenterEnv instance>>>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/core.py\", line 252, in __del__\n",
      "    self.close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/core.py\", line 190, in close\n",
      "    self._close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/wrappers/monitoring.py\", line 38, in _close\n",
      "    self._monitor.close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 193, in close\n",
      "    self._flush(force=True)\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 183, in _flush\n",
      "    'env_info': self._env_info(),\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 300, in _env_info\n",
      "    if self.env.spec:\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 98, in env\n",
      "    raise error.Error(\"env has been garbage collected. To keep using a monitor, you must keep around a reference to the env object. (HINT: try assigning the env to a variable in your code.)\")\n",
      "gym.error.Error: env has been garbage collected. To keep using a monitor, you must keep around a reference to the env object. (HINT: try assigning the env to a variable in your code.)\n",
      "Exception ignored in: <bound method MonitorManager.__del__ of <gym.monitoring.monitor_manager.MonitorManager object at 0x10f89edd8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 306, in __del__\n",
      "    self.close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 193, in close\n",
      "    self._flush(force=True)\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 183, in _flush\n",
      "    'env_info': self._env_info(),\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 300, in _env_info\n",
      "    if self.env.spec:\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 98, in env\n",
      "    raise error.Error(\"env has been garbage collected. To keep using a monitor, you must keep around a reference to the env object. (HINT: try assigning the env to a variable in your code.)\")\n",
      "gym.error.Error: env has been garbage collected. To keep using a monitor, you must keep around a reference to the env object. (HINT: try assigning the env to a variable in your code.)\n",
      "[2017-03-11 12:47:47,633] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/DoomDefendLine_q_learning/openaigym.video.2.17465.video000001.mp4\n",
      "[2017-03-11 12:47:57,175] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/DoomDefendLine_q_learning/openaigym.video.2.17465.video000008.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = -0.5\n",
      "Training on 105/105 positive/total out of 1062 1-step experiences with actions distribution [ 3  3 96  3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method Env.__del__ of <SkipWrapper<ToDiscreteWrapper<_Monitor<TimeLimit<DoomDefendCenterEnv instance>>>>>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/core.py\", line 252, in __del__\n",
      "    self.close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/core.py\", line 190, in close\n",
      "    self._close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/core.py\", line 346, in _close\n",
      "    return self.env.close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/core.py\", line 190, in close\n",
      "    self._close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/core.py\", line 346, in _close\n",
      "    return self.env.close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/core.py\", line 190, in close\n",
      "    self._close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/wrappers/monitoring.py\", line 38, in _close\n",
      "    self._monitor.close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 193, in close\n",
      "    self._flush(force=True)\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 183, in _flush\n",
      "    'env_info': self._env_info(),\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 300, in _env_info\n",
      "    if self.env.spec:\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 98, in env\n",
      "    raise error.Error(\"env has been garbage collected. To keep using a monitor, you must keep around a reference to the env object. (HINT: try assigning the env to a variable in your code.)\")\n",
      "gym.error.Error: env has been garbage collected. To keep using a monitor, you must keep around a reference to the env object. (HINT: try assigning the env to a variable in your code.)\n",
      "Exception ignored in: <bound method Env.__del__ of <ToDiscreteWrapper<_Monitor<TimeLimit<DoomDefendCenterEnv instance>>>>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/core.py\", line 252, in __del__\n",
      "    self.close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/core.py\", line 190, in close\n",
      "    self._close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/core.py\", line 346, in _close\n",
      "    return self.env.close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/core.py\", line 190, in close\n",
      "    self._close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/wrappers/monitoring.py\", line 38, in _close\n",
      "    self._monitor.close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 193, in close\n",
      "    self._flush(force=True)\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 183, in _flush\n",
      "    'env_info': self._env_info(),\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 300, in _env_info\n",
      "    if self.env.spec:\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 98, in env\n",
      "    raise error.Error(\"env has been garbage collected. To keep using a monitor, you must keep around a reference to the env object. (HINT: try assigning the env to a variable in your code.)\")\n",
      "gym.error.Error: env has been garbage collected. To keep using a monitor, you must keep around a reference to the env object. (HINT: try assigning the env to a variable in your code.)\n",
      "Exception ignored in: <bound method Env.__del__ of <_Monitor<TimeLimit<DoomDefendCenterEnv instance>>>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/core.py\", line 252, in __del__\n",
      "    self.close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/core.py\", line 190, in close\n",
      "    self._close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/wrappers/monitoring.py\", line 38, in _close\n",
      "    self._monitor.close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 193, in close\n",
      "    self._flush(force=True)\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 183, in _flush\n",
      "    'env_info': self._env_info(),\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 300, in _env_info\n",
      "    if self.env.spec:\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 98, in env\n",
      "    raise error.Error(\"env has been garbage collected. To keep using a monitor, you must keep around a reference to the env object. (HINT: try assigning the env to a variable in your code.)\")\n",
      "gym.error.Error: env has been garbage collected. To keep using a monitor, you must keep around a reference to the env object. (HINT: try assigning the env to a variable in your code.)\n",
      "Exception ignored in: <bound method MonitorManager.__del__ of <gym.monitoring.monitor_manager.MonitorManager object at 0x1161a0358>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 306, in __del__\n",
      "    self.close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 193, in close\n",
      "    self._flush(force=True)\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 183, in _flush\n",
      "    'env_info': self._env_info(),\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 300, in _env_info\n",
      "    if self.env.spec:\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 98, in env\n",
      "    raise error.Error(\"env has been garbage collected. To keep using a monitor, you must keep around a reference to the env object. (HINT: try assigning the env to a variable in your code.)\")\n",
      "gym.error.Error: env has been garbage collected. To keep using a monitor, you must keep around a reference to the env object. (HINT: try assigning the env to a variable in your code.)\n",
      "Exception ignored in: <bound method Env.__del__ of <TimeLimit<SkipWrapper<ToDiscreteWrapper<_Monitor<TimeLimit<DoomDefendCenterEnv instance>>>>>>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/core.py\", line 252, in __del__\n",
      "    self.close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/core.py\", line 190, in close\n",
      "    self._close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/core.py\", line 346, in _close\n",
      "    return self.env.close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/core.py\", line 190, in close\n",
      "    self._close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/core.py\", line 346, in _close\n",
      "    return self.env.close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/core.py\", line 190, in close\n",
      "    self._close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/core.py\", line 346, in _close\n",
      "    return self.env.close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/core.py\", line 190, in close\n",
      "    self._close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/wrappers/monitoring.py\", line 38, in _close\n",
      "    self._monitor.close()\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 193, in close\n",
      "    self._flush(force=True)\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 183, in _flush\n",
      "    'env_info': self._env_info(),\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 300, in _env_info\n",
      "    if self.env.spec:\n",
      "  File \"/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/gym/monitoring/monitor_manager.py\", line 98, in env\n",
      "    raise error.Error(\"env has been garbage collected. To keep using a monitor, you must keep around a reference to the env object. (HINT: try assigning the env to a variable in your code.)\")\n",
      "gym.error.Error: env has been garbage collected. To keep using a monitor, you must keep around a reference to the env object. (HINT: try assigning the env to a variable in your code.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = 0.6\n",
      "Training on 116/116 positive/total out of 1035 1-step experiences with actions distribution [  6 104   2   4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-11 12:48:19,526] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/DoomDefendLine_q_learning/openaigym.video.2.17465.video000027.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = -0.7\n",
      "Training on 102/103 positive/total out of 940 1-step experiences with actions distribution [96  3  1  3]\n",
      "average reward per episode = -0.2\n",
      "Training on 107/108 positive/total out of 967 1-step experiences with actions distribution [97  4  2  5]\n",
      "average reward per episode = 0.0\n",
      "Training on 109/110 positive/total out of 988 1-step experiences with actions distribution [97  4  7  2]\n",
      "average reward per episode = -0.1\n",
      "Training on 106/109 positive/total out of 993 1-step experiences with actions distribution [78 23  8]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-11 12:48:57,583] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/DoomDefendLine_q_learning/openaigym.video.2.17465.video000064.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = -0.2\n",
      "Training on 105/108 positive/total out of 957 1-step experiences with actions distribution [92  5  9  2]\n",
      "average reward per episode = -0.8\n",
      "Training on 101/102 positive/total out of 924 1-step experiences with actions distribution [92  2  5  3]\n",
      "average reward per episode = -0.2\n",
      "Training on 108/108 positive/total out of 979 1-step experiences with actions distribution [93  5  6  4]\n",
      "average reward per episode = -0.6\n",
      "Training on 104/104 positive/total out of 891 1-step experiences with actions distribution [93  3  7  1]\n",
      "average reward per episode = 0.2\n",
      "Training on 111/111 positive/total out of 1047 1-step experiences with actions distribution [94  7  9  1]\n",
      "average reward per episode = -0.2\n",
      "Training on 108/108 positive/total out of 1009 1-step experiences with actions distribution [80  9 16  3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-11 12:50:09,028] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/DoomDefendLine_q_learning/openaigym.video.2.17465.video000125.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = -0.4\n",
      "Training on 104/106 positive/total out of 986 1-step experiences with actions distribution [88  5 11  2]\n",
      "average reward per episode = -0.7\n",
      "Training on 102/103 positive/total out of 934 1-step experiences with actions distribution [77  4 18  4]\n",
      "average reward per episode = -0.1\n",
      "Training on 109/109 positive/total out of 942 1-step experiences with actions distribution [93  3 10  3]\n",
      "average reward per episode = 0.3\n",
      "Training on 113/113 positive/total out of 1014 1-step experiences with actions distribution [91 11  8  3]\n",
      "average reward per episode = -0.6\n",
      "Training on 102/104 positive/total out of 972 1-step experiences with actions distribution [75  8 18  3]\n",
      "average reward per episode = -0.2\n",
      "Training on 108/108 positive/total out of 1045 1-step experiences with actions distribution [85  6 14  3]\n",
      "average reward per episode = -0.3\n",
      "Training on 106/107 positive/total out of 990 1-step experiences with actions distribution [77 12 15  3]\n",
      "average reward per episode = 0.2\n",
      "Training on 112/112 positive/total out of 1045 1-step experiences with actions distribution [79  9 20  4]\n",
      "average reward per episode = 0.0\n",
      "Training on 109/110 positive/total out of 997 1-step experiences with actions distribution [88  7 13  2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-11 12:51:40,859] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/DoomDefendLine_q_learning/openaigym.video.2.17465.video000216.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = -0.5\n",
      "Training on 105/105 positive/total out of 992 1-step experiences with actions distribution [77  8 17  3]\n",
      "average reward per episode = 0.0\n",
      "Training on 110/110 positive/total out of 1058 1-step experiences with actions distribution [77 16 14  3]\n",
      "average reward per episode = -0.5\n",
      "Training on 102/105 positive/total out of 972 1-step experiences with actions distribution [77 12 15  1]\n",
      "average reward per episode = -0.2\n",
      "Training on 108/108 positive/total out of 1037 1-step experiences with actions distribution [85  8 14  1]\n",
      "average reward per episode = -0.1\n",
      "Training on 106/109 positive/total out of 958 1-step experiences with actions distribution [86  8  9  6]\n",
      "average reward per episode = -0.5\n",
      "Training on 104/105 positive/total out of 1013 1-step experiences with actions distribution [69 15 19  2]\n",
      "average reward per episode = 0.9\n",
      "Training on 119/119 positive/total out of 1260 1-step experiences with actions distribution [56 19 42  2]\n",
      "average reward per episode = 0.1\n",
      "Training on 110/110 positive/total out of 1053 1-step experiences with actions distribution [75  6 23  6]\n",
      "average reward per episode = 0.0\n",
      "Training on 110/110 positive/total out of 1073 1-step experiences with actions distribution [75 14 16  5]\n",
      "average reward per episode = 0.4\n",
      "Training on 114/114 positive/total out of 1074 1-step experiences with actions distribution [67 16 27  4]\n",
      "average reward per episode = 0.2\n",
      "Training on 112/112 positive/total out of 1055 1-step experiences with actions distribution [64 18 29  1]\n",
      "average reward per episode = 0.3\n",
      "Training on 113/113 positive/total out of 995 1-step experiences with actions distribution [67 12 27  7]\n",
      "average reward per episode = 0.1\n",
      "Training on 108/110 positive/total out of 1056 1-step experiences with actions distribution [67 14 28  1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-11 12:54:13,797] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/DoomDefendLine_q_learning/openaigym.video.2.17465.video000343.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = 1.1\n",
      "Training on 121/121 positive/total out of 1185 1-step experiences with actions distribution [52 15 47  7]\n",
      "average reward per episode = 2.0\n",
      "Training on 130/130 positive/total out of 1476 1-step experiences with actions distribution [39 35 55  1]\n",
      "average reward per episode = 0.9\n",
      "Training on 119/119 positive/total out of 1194 1-step experiences with actions distribution [42 25 50  2]\n",
      "average reward per episode = -0.4\n",
      "Training on 105/106 positive/total out of 967 1-step experiences with actions distribution [72 20 13  1]\n",
      "average reward per episode = 0.9\n",
      "Training on 119/119 positive/total out of 1153 1-step experiences with actions distribution [10 31 74  4]\n",
      "average reward per episode = 1.4\n",
      "Training on 123/124 positive/total out of 1287 1-step experiences with actions distribution [ 9 36 75  4]\n",
      "average reward per episode = 1.4\n",
      "Training on 124/124 positive/total out of 1393 1-step experiences with actions distribution [20 39 64  1]\n",
      "average reward per episode = 0.1\n",
      "Training on 108/111 positive/total out of 1119 1-step experiences with actions distribution [28 30 49  4]\n",
      "average reward per episode = 1.7\n",
      "Training on 127/127 positive/total out of 1440 1-step experiences with actions distribution [31 16 75  5]\n",
      "average reward per episode = 1.6\n",
      "Training on 125/125 positive/total out of 1365 1-step experiences with actions distribution [26 19 74  6]\n",
      "average reward per episode = -0.2\n",
      "Training on 107/107 positive/total out of 1061 1-step experiences with actions distribution [62  9 35  1]\n",
      "average reward per episode = -0.2\n",
      "Training on 108/108 positive/total out of 975 1-step experiences with actions distribution [65  7 31  5]\n",
      "average reward per episode = 1.0\n",
      "Training on 120/120 positive/total out of 1237 1-step experiences with actions distribution [55 19 41  5]\n",
      "average reward per episode = 0.2\n",
      "Training on 112/112 positive/total out of 1088 1-step experiences with actions distribution [64 16 30  2]\n",
      "average reward per episode = 2.2\n",
      "Training on 131/131 positive/total out of 1418 1-step experiences with actions distribution [48 34 47  2]\n",
      "average reward per episode = 1.2\n",
      "Training on 122/122 positive/total out of 1243 1-step experiences with actions distribution [55 27 37  3]\n",
      "average reward per episode = 1.1\n",
      "Training on 118/121 positive/total out of 1258 1-step experiences with actions distribution [40 33 44  4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-11 12:57:52,579] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/DoomDefendLine_q_learning/openaigym.video.2.17465.video000512.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = 0.2\n",
      "Training on 111/112 positive/total out of 1075 1-step experiences with actions distribution [64 29 15  4]\n",
      "average reward per episode = 1.4\n",
      "Training on 122/123 positive/total out of 1207 1-step experiences with actions distribution [15 33 73  2]\n",
      "average reward per episode = 0.9\n",
      "Training on 118/119 positive/total out of 1237 1-step experiences with actions distribution [25 25 63  6]\n",
      "average reward per episode = 0.9\n",
      "Training on 119/119 positive/total out of 1309 1-step experiences with actions distribution [12 42 62  3]\n",
      "average reward per episode = 0.6\n",
      "Training on 115/116 positive/total out of 1146 1-step experiences with actions distribution [16 36 60  4]\n",
      "average reward per episode = 2.0\n",
      "Training on 129/129 positive/total out of 1373 1-step experiences with actions distribution [26 47 51  5]\n",
      "average reward per episode = 0.8\n",
      "Training on 117/118 positive/total out of 1276 1-step experiences with actions distribution [21 28 63  6]\n",
      "average reward per episode = 1.5\n",
      "Training on 124/125 positive/total out of 1196 1-step experiences with actions distribution [25 58 39  3]\n",
      "average reward per episode = 1.6\n",
      "Training on 125/126 positive/total out of 1302 1-step experiences with actions distribution [27 48 42  9]\n",
      "average reward per episode = 2.6\n",
      "Training on 134/136 positive/total out of 1453 1-step experiences with actions distribution [24 61 44  7]\n",
      "average reward per episode = 2.4\n",
      "Training on 133/133 positive/total out of 1476 1-step experiences with actions distribution [24 54 52  3]\n",
      "average reward per episode = 3.1\n",
      "Training on 140/140 positive/total out of 1552 1-step experiences with actions distribution [15 66 48 11]\n",
      "average reward per episode = 2.3\n",
      "Training on 133/133 positive/total out of 1407 1-step experiences with actions distribution [16 54 58  5]\n",
      "average reward per episode = 3.5\n",
      "Training on 144/145 positive/total out of 1724 1-step experiences with actions distribution [10 74 56  5]\n",
      "average reward per episode = 2.4\n",
      "Training on 134/134 positive/total out of 1361 1-step experiences with actions distribution [15 51 61  7]\n",
      "average reward per episode = 2.4\n",
      "Training on 134/134 positive/total out of 1422 1-step experiences with actions distribution [14 50 63  7]\n",
      "average reward per episode = 1.9\n",
      "Training on 129/129 positive/total out of 1445 1-step experiences with actions distribution [11 59 54  5]\n",
      "average reward per episode = 0.4\n",
      "Training on 113/114 positive/total out of 1049 1-step experiences with actions distribution [43 46 18  7]\n",
      "average reward per episode = 0.5\n",
      "Training on 115/115 positive/total out of 934 1-step experiences with actions distribution [49 49  7 10]\n",
      "average reward per episode = 1.5\n",
      "Training on 124/124 positive/total out of 1282 1-step experiences with actions distribution [27 38 43 16]\n",
      "average reward per episode = 1.3\n",
      "Training on 123/123 positive/total out of 1283 1-step experiences with actions distribution [15 42 59  7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-11 13:03:08,771] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/DoomDefendLine_q_learning/openaigym.video.2.17465.video000729.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = 1.4\n",
      "Training on 124/124 positive/total out of 1117 1-step experiences with actions distribution [28 45 28 23]\n",
      "average reward per episode = 0.7\n",
      "Training on 116/117 positive/total out of 1031 1-step experiences with actions distribution [43 42 21 11]\n",
      "average reward per episode = 0.3\n",
      "Training on 112/113 positive/total out of 1192 1-step experiences with actions distribution [19 50 25 19]\n",
      "average reward per episode = 2.1\n",
      "Training on 130/130 positive/total out of 1365 1-step experiences with actions distribution [14 62 43 11]\n",
      "average reward per episode = 1.3\n",
      "Training on 123/123 positive/total out of 1042 1-step experiences with actions distribution [23 61 28 11]\n",
      "average reward per episode = 1.8\n",
      "Training on 128/128 positive/total out of 1213 1-step experiences with actions distribution [20 47 37 24]\n",
      "average reward per episode = 1.0\n",
      "Training on 120/120 positive/total out of 1271 1-step experiences with actions distribution [13 70 30  7]\n",
      "average reward per episode = 2.0\n",
      "Training on 127/130 positive/total out of 1265 1-step experiences with actions distribution [16 77 28  9]\n",
      "average reward per episode = 0.5\n",
      "Training on 115/115 positive/total out of 1187 1-step experiences with actions distribution [17 46 37 15]\n",
      "average reward per episode = 1.1\n",
      "Training on 121/121 positive/total out of 1211 1-step experiences with actions distribution [31 48 35  7]\n",
      "average reward per episode = 0.8\n",
      "Training on 116/118 positive/total out of 1194 1-step experiences with actions distribution [11 56 40 11]\n",
      "average reward per episode = 2.1\n",
      "Training on 128/131 positive/total out of 1345 1-step experiences with actions distribution [25 43 46 17]\n",
      "average reward per episode = 1.0\n",
      "Training on 120/120 positive/total out of 1201 1-step experiences with actions distribution [14 52 33 21]\n",
      "average reward per episode = 2.4\n",
      "Training on 131/134 positive/total out of 1544 1-step experiences with actions distribution [18 38 61 17]\n",
      "average reward per episode = 0.8\n",
      "Training on 118/118 positive/total out of 1105 1-step experiences with actions distribution [ 1 81 20 16]\n",
      "average reward per episode = 0.5\n",
      "Training on 112/114 positive/total out of 1067 1-step experiences with actions distribution [ 2 71 27 14]\n",
      "average reward per episode = 2.1\n",
      "Training on 129/131 positive/total out of 1407 1-step experiences with actions distribution [ 9 51 52 19]\n",
      "average reward per episode = 1.9\n",
      "Training on 127/127 positive/total out of 1319 1-step experiences with actions distribution [30 50 36 11]\n",
      "average reward per episode = 2.0\n",
      "Training on 128/130 positive/total out of 1388 1-step experiences with actions distribution [12 54 47 17]\n",
      "average reward per episode = 1.9\n",
      "Training on 129/129 positive/total out of 1405 1-step experiences with actions distribution [13 41 53 22]\n",
      "average reward per episode = 0.2\n",
      "Training on 112/112 positive/total out of 1052 1-step experiences with actions distribution [20 49 33 10]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-283-04a0a9a5f8df>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDeepQNetworkTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macting_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdouble_q\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN_BATCHES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0msares\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepisode_sares\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEpsilonGreedyQAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macting_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN_BATCHED_EPISODES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_episodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msares\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-282-87cd91f76faf>\u001b[0m in \u001b[0;36mepisode_sares\u001b[0;34m(env, agent, episode_count)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mepisode_sares\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0msares\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_sares\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     print('average reward per episode = {}'.format(\n\u001b[1;32m     26\u001b[0m         sum(r for _, _, r, _ in sares) / float(sum(e for _, _, _, e in sares))))\n",
      "\u001b[0;32m<ipython-input-282-87cd91f76faf>\u001b[0m in \u001b[0;36mgenerate_sares\u001b[0;34m(env, agent, episode_count)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mnew_observation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mSARE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-276-00e822c5706f>\u001b[0m in \u001b[0;36mstep_with_resized_obs\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep_with_resized_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mimresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMethodType\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_with_resized_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/scipy/misc/pilutil.py\u001b[0m in \u001b[0;36mimresize\u001b[0;34m(arr, size, interp, mode)\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'nearest'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lanczos'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'bicubic'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cubic'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 488\u001b[0;31m     \u001b[0mimnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minterp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    489\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfromimage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimnew\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/gui/.virtualenvs/gui3/lib/python3.6/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mresize\u001b[0;34m(self, size, resample)\u001b[0m\n\u001b[1;32m   1554\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGBa'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGBA'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1556\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1558\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mangle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNEAREST\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcenter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtranslate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class EpsilonGreedyQAgent(object):\n",
    "    def __init__(self, model, epsilon=.1):\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        if random.uniform(0, 1) <= self.epsilon:\n",
    "            return random.choice([NOOP, SHOOT, LEFT, RIGHT])\n",
    "        else:\n",
    "            return self.model.predict(observation[np.newaxis])[0].argmax()\n",
    "\n",
    "\n",
    "N_BATCHES = 600\n",
    "N_BATCHED_EPISODES = 10\n",
    "directory = 'tmp/DoomDefendLine_q_learning'\n",
    "\n",
    "env = create_env(monitor_directory=directory)\n",
    "\n",
    "trainer = DeepQNetworkTrainer(acting_model, target_model, double_q=True)\n",
    "for _ in range(N_BATCHES):\n",
    "    sares = episode_sares(env, EpsilonGreedyQAgent(trainer.acting_model, epsilon=.1), N_BATCHED_EPISODES)\n",
    "    trainer.fit_episodes(sares)\n",
    "\n",
    "# final greedy episodes\n",
    "sares = episode_sares(env, EpsilonGreedyQAgent(trainer.acting_model, epsilon=0), episode_count=1000)\n",
    "\n",
    "env.close()\n",
    "gym.upload(directory, api_key='sk_bNZUvCfkTfabQCoKoKbjFA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
