{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import ppaquette_gym_doom\n",
    "from ppaquette_gym_doom.wrappers import SetResolution, ToDiscrete\n",
    "from gym.wrappers import SkipWrapper\n",
    "from gym import wrappers\n",
    "\n",
    "# (see https://github.com/ppaquette/gym-doom/blob/master/ppaquette_gym_doom/doom_basic.py)\n",
    "def create_env(seed=None):\n",
    "    env_spec = gym.spec('ppaquette/DoomDefendCenter-v0')\n",
    "    env_spec.id = 'DoomDefendCenter-v0'\n",
    "    env = env_spec.make()\n",
    "\n",
    "    if seed is not None:\n",
    "        env.seed(seed)\n",
    "\n",
    "    return SetResolution('200x150')(\n",
    "        SkipWrapper(repeat_count=4)(\n",
    "        ToDiscrete('minimal')(env)))\n",
    "\n",
    "env = create_env()\n",
    "WIDTH, HEIGHT = env.screen_width, env.screen_height\n",
    "\n",
    "NOOP, SHOOT, RIGHT, LEFT = 0, 1, 2, 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collecting experiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import operator\n",
    "\n",
    "SARE = namedtuple('SAR', ['state', 'action', 'reward', 'end'])\n",
    "\n",
    "\n",
    "def generate_sares(env, agent, episode_count=100):\n",
    "    reward = 0\n",
    "    done = False\n",
    "\n",
    "    for i in range(episode_count):\n",
    "        observation = env.reset()\n",
    "        while True:\n",
    "            action = agent.act(observation, reward, done)\n",
    "            new_observation, reward, done, _ = env.step(action)\n",
    "            yield SARE(observation, action, reward, done)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "            else:\n",
    "                observation = new_observation\n",
    "\n",
    "def episode_sares(env, agent, episode_count=100):\n",
    "    sares = list(generate_sares(env, agent, episode_count))\n",
    "    print('average reward per episode = {}'.format(\n",
    "        sum(r for _, _, r, _ in sares) / float(sum(e for _, _, _, e in sares))))\n",
    "    return sares\n",
    "\n",
    "\n",
    "def to_experiences(sares, only_n_misses=100):\n",
    "    experiences = [\n",
    "        (previous_s, a, r, next_s, end)\n",
    "        for (previous_s, a, r, end), (next_s, _, _, _) in zip(sares[:-1], sares[1:])\n",
    "    ]\n",
    "\n",
    "    # simplistic experience prioritization\n",
    "    shuffled_exps = experiences if only_n_misses is None\\\n",
    "        else random.choices(experiences, k=only_n_misses) + [e for e in experiences if e[2] > 0]\n",
    "    random.shuffle(shuffled_exps)\n",
    "\n",
    "    prev_frames, actions, rewards, next_frames, is_ends = zip(*shuffled_exps)\n",
    "    prev_frames = np.asarray(prev_frames)\n",
    "    next_frames = np.asarray(next_frames)\n",
    "    actions = np.asarray(actions)\n",
    "    rewards = np.asarray(rewards)\n",
    "    is_ends = np.asarray(is_ends)\n",
    "\n",
    "    print('Training on {}/{} positive/total out of {} 1-step experiences with actions distribution {}'.format(\n",
    "        np.sum(rewards>=0),\n",
    "        len(rewards),\n",
    "        len(experiences),\n",
    "        np.bincount(actions)))\n",
    "    \n",
    "    return (prev_frames, next_frames, actions, rewards, is_ends)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.layers import Dense, Convolution2D, Flatten, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "K.set_session(sess)\n",
    "\n",
    "def create_q_model(conv1_weights=None, conv2_weights=None, dense1_weights=None, dense2_weights=None):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(\n",
    "        Convolution2D(nb_filter=4, nb_row=6, nb_col=6, subsample=(1, 1), border_mode='valid', weights=conv1_weights,\n",
    "            input_shape=[HEIGHT, WIDTH, 3], dim_ordering='tf'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(\n",
    "        Convolution2D(nb_filter=8, nb_row=2, nb_col=2, weights=conv2_weights))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, init='normal', weights=dense1_weights))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(4, init='normal', weights=dense2_weights))\n",
    "    model.compile(loss='mse', optimizer=Adam())\n",
    "    \n",
    "    return model\n",
    "\n",
    "acting_model = create_q_model()\n",
    "target_model = create_q_model()\n",
    "\n",
    "def copy_model(model):\n",
    "    conv1_weights =  [w.eval() for w in model.layers[0].weights]\n",
    "    conv2_weights = [w.eval() for w in model.layers[2].weights]\n",
    "    dense1_weights = [w.eval() for w in model.layers[5].weights]\n",
    "    dense2_weights = [w.eval() for w in model.layers[7].weights]\n",
    "    return create_q_model(conv1_weights, conv2_weights, dense1_weights, dense2_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sares_to_input_targets(model, sares, gamma = .99, reward_clip=5, only_n_misses=100):\n",
    "    \n",
    "    prev_frames, next_frames, actions, rewards, is_ends = to_experiences(sares, only_n_misses)\n",
    "    \n",
    "    n_samples = len(actions)\n",
    "    clipped_rewards = np.clip(rewards, -np.inf, reward_clip)\n",
    "    \n",
    "    # Transcription of the Q-learning target formula\n",
    "    targets = clipped_rewards + gamma * (1 - is_ends) * model.predict(next_frames).max(axis=1)\n",
    "\n",
    "    target_action_rewards = model.predict(prev_frames)\n",
    "    target_action_rewards[np.arange(n_samples), actions] = targets\n",
    "\n",
    "    return prev_frames, target_action_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 18:13:42,554] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-03-04 18:13:42,555] Clearing 22 monitor files from previous run (because force=True was provided)\n",
      "[2017-03-04 18:13:42,990] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/DoomDefendLine_q_learning/openaigym.video.0.52850.video000000.mp4\n",
      "[2017-03-04 18:13:44,139] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/DoomDefendLine_q_learning/openaigym.video.0.52850.video000001.mp4\n",
      "[2017-03-04 18:13:49,037] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/DoomDefendLine_q_learning/openaigym.video.0.52850.video000008.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = 1.4\n",
      "Training on 220/224 positive/total out of 666 1-step experiences with actions distribution [  2 159  54   9]\n",
      "average reward per episode = 0.3\n",
      "Training on 208/213 positive/total out of 571 1-step experiences with actions distribution [  3 198   5   7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 18:14:22,767] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/DoomDefendLine_q_learning/openaigym.video.0.52850.video000027.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = 0.1\n",
      "Training on 211/211 positive/total out of 569 1-step experiences with actions distribution [  1 198   7   5]\n",
      "average reward per episode = 0.0\n",
      "Training on 207/210 positive/total out of 593 1-step experiences with actions distribution [  4 201   3   2]\n",
      "average reward per episode = 0.4\n",
      "Training on 211/214 positive/total out of 633 1-step experiences with actions distribution [  7 197   7   3]\n",
      "average reward per episode = 0.5\n",
      "Training on 213/215 positive/total out of 626 1-step experiences with actions distribution [  9 196   6   4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 18:15:27,529] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/DoomDefendLine_q_learning/openaigym.video.0.52850.video000064.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = 0.4\n",
      "Training on 212/214 positive/total out of 632 1-step experiences with actions distribution [  3 203   6   2]\n",
      "average reward per episode = 0.2\n",
      "Training on 208/212 positive/total out of 594 1-step experiences with actions distribution [  6 196   6   4]\n",
      "average reward per episode = 0.5\n",
      "Training on 210/214 positive/total out of 635 1-step experiences with actions distribution [  0 204   5   5]\n",
      "average reward per episode = 0.3\n",
      "Training on 208/213 positive/total out of 588 1-step experiences with actions distribution [  5 196   9   3]\n",
      "average reward per episode = 0.3\n",
      "Training on 212/213 positive/total out of 587 1-step experiences with actions distribution [ 10 192   5   6]\n",
      "average reward per episode = 0.2\n",
      "Training on 210/212 positive/total out of 558 1-step experiences with actions distribution [  1 198   6   7]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 18:17:05,928] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/DoomDefendLine_q_learning/openaigym.video.0.52850.video000125.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = 0.7\n",
      "Training on 214/217 positive/total out of 588 1-step experiences with actions distribution [  4 209   3   1]\n",
      "average reward per episode = 0.3\n",
      "Training on 207/213 positive/total out of 538 1-step experiences with actions distribution [  6 196   8   3]\n",
      "average reward per episode = 0.3\n",
      "Training on 210/213 positive/total out of 598 1-step experiences with actions distribution [  7 197   2   7]\n",
      "average reward per episode = 0.6\n",
      "Training on 213/216 positive/total out of 658 1-step experiences with actions distribution [  5 199   9   3]\n",
      "average reward per episode = 0.0\n",
      "Training on 207/210 positive/total out of 522 1-step experiences with actions distribution [  4 192   7   7]\n",
      "average reward per episode = 0.3\n",
      "Training on 209/213 positive/total out of 584 1-step experiences with actions distribution [  8 203   1   1]\n",
      "average reward per episode = 0.4\n",
      "Training on 210/214 positive/total out of 621 1-step experiences with actions distribution [  7 197   8   2]\n",
      "average reward per episode = 0.2\n",
      "Training on 212/212 positive/total out of 559 1-step experiences with actions distribution [  3 198   5   6]\n",
      "average reward per episode = 0.4\n",
      "Training on 208/214 positive/total out of 628 1-step experiences with actions distribution [  3 201   7   3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 18:19:34,647] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/DoomDefendLine_q_learning/openaigym.video.0.52850.video000216.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = 0.7\n",
      "Training on 215/217 positive/total out of 624 1-step experiences with actions distribution [  6 204   4   3]\n",
      "average reward per episode = 0.7\n",
      "Training on 213/217 positive/total out of 611 1-step experiences with actions distribution [  7 200   5   5]\n",
      "average reward per episode = 0.2\n",
      "Training on 206/212 positive/total out of 586 1-step experiences with actions distribution [  0 204   4   4]\n",
      "average reward per episode = 0.5\n",
      "Training on 213/215 positive/total out of 613 1-step experiences with actions distribution [  6 191  13   5]\n",
      "average reward per episode = 0.5\n",
      "Training on 210/215 positive/total out of 618 1-step experiences with actions distribution [  3 198  10   4]\n",
      "average reward per episode = 0.3\n",
      "Training on 207/213 positive/total out of 625 1-step experiences with actions distribution [  1 205   4   3]\n",
      "average reward per episode = 0.3\n",
      "Training on 211/213 positive/total out of 581 1-step experiences with actions distribution [  5 196   4   8]\n",
      "average reward per episode = 0.3\n",
      "Training on 209/213 positive/total out of 585 1-step experiences with actions distribution [  6 189  11   7]\n",
      "average reward per episode = 0.6\n",
      "Training on 213/216 positive/total out of 659 1-step experiences with actions distribution [  5 198   9   4]\n",
      "average reward per episode = 0.2\n",
      "Training on 211/212 positive/total out of 544 1-step experiences with actions distribution [  7 187  11   7]\n",
      "average reward per episode = 0.7\n",
      "Training on 215/217 positive/total out of 600 1-step experiences with actions distribution [  6 196  12   3]\n",
      "average reward per episode = 0.4\n",
      "Training on 212/214 positive/total out of 601 1-step experiences with actions distribution [ 10 183  18   3]\n",
      "average reward per episode = 0.3\n",
      "Training on 212/213 positive/total out of 551 1-step experiences with actions distribution [  7 178  19   9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 18:23:08,483] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/DoomDefendLine_q_learning/openaigym.video.0.52850.video000343.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = 0.3\n",
      "Training on 207/213 positive/total out of 593 1-step experiences with actions distribution [  8 191  14]\n",
      "average reward per episode = 0.0\n",
      "Training on 209/210 positive/total out of 614 1-step experiences with actions distribution [  3 185  15   7]\n",
      "average reward per episode = 0.6\n",
      "Training on 216/216 positive/total out of 592 1-step experiences with actions distribution [  6 184  23   3]\n",
      "average reward per episode = 0.5\n",
      "Training on 214/215 positive/total out of 619 1-step experiences with actions distribution [  3 195  11   6]\n",
      "average reward per episode = 0.6\n",
      "Training on 212/216 positive/total out of 652 1-step experiences with actions distribution [  7 175  30   4]\n",
      "average reward per episode = 0.6\n",
      "Training on 212/216 positive/total out of 641 1-step experiences with actions distribution [  6 178  25   7]\n",
      "average reward per episode = 1.3\n",
      "Training on 219/223 positive/total out of 624 1-step experiences with actions distribution [  3 192  22   6]\n",
      "average reward per episode = 1.0\n",
      "Training on 215/220 positive/total out of 607 1-step experiences with actions distribution [  1 182  29   8]\n",
      "average reward per episode = 0.8\n",
      "Training on 218/218 positive/total out of 620 1-step experiences with actions distribution [  2 189  24   3]\n",
      "average reward per episode = 1.1\n",
      "Training on 217/220 positive/total out of 634 1-step experiences with actions distribution [  5 172  38   5]\n",
      "average reward per episode = 0.5\n",
      "Training on 213/215 positive/total out of 570 1-step experiences with actions distribution [  4 175  30   6]\n",
      "average reward per episode = 0.7\n",
      "Training on 213/217 positive/total out of 644 1-step experiences with actions distribution [  3 168  40   6]\n",
      "average reward per episode = 0.5\n",
      "Training on 210/214 positive/total out of 553 1-step experiences with actions distribution [  3 173  37   1]\n",
      "average reward per episode = 1.1\n",
      "Training on 218/220 positive/total out of 620 1-step experiences with actions distribution [  5 169  40   6]\n",
      "average reward per episode = 0.3\n",
      "Training on 211/213 positive/total out of 630 1-step experiences with actions distribution [  6 169  28  10]\n",
      "average reward per episode = 0.2\n",
      "Training on 206/211 positive/total out of 546 1-step experiences with actions distribution [ 10 170  26   5]\n",
      "average reward per episode = 1.0\n",
      "Training on 217/220 positive/total out of 634 1-step experiences with actions distribution [  3 160  54   3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 18:27:57,053] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/DoomDefendLine_q_learning/openaigym.video.0.52850.video000512.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = 1.1\n",
      "Training on 219/221 positive/total out of 597 1-step experiences with actions distribution [  6 169  41   5]\n",
      "average reward per episode = 0.8\n",
      "Training on 218/218 positive/total out of 595 1-step experiences with actions distribution [  5 164  48   1]\n",
      "average reward per episode = 0.5\n",
      "Training on 213/215 positive/total out of 645 1-step experiences with actions distribution [  3 152  55   5]\n",
      "average reward per episode = 0.5\n",
      "Training on 213/215 positive/total out of 624 1-step experiences with actions distribution [  5 155  46   9]\n",
      "average reward per episode = 0.9\n",
      "Training on 214/218 positive/total out of 611 1-step experiences with actions distribution [  3 161  51   3]\n",
      "average reward per episode = 1.0\n",
      "Training on 215/220 positive/total out of 683 1-step experiences with actions distribution [  8 145  57  10]\n",
      "average reward per episode = 0.3\n",
      "Training on 207/213 positive/total out of 639 1-step experiences with actions distribution [  6 141  58   8]\n",
      "average reward per episode = 0.0\n",
      "Training on 207/210 positive/total out of 616 1-step experiences with actions distribution [  3 134  65   8]\n",
      "average reward per episode = 0.5\n",
      "Training on 213/215 positive/total out of 716 1-step experiences with actions distribution [  6 154  52   3]\n",
      "average reward per episode = 1.3\n",
      "Training on 221/223 positive/total out of 766 1-step experiences with actions distribution [  8 131  81   3]\n",
      "average reward per episode = 0.0\n",
      "Training on 208/210 positive/total out of 586 1-step experiences with actions distribution [  5 148  53   4]\n",
      "average reward per episode = 1.0\n",
      "Training on 219/220 positive/total out of 726 1-step experiences with actions distribution [  2 152  59   7]\n",
      "average reward per episode = 0.8\n",
      "Training on 213/218 positive/total out of 753 1-step experiences with actions distribution [  9 134  71   4]\n",
      "average reward per episode = 1.0\n",
      "Training on 217/220 positive/total out of 712 1-step experiences with actions distribution [  3 152  59   6]\n",
      "average reward per episode = -0.1\n",
      "Training on 207/209 positive/total out of 592 1-step experiences with actions distribution [  5 147  51   6]\n",
      "average reward per episode = 0.2\n",
      "Training on 208/212 positive/total out of 591 1-step experiences with actions distribution [  7 139  57   9]\n",
      "average reward per episode = 0.7\n",
      "Training on 214/216 positive/total out of 732 1-step experiences with actions distribution [  3 133  70  10]\n",
      "average reward per episode = 0.5\n",
      "Training on 211/215 positive/total out of 659 1-step experiences with actions distribution [  8 135  71   1]\n",
      "average reward per episode = 0.2\n",
      "Training on 209/212 positive/total out of 642 1-step experiences with actions distribution [  4 143  63   2]\n",
      "average reward per episode = 0.9\n",
      "Training on 215/219 positive/total out of 680 1-step experiences with actions distribution [  4 146  63   6]\n",
      "average reward per episode = 0.4\n",
      "Training on 211/213 positive/total out of 623 1-step experiences with actions distribution [  4 141  65   3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 18:34:15,083] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/DoomDefendLine_q_learning/openaigym.video.0.52850.video000729.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = 1.1\n",
      "Training on 221/221 positive/total out of 776 1-step experiences with actions distribution [  5 140  73   3]\n",
      "average reward per episode = 1.5\n",
      "Training on 224/225 positive/total out of 757 1-step experiences with actions distribution [  6 134  79   6]\n",
      "average reward per episode = 0.9\n",
      "Training on 216/219 positive/total out of 697 1-step experiences with actions distribution [  4 133  78   4]\n",
      "average reward per episode = 0.5\n",
      "Training on 213/215 positive/total out of 672 1-step experiences with actions distribution [  3 132  74   6]\n",
      "average reward per episode = 1.8\n",
      "Training on 227/228 positive/total out of 821 1-step experiences with actions distribution [  6 123  94   5]\n",
      "average reward per episode = 1.6\n",
      "Training on 222/225 positive/total out of 776 1-step experiences with actions distribution [  7 131  86   1]\n",
      "average reward per episode = 1.0\n",
      "Training on 220/220 positive/total out of 757 1-step experiences with actions distribution [  1 117  96   6]\n",
      "average reward per episode = 1.6\n",
      "Training on 223/225 positive/total out of 807 1-step experiences with actions distribution [  1  58 164   2]\n",
      "average reward per episode = 1.3\n",
      "Training on 220/223 positive/total out of 730 1-step experiences with actions distribution [  5 128  86   4]\n",
      "average reward per episode = 1.4\n",
      "Training on 223/224 positive/total out of 770 1-step experiences with actions distribution [  5  85 128   6]\n",
      "average reward per episode = 1.0\n",
      "Training on 220/220 positive/total out of 760 1-step experiences with actions distribution [  3 119  88  10]\n",
      "average reward per episode = 1.5\n",
      "Training on 223/225 positive/total out of 816 1-step experiences with actions distribution [  2 113 105   5]\n",
      "average reward per episode = 1.0\n",
      "Training on 219/220 positive/total out of 768 1-step experiences with actions distribution [  7 115  95   3]\n",
      "average reward per episode = 1.5\n",
      "Training on 224/225 positive/total out of 745 1-step experiences with actions distribution [  7 138  74   6]\n",
      "average reward per episode = 1.2\n",
      "Training on 221/222 positive/total out of 783 1-step experiences with actions distribution [  8 120  89   5]\n",
      "average reward per episode = 1.6\n",
      "Training on 225/226 positive/total out of 831 1-step experiences with actions distribution [ 10 121  94   1]\n",
      "average reward per episode = 2.0\n",
      "Training on 230/230 positive/total out of 852 1-step experiences with actions distribution [  7 121  96   6]\n",
      "average reward per episode = 1.5\n",
      "Training on 221/225 positive/total out of 800 1-step experiences with actions distribution [  2 113 100  10]\n",
      "average reward per episode = 1.0\n",
      "Training on 218/220 positive/total out of 783 1-step experiences with actions distribution [ 11 113  90   6]\n",
      "average reward per episode = 1.5\n",
      "Training on 224/225 positive/total out of 781 1-step experiences with actions distribution [  4 101 112   8]\n",
      "average reward per episode = 1.8\n",
      "Training on 226/228 positive/total out of 814 1-step experiences with actions distribution [  3  97 121   7]\n",
      "average reward per episode = 0.9\n",
      "Training on 215/219 positive/total out of 772 1-step experiences with actions distribution [  4  92 119   4]\n",
      "average reward per episode = 1.7\n",
      "Training on 224/227 positive/total out of 843 1-step experiences with actions distribution [  2 111 111   3]\n",
      "average reward per episode = 1.4\n",
      "Training on 224/224 positive/total out of 835 1-step experiences with actions distribution [  3 127  91   3]\n",
      "average reward per episode = 1.6\n",
      "Training on 223/226 positive/total out of 763 1-step experiences with actions distribution [  9 123  87   7]\n",
      "average reward per episode = 1.2\n",
      "Training on 219/222 positive/total out of 775 1-step experiences with actions distribution [  6  98 114   4]\n",
      "average reward per episode = 0.8\n",
      "Training on 215/218 positive/total out of 753 1-step experiences with actions distribution [ 10 115  89   4]\n",
      "average reward per episode = 1.4\n",
      "Training on 222/224 positive/total out of 752 1-step experiences with actions distribution [  8  92 119   5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 18:43:13,325] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/DoomDefendLine_q_learning/openaigym.video.0.52850.video001000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = 1.1\n",
      "Training on 216/221 positive/total out of 726 1-step experiences with actions distribution [  1 113 102   5]\n",
      "average reward per episode = 0.6\n",
      "Training on 213/216 positive/total out of 670 1-step experiences with actions distribution [  3 130  77   6]\n",
      "average reward per episode = 1.2\n",
      "Training on 219/222 positive/total out of 687 1-step experiences with actions distribution [  2 118  99   3]\n",
      "average reward per episode = 1.2\n",
      "Training on 218/221 positive/total out of 720 1-step experiences with actions distribution [  9 115  94   3]\n",
      "average reward per episode = 1.1\n",
      "Training on 220/221 positive/total out of 787 1-step experiences with actions distribution [  5 122  89   5]\n",
      "average reward per episode = 0.9\n",
      "Training on 219/219 positive/total out of 668 1-step experiences with actions distribution [  5 130  78   6]\n",
      "average reward per episode = 0.7\n",
      "Training on 214/217 positive/total out of 728 1-step experiences with actions distribution [  5 134  67  11]\n",
      "average reward per episode = 1.4\n",
      "Training on 223/224 positive/total out of 747 1-step experiences with actions distribution [  7 131  81   5]\n",
      "average reward per episode = 0.8\n",
      "Training on 215/218 positive/total out of 741 1-step experiences with actions distribution [  2 106 105   5]\n",
      "average reward per episode = 0.1\n",
      "Training on 209/211 positive/total out of 646 1-step experiences with actions distribution [  3 134  69   5]\n",
      "average reward per episode = 0.5\n",
      "Training on 213/215 positive/total out of 676 1-step experiences with actions distribution [  3 118  82  12]\n",
      "average reward per episode = 0.9\n",
      "Training on 219/219 positive/total out of 728 1-step experiences with actions distribution [  6 111  94   8]\n",
      "average reward per episode = 1.2\n",
      "Training on 220/222 positive/total out of 755 1-step experiences with actions distribution [  4 127  85   6]\n",
      "average reward per episode = 1.1\n",
      "Training on 219/221 positive/total out of 763 1-step experiences with actions distribution [  6 112  95   8]\n",
      "average reward per episode = 1.5\n",
      "Training on 224/225 positive/total out of 804 1-step experiences with actions distribution [  5 112 107   1]\n",
      "average reward per episode = 1.3\n",
      "Training on 222/223 positive/total out of 775 1-step experiences with actions distribution [  8  98 115   2]\n",
      "average reward per episode = 1.2\n",
      "Training on 221/222 positive/total out of 813 1-step experiences with actions distribution [  5 104 105   8]\n",
      "average reward per episode = 1.7\n",
      "Training on 226/227 positive/total out of 788 1-step experiences with actions distribution [  7 107 111   2]\n",
      "average reward per episode = 0.1\n",
      "Training on 208/211 positive/total out of 691 1-step experiences with actions distribution [  6  99 101   5]\n",
      "average reward per episode = 1.7\n",
      "Training on 224/227 positive/total out of 782 1-step experiences with actions distribution [  4  97 124   2]\n",
      "average reward per episode = 1.2\n",
      "Training on 219/222 positive/total out of 771 1-step experiences with actions distribution [  2 113 101   6]\n",
      "average reward per episode = 1.8\n",
      "Training on 225/228 positive/total out of 888 1-step experiences with actions distribution [  5 112 105   6]\n",
      "average reward per episode = 0.9\n",
      "Training on 217/219 positive/total out of 768 1-step experiences with actions distribution [  6 127  82   4]\n",
      "average reward per episode = 1.5\n",
      "Training on 224/225 positive/total out of 819 1-step experiences with actions distribution [  4 102 111   8]\n",
      "average reward per episode = 1.1\n",
      "Training on 220/221 positive/total out of 733 1-step experiences with actions distribution [  6 123  89   3]\n",
      "average reward per episode = 1.9\n",
      "Training on 227/229 positive/total out of 833 1-step experiences with actions distribution [  2 123  98   6]\n",
      "average reward per episode = 0.8\n",
      "Training on 214/218 positive/total out of 722 1-step experiences with actions distribution [  7 129  79   3]\n",
      "average reward per episode = 0.3\n",
      "Training on 211/213 positive/total out of 683 1-step experiences with actions distribution [  4 115  90   4]\n",
      "average reward per episode = 1.0\n",
      "Training on 218/220 positive/total out of 736 1-step experiences with actions distribution [  2 119  89  10]\n",
      "average reward per episode = 0.4\n",
      "Training on 213/214 positive/total out of 681 1-step experiences with actions distribution [  8 119  84   3]\n",
      "average reward per episode = 0.4\n",
      "Training on 212/214 positive/total out of 631 1-step experiences with actions distribution [ 10 125  75   4]\n",
      "average reward per episode = 0.6\n",
      "Training on 214/216 positive/total out of 675 1-step experiences with actions distribution [  6 125  76   9]\n",
      "average reward per episode = 1.1\n",
      "Training on 218/221 positive/total out of 723 1-step experiences with actions distribution [  2  97 115   7]\n",
      "average reward per episode = 1.1\n",
      "Training on 219/221 positive/total out of 755 1-step experiences with actions distribution [ 11 119  88   3]\n",
      "average reward per episode = 1.9\n",
      "Training on 223/229 positive/total out of 813 1-step experiences with actions distribution [  5 122  97   5]\n",
      "average reward per episode = 1.7\n",
      "Training on 226/227 positive/total out of 806 1-step experiences with actions distribution [  7 123  93   4]\n",
      "average reward per episode = 0.7\n",
      "Training on 216/217 positive/total out of 721 1-step experiences with actions distribution [  5 100 106   6]\n",
      "average reward per episode = 1.3\n",
      "Training on 223/223 positive/total out of 772 1-step experiences with actions distribution [  6 116  96   5]\n",
      "average reward per episode = 2.1\n",
      "Training on 230/231 positive/total out of 808 1-step experiences with actions distribution [  7 115 104   5]\n",
      "average reward per episode = 2.3\n",
      "Training on 232/232 positive/total out of 873 1-step experiences with actions distribution [  4 115 105   8]\n",
      "average reward per episode = 1.0\n",
      "Training on 220/220 positive/total out of 765 1-step experiences with actions distribution [  7 111  96   6]\n",
      "average reward per episode = 2.1\n",
      "Training on 228/231 positive/total out of 865 1-step experiences with actions distribution [  5 123 100   3]\n",
      "average reward per episode = 1.9\n",
      "Training on 229/229 positive/total out of 770 1-step experiences with actions distribution [  2 129  85  13]\n",
      "average reward per episode = 1.5\n",
      "Training on 223/225 positive/total out of 822 1-step experiences with actions distribution [  2 115 101   7]\n",
      "average reward per episode = 0.4\n",
      "Training on 212/214 positive/total out of 705 1-step experiences with actions distribution [  5  79 126   4]\n",
      "average reward per episode = 1.1\n",
      "Training on 216/221 positive/total out of 793 1-step experiences with actions distribution [  4 105 109   3]\n",
      "average reward per episode = 1.7\n",
      "Training on 225/227 positive/total out of 833 1-step experiences with actions distribution [  5 115 100   7]\n",
      "average reward per episode = 2.0\n",
      "Training on 229/230 positive/total out of 828 1-step experiences with actions distribution [  6 113 105   6]\n",
      "average reward per episode = 0.4\n",
      "Training on 210/214 positive/total out of 704 1-step experiences with actions distribution [  4  83 121   6]\n",
      "average reward per episode = 1.1\n",
      "Training on 218/221 positive/total out of 769 1-step experiences with actions distribution [  1 104 111   5]\n",
      "average reward per episode = 1.7\n",
      "Training on 226/227 positive/total out of 833 1-step experiences with actions distribution [  1  97 125   4]\n",
      "average reward per episode = 1.2\n",
      "Training on 220/222 positive/total out of 728 1-step experiences with actions distribution [  3  96 117   6]\n",
      "average reward per episode = 1.5\n",
      "Training on 221/225 positive/total out of 818 1-step experiences with actions distribution [  9 100 111   5]\n",
      "average reward per episode = 1.7\n",
      "Training on 226/227 positive/total out of 804 1-step experiences with actions distribution [  9  92 122   4]\n",
      "average reward per episode = 1.2\n",
      "Training on 222/222 positive/total out of 801 1-step experiences with actions distribution [  6  92 115   9]\n",
      "average reward per episode = 1.8\n",
      "Training on 225/228 positive/total out of 812 1-step experiences with actions distribution [  3 112 112   1]\n",
      "average reward per episode = 2.2\n",
      "Training on 227/232 positive/total out of 864 1-step experiences with actions distribution [  8 114 100  10]\n",
      "average reward per episode = 2.6\n",
      "Training on 235/236 positive/total out of 887 1-step experiences with actions distribution [  8 119 102   7]\n",
      "average reward per episode = 2.3\n",
      "Training on 231/233 positive/total out of 861 1-step experiences with actions distribution [  9 108 113   3]\n",
      "average reward per episode = 2.2\n",
      "Training on 230/232 positive/total out of 871 1-step experiences with actions distribution [  6 112  99  15]\n",
      "average reward per episode = 1.8\n",
      "Training on 226/228 positive/total out of 816 1-step experiences with actions distribution [  6 103 114   5]\n",
      "average reward per episode = 3.1\n",
      "Training on 240/240 positive/total out of 854 1-step experiences with actions distribution [  2 121 112   5]\n",
      "average reward per episode = 2.7\n",
      "Training on 236/237 positive/total out of 862 1-step experiences with actions distribution [  3 117 113   4]\n",
      "average reward per episode = 2.3\n",
      "Training on 230/232 positive/total out of 767 1-step experiences with actions distribution [  8 114 104   6]\n",
      "average reward per episode = 2.6\n",
      "Training on 231/236 positive/total out of 871 1-step experiences with actions distribution [  6 118 106   6]\n",
      "average reward per episode = 2.0\n",
      "Training on 229/230 positive/total out of 832 1-step experiences with actions distribution [  7 111 105   7]\n",
      "average reward per episode = 2.7\n",
      "Training on 236/237 positive/total out of 847 1-step experiences with actions distribution [  9  98 126   4]\n",
      "average reward per episode = 1.7\n",
      "Training on 224/227 positive/total out of 804 1-step experiences with actions distribution [  7 114 101   5]\n",
      "average reward per episode = 1.8\n",
      "Training on 227/228 positive/total out of 786 1-step experiences with actions distribution [  4 100 119   5]\n",
      "average reward per episode = 2.2\n",
      "Training on 230/232 positive/total out of 837 1-step experiences with actions distribution [  2 110 118   2]\n",
      "average reward per episode = 1.7\n",
      "Training on 225/227 positive/total out of 796 1-step experiences with actions distribution [ 10  85 121  11]\n",
      "average reward per episode = 3.0\n",
      "Training on 238/240 positive/total out of 838 1-step experiences with actions distribution [  1 125 109   5]\n",
      "average reward per episode = 1.9\n",
      "Training on 228/229 positive/total out of 753 1-step experiences with actions distribution [  6 114 105   4]\n",
      "average reward per episode = 2.3\n",
      "Training on 233/233 positive/total out of 771 1-step experiences with actions distribution [  6 109 113   5]\n",
      "average reward per episode = 2.1\n",
      "Training on 228/230 positive/total out of 775 1-step experiences with actions distribution [  3 108 115   4]\n",
      "average reward per episode = 1.7\n",
      "Training on 224/227 positive/total out of 675 1-step experiences with actions distribution [  5 112 106   4]\n",
      "average reward per episode = 2.5\n",
      "Training on 231/235 positive/total out of 754 1-step experiences with actions distribution [  4 144  82   5]\n",
      "average reward per episode = 3.6\n",
      "Training on 244/246 positive/total out of 876 1-step experiences with actions distribution [ 10 128 105   3]\n",
      "average reward per episode = 2.1\n",
      "Training on 229/231 positive/total out of 756 1-step experiences with actions distribution [  6 120 102   3]\n",
      "average reward per episode = 3.0\n",
      "Training on 238/240 positive/total out of 869 1-step experiences with actions distribution [  5 129 103   3]\n",
      "average reward per episode = 1.6\n",
      "Training on 222/226 positive/total out of 620 1-step experiences with actions distribution [  6 143  74   3]\n",
      "average reward per episode = 1.9\n",
      "Training on 227/229 positive/total out of 757 1-step experiences with actions distribution [  8 136  80   5]\n",
      "average reward per episode = 2.6\n",
      "Training on 230/234 positive/total out of 806 1-step experiences with actions distribution [  6 115 108   5]\n",
      "average reward per episode = 2.5\n",
      "Training on 234/235 positive/total out of 845 1-step experiences with actions distribution [  4 143  86   2]\n",
      "average reward per episode = 2.5\n",
      "Training on 233/235 positive/total out of 789 1-step experiences with actions distribution [  5 135  88   7]\n",
      "average reward per episode = 1.1\n",
      "Training on 217/221 positive/total out of 660 1-step experiences with actions distribution [  4 135  71  11]\n",
      "average reward per episode = 2.0\n",
      "Training on 225/230 positive/total out of 730 1-step experiences with actions distribution [  4 145  77   4]\n",
      "average reward per episode = 2.9\n",
      "Training on 237/239 positive/total out of 856 1-step experiences with actions distribution [  6 127 102   4]\n",
      "average reward per episode = 2.2\n",
      "Training on 231/232 positive/total out of 783 1-step experiences with actions distribution [  5 100 123   4]\n",
      "average reward per episode = 2.5\n",
      "Training on 233/235 positive/total out of 786 1-step experiences with actions distribution [  3 129 101   2]\n",
      "average reward per episode = 1.7\n",
      "Training on 227/227 positive/total out of 684 1-step experiences with actions distribution [ 10 113  97   7]\n",
      "average reward per episode = 3.2\n",
      "Training on 240/242 positive/total out of 837 1-step experiences with actions distribution [  4 131 102   5]\n",
      "average reward per episode = 1.5\n",
      "Training on 221/225 positive/total out of 702 1-step experiences with actions distribution [  3 109 108   5]\n",
      "average reward per episode = 2.2\n",
      "Training on 229/232 positive/total out of 764 1-step experiences with actions distribution [  3 123 104   2]\n",
      "average reward per episode = 2.2\n",
      "Training on 229/232 positive/total out of 757 1-step experiences with actions distribution [  7 120 102   3]\n",
      "average reward per episode = 2.5\n",
      "Training on 235/235 positive/total out of 732 1-step experiences with actions distribution [  4 144  85   2]\n",
      "average reward per episode = 2.2\n",
      "Training on 231/232 positive/total out of 774 1-step experiences with actions distribution [ 10 137  79   6]\n",
      "average reward per episode = 3.5\n",
      "Training on 239/243 positive/total out of 876 1-step experiences with actions distribution [  7 112 116   8]\n",
      "average reward per episode = 1.6\n",
      "Training on 224/225 positive/total out of 707 1-step experiences with actions distribution [  4 132  86   3]\n",
      "average reward per episode = 2.6\n",
      "Training on 232/235 positive/total out of 739 1-step experiences with actions distribution [  6 158  62   9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 19:07:56,006] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/DoomDefendLine_q_learning/openaigym.video.0.52850.video002000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = 1.3\n",
      "Training on 221/223 positive/total out of 615 1-step experiences with actions distribution [  6 140  72   5]\n",
      "average reward per episode = 2.2\n",
      "Training on 231/232 positive/total out of 788 1-step experiences with actions distribution [  3 133  88   8]\n",
      "average reward per episode = 2.8\n",
      "Training on 238/238 positive/total out of 826 1-step experiences with actions distribution [ 11 141  78   8]\n",
      "average reward per episode = 0.9\n",
      "Training on 214/219 positive/total out of 569 1-step experiences with actions distribution [  2 142  68   7]\n",
      "average reward per episode = 2.5\n",
      "Training on 229/234 positive/total out of 794 1-step experiences with actions distribution [  2 151  79   2]\n",
      "average reward per episode = 2.0\n",
      "Training on 228/230 positive/total out of 724 1-step experiences with actions distribution [  6 121  93  10]\n",
      "average reward per episode = 1.8\n",
      "Training on 225/228 positive/total out of 744 1-step experiences with actions distribution [  1 151  69   7]\n",
      "average reward per episode = 1.5\n",
      "Training on 220/225 positive/total out of 688 1-step experiences with actions distribution [  7 129  80   9]\n",
      "average reward per episode = 1.7\n",
      "Training on 224/227 positive/total out of 679 1-step experiences with actions distribution [  7 136  76   8]\n",
      "average reward per episode = 2.3\n",
      "Training on 231/233 positive/total out of 781 1-step experiences with actions distribution [  7 133  89   4]\n",
      "average reward per episode = 3.1\n",
      "Training on 240/241 positive/total out of 805 1-step experiences with actions distribution [  3 148  87   3]\n",
      "average reward per episode = 2.7\n",
      "Training on 234/237 positive/total out of 783 1-step experiences with actions distribution [  8 148  78   3]\n",
      "average reward per episode = 2.2\n",
      "Training on 229/232 positive/total out of 749 1-step experiences with actions distribution [  6 150  75   1]\n",
      "average reward per episode = 2.3\n",
      "Training on 232/233 positive/total out of 826 1-step experiences with actions distribution [  5 119 103   6]\n",
      "average reward per episode = 2.0\n",
      "Training on 225/230 positive/total out of 756 1-step experiences with actions distribution [  7 134  83   6]\n",
      "average reward per episode = 1.9\n",
      "Training on 226/228 positive/total out of 656 1-step experiences with actions distribution [  5 124  89  10]\n",
      "average reward per episode = 0.8\n",
      "Training on 215/218 positive/total out of 580 1-step experiences with actions distribution [  4 152  55   7]\n",
      "average reward per episode = 2.9\n",
      "Training on 238/239 positive/total out of 790 1-step experiences with actions distribution [  4 117 109   9]\n",
      "average reward per episode = 2.2\n",
      "Training on 230/232 positive/total out of 795 1-step experiences with actions distribution [  8 134  86   4]\n",
      "average reward per episode = 2.6\n",
      "Training on 233/236 positive/total out of 771 1-step experiences with actions distribution [  1 138  91   6]\n",
      "average reward per episode = 1.3\n",
      "Training on 221/223 positive/total out of 652 1-step experiences with actions distribution [  6 123  88   6]\n",
      "average reward per episode = 1.2\n",
      "Training on 216/222 positive/total out of 651 1-step experiences with actions distribution [  3 135  74  10]\n",
      "average reward per episode = 1.7\n",
      "Training on 216/227 positive/total out of 706 1-step experiences with actions distribution [ 10 134  81   2]\n",
      "average reward per episode = 2.9\n",
      "Training on 239/239 positive/total out of 798 1-step experiences with actions distribution [  4 128  97  10]\n",
      "average reward per episode = 0.5\n",
      "Training on 213/215 positive/total out of 589 1-step experiences with actions distribution [  4 160  49   2]\n",
      "average reward per episode = 1.6\n",
      "Training on 223/226 positive/total out of 617 1-step experiences with actions distribution [  7 150  65   4]\n",
      "average reward per episode = 2.3\n",
      "Training on 231/233 positive/total out of 816 1-step experiences with actions distribution [ 13 159  55   6]\n",
      "average reward per episode = 1.2\n",
      "Training on 216/222 positive/total out of 681 1-step experiences with actions distribution [  7 140  71   4]\n",
      "average reward per episode = 1.6\n",
      "Training on 218/226 positive/total out of 678 1-step experiences with actions distribution [  5 149  67   5]\n",
      "average reward per episode = 2.2\n",
      "Training on 228/232 positive/total out of 792 1-step experiences with actions distribution [  8 127  95   2]\n",
      "average reward per episode = 1.6\n",
      "Training on 224/226 positive/total out of 679 1-step experiences with actions distribution [  6 139  76   5]\n",
      "average reward per episode = 3.0\n",
      "Training on 238/240 positive/total out of 837 1-step experiences with actions distribution [  5 138  90   7]\n",
      "average reward per episode = 1.0\n",
      "Training on 216/220 positive/total out of 576 1-step experiences with actions distribution [  1 165  46   8]\n",
      "average reward per episode = 1.2\n",
      "Training on 217/222 positive/total out of 623 1-step experiences with actions distribution [  8 150  59   5]\n",
      "average reward per episode = 2.7\n",
      "Training on 234/237 positive/total out of 811 1-step experiences with actions distribution [  4 133  94   6]\n",
      "average reward per episode = 2.1\n",
      "Training on 226/231 positive/total out of 732 1-step experiences with actions distribution [  7 145  73   6]\n",
      "average reward per episode = 2.9\n",
      "Training on 236/239 positive/total out of 806 1-step experiences with actions distribution [  3 142  88   6]\n",
      "average reward per episode = 1.1\n",
      "Training on 217/221 positive/total out of 617 1-step experiences with actions distribution [  9 173  36   3]\n",
      "average reward per episode = 1.3\n",
      "Training on 220/222 positive/total out of 681 1-step experiences with actions distribution [  4 143  70   5]\n",
      "average reward per episode = 1.7\n",
      "Training on 222/227 positive/total out of 722 1-step experiences with actions distribution [  8 140  75   4]\n",
      "average reward per episode = 1.6\n",
      "Training on 224/226 positive/total out of 653 1-step experiences with actions distribution [  4 141  72   9]\n",
      "average reward per episode = 2.7\n",
      "Training on 236/237 positive/total out of 841 1-step experiences with actions distribution [ 13 150  71   3]\n",
      "average reward per episode = 2.8\n",
      "Training on 237/238 positive/total out of 819 1-step experiences with actions distribution [  9 122  98   9]\n",
      "average reward per episode = 3.1\n",
      "Training on 241/241 positive/total out of 926 1-step experiences with actions distribution [ 10 126  97   8]\n",
      "average reward per episode = 2.7\n",
      "Training on 234/237 positive/total out of 776 1-step experiences with actions distribution [  8 146  79   4]\n",
      "average reward per episode = 2.5\n",
      "Training on 230/235 positive/total out of 792 1-step experiences with actions distribution [ 10 137  78  10]\n",
      "average reward per episode = 3.3\n",
      "Training on 242/243 positive/total out of 866 1-step experiences with actions distribution [  3 146  90   4]\n",
      "average reward per episode = 1.5\n",
      "Training on 223/225 positive/total out of 669 1-step experiences with actions distribution [  5 146  72   2]\n",
      "average reward per episode = 3.3\n",
      "Training on 243/243 positive/total out of 903 1-step experiences with actions distribution [  4 137  95   7]\n",
      "average reward per episode = 3.6\n",
      "Training on 245/246 positive/total out of 905 1-step experiences with actions distribution [  4 130 107   5]\n",
      "average reward per episode = 2.6\n",
      "Training on 235/236 positive/total out of 802 1-step experiences with actions distribution [ 11 138  82   5]\n",
      "average reward per episode = 2.5\n",
      "Training on 233/235 positive/total out of 763 1-step experiences with actions distribution [ 11 110 104  10]\n",
      "average reward per episode = 2.6\n",
      "Training on 232/236 positive/total out of 843 1-step experiences with actions distribution [  8 116 111   1]\n",
      "average reward per episode = 2.9\n",
      "Training on 239/239 positive/total out of 856 1-step experiences with actions distribution [  7 122 104   6]\n",
      "average reward per episode = 4.2\n",
      "Training on 251/252 positive/total out of 977 1-step experiences with actions distribution [  4 111 131   6]\n",
      "average reward per episode = 3.0\n",
      "Training on 236/240 positive/total out of 888 1-step experiences with actions distribution [  4 132  99   5]\n",
      "average reward per episode = 3.8\n",
      "Training on 246/248 positive/total out of 877 1-step experiences with actions distribution [  8 139  97   4]\n",
      "average reward per episode = 3.4\n",
      "Training on 243/244 positive/total out of 865 1-step experiences with actions distribution [  8 126 108   2]\n",
      "average reward per episode = 3.4\n",
      "Training on 241/244 positive/total out of 781 1-step experiences with actions distribution [ 11 151  76   6]\n",
      "average reward per episode = 2.7\n",
      "Training on 235/237 positive/total out of 817 1-step experiences with actions distribution [ 10 142  82   3]\n",
      "average reward per episode = 1.9\n",
      "Training on 226/229 positive/total out of 722 1-step experiences with actions distribution [  4 122 101   2]\n",
      "average reward per episode = 2.7\n",
      "Training on 234/237 positive/total out of 826 1-step experiences with actions distribution [  4 106 121   6]\n",
      "average reward per episode = 3.0\n",
      "Training on 240/240 positive/total out of 892 1-step experiences with actions distribution [  1 130 104   5]\n",
      "average reward per episode = 3.0\n",
      "Training on 238/240 positive/total out of 812 1-step experiences with actions distribution [  8 153  76   3]\n",
      "average reward per episode = 3.0\n",
      "Training on 239/240 positive/total out of 840 1-step experiences with actions distribution [  2 121 109   8]\n",
      "average reward per episode = 3.4\n",
      "Training on 240/243 positive/total out of 872 1-step experiences with actions distribution [  6 144  90   3]\n",
      "average reward per episode = 3.5\n",
      "Training on 241/245 positive/total out of 916 1-step experiences with actions distribution [  8 109 121   7]\n",
      "average reward per episode = 4.2\n",
      "Training on 251/252 positive/total out of 1044 1-step experiences with actions distribution [  2 118 127   5]\n",
      "average reward per episode = 3.3\n",
      "Training on 242/243 positive/total out of 889 1-step experiences with actions distribution [  3 132 105   3]\n",
      "average reward per episode = 2.4\n",
      "Training on 233/234 positive/total out of 731 1-step experiences with actions distribution [  6 156  68   4]\n",
      "average reward per episode = 3.0\n",
      "Training on 238/240 positive/total out of 889 1-step experiences with actions distribution [  3 120 111   6]\n",
      "average reward per episode = 3.4\n",
      "Training on 243/244 positive/total out of 940 1-step experiences with actions distribution [  8 108 121   7]\n",
      "average reward per episode = 3.4\n",
      "Training on 243/244 positive/total out of 917 1-step experiences with actions distribution [  5 129 102   8]\n",
      "average reward per episode = 3.8\n",
      "Training on 245/247 positive/total out of 936 1-step experiences with actions distribution [  3 122 120   2]\n",
      "average reward per episode = 3.5\n",
      "Training on 243/244 positive/total out of 915 1-step experiences with actions distribution [  7 122 108   7]\n",
      "average reward per episode = 2.2\n",
      "Training on 232/232 positive/total out of 770 1-step experiences with actions distribution [ 11 128  89   4]\n",
      "average reward per episode = 2.8\n",
      "Training on 237/238 positive/total out of 829 1-step experiences with actions distribution [  6 116 109   7]\n",
      "average reward per episode = 3.3\n",
      "Training on 240/243 positive/total out of 887 1-step experiences with actions distribution [  5 109 124   5]\n",
      "average reward per episode = 3.1\n",
      "Training on 240/241 positive/total out of 801 1-step experiences with actions distribution [  2 152  83   4]\n",
      "average reward per episode = 2.7\n",
      "Training on 235/237 positive/total out of 787 1-step experiences with actions distribution [  6 117 111   3]\n",
      "average reward per episode = 2.2\n",
      "Training on 230/232 positive/total out of 738 1-step experiences with actions distribution [  4 131  85  12]\n",
      "average reward per episode = 1.6\n",
      "Training on 224/226 positive/total out of 645 1-step experiences with actions distribution [  5 162  50   9]\n",
      "average reward per episode = 3.5\n",
      "Training on 245/245 positive/total out of 966 1-step experiences with actions distribution [  6 102 128   9]\n",
      "average reward per episode = 2.6\n",
      "Training on 236/236 positive/total out of 819 1-step experiences with actions distribution [  6 115 109   6]\n",
      "average reward per episode = 2.7\n",
      "Training on 234/237 positive/total out of 828 1-step experiences with actions distribution [  6 106 116   9]\n",
      "average reward per episode = 2.9\n",
      "Training on 236/238 positive/total out of 845 1-step experiences with actions distribution [ 11 120  93  14]\n",
      "average reward per episode = 2.3\n",
      "Training on 231/233 positive/total out of 763 1-step experiences with actions distribution [  2 153  73   5]\n",
      "average reward per episode = 2.8\n",
      "Training on 235/238 positive/total out of 841 1-step experiences with actions distribution [  1 121 105  11]\n",
      "average reward per episode = 2.6\n",
      "Training on 235/236 positive/total out of 858 1-step experiences with actions distribution [  8 123 104   1]\n",
      "average reward per episode = 4.0\n",
      "Training on 248/248 positive/total out of 922 1-step experiences with actions distribution [  4 126 113   5]\n",
      "average reward per episode = 2.4\n",
      "Training on 231/233 positive/total out of 722 1-step experiences with actions distribution [  0 149  76   8]\n",
      "average reward per episode = 2.1\n",
      "Training on 230/231 positive/total out of 748 1-step experiences with actions distribution [  7 108 107   9]\n",
      "average reward per episode = 2.4\n",
      "Training on 233/234 positive/total out of 770 1-step experiences with actions distribution [  5 140  84   5]\n",
      "average reward per episode = 1.8\n",
      "Training on 224/228 positive/total out of 715 1-step experiences with actions distribution [  6 130  85   7]\n",
      "average reward per episode = 1.2\n",
      "Training on 219/222 positive/total out of 616 1-step experiences with actions distribution [  5 152  39  26]\n",
      "average reward per episode = 1.9\n",
      "Training on 227/229 positive/total out of 714 1-step experiences with actions distribution [  7 134  73  15]\n",
      "average reward per episode = 3.6\n",
      "Training on 239/246 positive/total out of 927 1-step experiences with actions distribution [  6 132  97  11]\n",
      "average reward per episode = 1.1\n",
      "Training on 217/221 positive/total out of 661 1-step experiences with actions distribution [  5 131  75  10]\n",
      "average reward per episode = 1.2\n",
      "Training on 219/222 positive/total out of 661 1-step experiences with actions distribution [  4 133  59  26]\n",
      "average reward per episode = 2.4\n",
      "Training on 232/234 positive/total out of 720 1-step experiences with actions distribution [  9 133  76  16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 19:34:04,017] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/DoomDefendLine_q_learning/openaigym.video.0.52850.video003000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = 1.7\n",
      "Training on 224/227 positive/total out of 611 1-step experiences with actions distribution [  8 145  49  25]\n",
      "average reward per episode = 0.9\n",
      "Training on 216/219 positive/total out of 633 1-step experiences with actions distribution [  9 126  52  32]\n",
      "average reward per episode = 0.4\n",
      "Training on 208/214 positive/total out of 548 1-step experiences with actions distribution [  6 149  41  18]\n",
      "average reward per episode = 1.4\n",
      "Training on 221/224 positive/total out of 677 1-step experiences with actions distribution [ 10 137  64  13]\n",
      "average reward per episode = 1.5\n",
      "Training on 223/225 positive/total out of 667 1-step experiences with actions distribution [  5 151  53  16]\n",
      "average reward per episode = 1.2\n",
      "Training on 219/222 positive/total out of 671 1-step experiences with actions distribution [  8 118  76  20]\n",
      "average reward per episode = 1.0\n",
      "Training on 214/220 positive/total out of 602 1-step experiences with actions distribution [  3 142  50  25]\n",
      "average reward per episode = 3.3\n",
      "Training on 238/242 positive/total out of 853 1-step experiences with actions distribution [ 15 121  81  25]\n",
      "average reward per episode = 1.6\n",
      "Training on 221/226 positive/total out of 708 1-step experiences with actions distribution [ 12 136  62  16]\n",
      "average reward per episode = 1.2\n",
      "Training on 217/222 positive/total out of 586 1-step experiences with actions distribution [ 11 163  31  17]\n",
      "average reward per episode = 2.9\n",
      "Training on 235/239 positive/total out of 826 1-step experiences with actions distribution [ 20 129  72  18]\n",
      "average reward per episode = 1.4\n",
      "Training on 222/224 positive/total out of 720 1-step experiences with actions distribution [ 12 144  52  16]\n",
      "average reward per episode = 3.5\n",
      "Training on 241/245 positive/total out of 937 1-step experiences with actions distribution [ 16 121  87  21]\n",
      "average reward per episode = 2.6\n",
      "Training on 232/236 positive/total out of 748 1-step experiences with actions distribution [ 10 121  91  14]\n",
      "average reward per episode = 1.3\n",
      "Training on 218/223 positive/total out of 662 1-step experiences with actions distribution [  7 122  68  26]\n",
      "average reward per episode = 2.3\n",
      "Training on 232/233 positive/total out of 778 1-step experiences with actions distribution [ 18 136  63  16]\n",
      "average reward per episode = 2.3\n",
      "Training on 229/232 positive/total out of 768 1-step experiences with actions distribution [ 21 121  65  25]\n",
      "average reward per episode = 2.3\n",
      "Training on 232/233 positive/total out of 743 1-step experiences with actions distribution [ 21 132  52  28]\n",
      "average reward per episode = 2.1\n",
      "Training on 228/231 positive/total out of 716 1-step experiences with actions distribution [ 17 116  75  23]\n",
      "average reward per episode = 0.7\n",
      "Training on 215/217 positive/total out of 585 1-step experiences with actions distribution [ 22 136  40  19]\n",
      "average reward per episode = 2.5\n",
      "Training on 233/234 positive/total out of 777 1-step experiences with actions distribution [ 16 117  87  14]\n",
      "average reward per episode = 2.6\n",
      "Training on 231/236 positive/total out of 817 1-step experiences with actions distribution [ 11 108 106  11]\n",
      "average reward per episode = 2.0\n",
      "Training on 227/230 positive/total out of 743 1-step experiences with actions distribution [ 32 117  71  10]\n",
      "average reward per episode = 1.9\n",
      "Training on 228/229 positive/total out of 743 1-step experiences with actions distribution [ 26 108  71  24]\n",
      "average reward per episode = 2.7\n",
      "Training on 235/236 positive/total out of 788 1-step experiences with actions distribution [ 13 124  85  14]\n",
      "average reward per episode = 2.2\n",
      "Training on 230/232 positive/total out of 787 1-step experiences with actions distribution [ 27 120  73  12]\n",
      "average reward per episode = 2.4\n",
      "Training on 232/234 positive/total out of 822 1-step experiences with actions distribution [ 16 114  73  31]\n",
      "average reward per episode = 2.1\n",
      "Training on 226/231 positive/total out of 758 1-step experiences with actions distribution [ 22 115  72  22]\n",
      "average reward per episode = 3.2\n",
      "Training on 239/242 positive/total out of 813 1-step experiences with actions distribution [ 22 123  79  18]\n",
      "average reward per episode = 1.5\n",
      "Training on 223/225 positive/total out of 688 1-step experiences with actions distribution [ 23 123  59  20]\n",
      "average reward per episode = 1.9\n",
      "Training on 228/229 positive/total out of 706 1-step experiences with actions distribution [ 27 111  76  15]\n",
      "average reward per episode = 1.2\n",
      "Training on 219/222 positive/total out of 638 1-step experiences with actions distribution [ 35 113  50  24]\n",
      "average reward per episode = 1.4\n",
      "Training on 224/224 positive/total out of 707 1-step experiences with actions distribution [ 20 118  69  17]\n",
      "average reward per episode = 2.1\n",
      "Training on 228/231 positive/total out of 761 1-step experiences with actions distribution [ 16 109  80  26]\n",
      "average reward per episode = 3.2\n",
      "Training on 239/242 positive/total out of 864 1-step experiences with actions distribution [ 32 106  83  21]\n",
      "average reward per episode = 1.0\n",
      "Training on 217/220 positive/total out of 629 1-step experiences with actions distribution [27 80 77 36]\n",
      "average reward per episode = 1.3\n",
      "Training on 221/223 positive/total out of 675 1-step experiences with actions distribution [ 32 100  68  23]\n",
      "average reward per episode = 2.4\n",
      "Training on 230/234 positive/total out of 730 1-step experiences with actions distribution [ 22 127  61  24]\n",
      "average reward per episode = 2.6\n",
      "Training on 235/236 positive/total out of 779 1-step experiences with actions distribution [ 23 114  80  19]\n",
      "average reward per episode = 2.6\n",
      "Training on 234/236 positive/total out of 786 1-step experiences with actions distribution [36 99 65 36]\n",
      "average reward per episode = 1.7\n",
      "Training on 226/227 positive/total out of 711 1-step experiences with actions distribution [30 95 80 22]\n",
      "average reward per episode = 2.5\n",
      "Training on 233/235 positive/total out of 776 1-step experiences with actions distribution [ 20 108  71  36]\n",
      "average reward per episode = 4.0\n",
      "Training on 247/249 positive/total out of 925 1-step experiences with actions distribution [ 21  98 111  19]\n",
      "average reward per episode = 2.4\n",
      "Training on 233/234 positive/total out of 800 1-step experiences with actions distribution [ 35 105  76  18]\n",
      "average reward per episode = 3.4\n",
      "Training on 242/244 positive/total out of 937 1-step experiences with actions distribution [ 22 112  83  27]\n",
      "average reward per episode = 3.2\n",
      "Training on 238/242 positive/total out of 811 1-step experiences with actions distribution [ 33 106  82  21]\n",
      "average reward per episode = 3.0\n",
      "Training on 237/240 positive/total out of 891 1-step experiences with actions distribution [ 26 102  86  26]\n",
      "average reward per episode = 4.1\n",
      "Training on 251/251 positive/total out of 944 1-step experiences with actions distribution [ 36 129  70  16]\n",
      "average reward per episode = 2.1\n",
      "Training on 229/231 positive/total out of 790 1-step experiences with actions distribution [ 27 120  71  13]\n",
      "average reward per episode = 3.6\n",
      "Training on 244/246 positive/total out of 910 1-step experiences with actions distribution [ 29 108  94  15]\n",
      "average reward per episode = 3.4\n",
      "Training on 240/244 positive/total out of 807 1-step experiences with actions distribution [ 32 107  87  18]\n",
      "average reward per episode = 2.9\n",
      "Training on 239/239 positive/total out of 791 1-step experiences with actions distribution [ 36 105  79  19]\n",
      "average reward per episode = 3.1\n",
      "Training on 239/241 positive/total out of 861 1-step experiences with actions distribution [ 30 108  88  15]\n",
      "average reward per episode = 2.6\n",
      "Training on 236/236 positive/total out of 830 1-step experiences with actions distribution [40 92 91 13]\n",
      "average reward per episode = 2.8\n",
      "Training on 236/238 positive/total out of 812 1-step experiences with actions distribution [ 42 107  73  16]\n",
      "average reward per episode = 4.1\n",
      "Training on 248/251 positive/total out of 985 1-step experiences with actions distribution [ 22 101 111  17]\n",
      "average reward per episode = 3.0\n",
      "Training on 237/240 positive/total out of 902 1-step experiences with actions distribution [ 21 129  75  15]\n",
      "average reward per episode = 1.6\n",
      "Training on 222/226 positive/total out of 671 1-step experiences with actions distribution [ 22 123  70  11]\n",
      "average reward per episode = 3.9\n",
      "Training on 248/249 positive/total out of 935 1-step experiences with actions distribution [ 20 103 111  15]\n",
      "average reward per episode = 4.4\n",
      "Training on 250/254 positive/total out of 1041 1-step experiences with actions distribution [ 24  98 119  13]\n",
      "average reward per episode = 4.2\n",
      "Training on 248/252 positive/total out of 936 1-step experiences with actions distribution [ 38 113  84  17]\n",
      "average reward per episode = 2.6\n",
      "Training on 234/236 positive/total out of 815 1-step experiences with actions distribution [ 33 105  84  14]\n",
      "average reward per episode = 2.2\n",
      "Training on 231/232 positive/total out of 740 1-step experiences with actions distribution [ 17 107  85  23]\n",
      "average reward per episode = 2.6\n",
      "Training on 234/236 positive/total out of 804 1-step experiences with actions distribution [ 21 106  87  22]\n",
      "average reward per episode = 3.0\n",
      "Training on 238/240 positive/total out of 797 1-step experiences with actions distribution [ 30 111  79  20]\n",
      "average reward per episode = 0.3\n",
      "Training on 207/213 positive/total out of 550 1-step experiences with actions distribution [ 28 135  37  13]\n",
      "average reward per episode = 3.1\n",
      "Training on 237/241 positive/total out of 804 1-step experiences with actions distribution [ 40 111  75  15]\n",
      "average reward per episode = 3.1\n",
      "Training on 238/240 positive/total out of 861 1-step experiences with actions distribution [ 40 119  71  10]\n",
      "average reward per episode = 2.7\n",
      "Training on 235/237 positive/total out of 812 1-step experiences with actions distribution [ 19 124  85   9]\n",
      "average reward per episode = 3.3\n",
      "Training on 238/243 positive/total out of 903 1-step experiences with actions distribution [ 33  98 102  10]\n",
      "average reward per episode = 3.8\n",
      "Training on 247/247 positive/total out of 969 1-step experiences with actions distribution [ 15 136  85  11]\n",
      "average reward per episode = 3.4\n",
      "Training on 242/244 positive/total out of 874 1-step experiences with actions distribution [ 22 112  93  17]\n",
      "average reward per episode = 3.5\n",
      "Training on 243/245 positive/total out of 917 1-step experiences with actions distribution [ 18 124  97   6]\n",
      "average reward per episode = 4.0\n",
      "Training on 249/250 positive/total out of 916 1-step experiences with actions distribution [ 28 129  83  10]\n",
      "average reward per episode = 2.8\n",
      "Training on 237/238 positive/total out of 828 1-step experiences with actions distribution [ 11 131  91   5]\n",
      "average reward per episode = 3.6\n",
      "Training on 243/246 positive/total out of 864 1-step experiences with actions distribution [ 11 117 106  12]\n",
      "average reward per episode = 1.8\n",
      "Training on 224/228 positive/total out of 759 1-step experiences with actions distribution [ 14 123  77  14]\n",
      "average reward per episode = 1.9\n",
      "Training on 227/229 positive/total out of 693 1-step experiences with actions distribution [ 25 138  58   8]\n",
      "average reward per episode = 1.1\n",
      "Training on 218/221 positive/total out of 618 1-step experiences with actions distribution [ 16 138  58   9]\n",
      "average reward per episode = 3.4\n",
      "Training on 243/244 positive/total out of 907 1-step experiences with actions distribution [ 16 117  90  21]\n",
      "average reward per episode = 2.0\n",
      "Training on 227/230 positive/total out of 790 1-step experiences with actions distribution [ 18 136  59  17]\n",
      "average reward per episode = 2.5\n",
      "Training on 232/235 positive/total out of 805 1-step experiences with actions distribution [ 32 120  69  14]\n",
      "average reward per episode = 1.8\n",
      "Training on 222/228 positive/total out of 698 1-step experiences with actions distribution [ 17 119  67  25]\n",
      "average reward per episode = 2.3\n",
      "Training on 232/233 positive/total out of 830 1-step experiences with actions distribution [ 19 122  73  19]\n",
      "average reward per episode = 1.4\n",
      "Training on 221/224 positive/total out of 646 1-step experiences with actions distribution [ 15 133  52  24]\n",
      "average reward per episode = 1.9\n",
      "Training on 225/229 positive/total out of 700 1-step experiences with actions distribution [ 19 126  70  14]\n",
      "average reward per episode = 2.6\n",
      "Training on 233/235 positive/total out of 808 1-step experiences with actions distribution [ 14 115  96  10]\n",
      "average reward per episode = 2.0\n",
      "Training on 229/230 positive/total out of 683 1-step experiences with actions distribution [ 19 127  56  28]\n",
      "average reward per episode = 2.9\n",
      "Training on 235/239 positive/total out of 829 1-step experiences with actions distribution [ 25 123  74  17]\n",
      "average reward per episode = 3.0\n",
      "Training on 236/240 positive/total out of 855 1-step experiences with actions distribution [ 32 114  83  11]\n",
      "average reward per episode = 1.9\n",
      "Training on 227/229 positive/total out of 734 1-step experiences with actions distribution [ 32 115  61  21]\n",
      "average reward per episode = 1.4\n",
      "Training on 222/224 positive/total out of 691 1-step experiences with actions distribution [ 27 122  61  14]\n",
      "average reward per episode = 1.5\n",
      "Training on 225/225 positive/total out of 669 1-step experiences with actions distribution [ 24 116  66  19]\n",
      "average reward per episode = 3.5\n",
      "Training on 243/245 positive/total out of 921 1-step experiences with actions distribution [ 25 112  92  16]\n",
      "average reward per episode = 1.3\n",
      "Training on 221/223 positive/total out of 687 1-step experiences with actions distribution [ 12 112  66  33]\n",
      "average reward per episode = 1.0\n",
      "Training on 217/220 positive/total out of 653 1-step experiences with actions distribution [ 26 113  59  22]\n",
      "average reward per episode = 2.1\n",
      "Training on 230/231 positive/total out of 734 1-step experiences with actions distribution [ 23 118  75  15]\n",
      "average reward per episode = 2.2\n",
      "Training on 231/232 positive/total out of 795 1-step experiences with actions distribution [ 18 111  85  18]\n",
      "average reward per episode = 1.6\n",
      "Training on 222/226 positive/total out of 676 1-step experiences with actions distribution [ 34 100  63  29]\n",
      "average reward per episode = 1.5\n",
      "Training on 225/225 positive/total out of 642 1-step experiences with actions distribution [ 43 114  53  15]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 20:01:37,022] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/DoomDefendLine_q_learning/openaigym.video.0.52850.video004000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = 2.0\n",
      "Training on 226/230 positive/total out of 722 1-step experiences with actions distribution [ 28 133  49  20]\n",
      "average reward per episode = 2.3\n",
      "Training on 232/233 positive/total out of 709 1-step experiences with actions distribution [ 24 146  56   7]\n",
      "average reward per episode = 1.6\n",
      "Training on 225/226 positive/total out of 697 1-step experiences with actions distribution [ 42 126  46  12]\n",
      "average reward per episode = 2.3\n",
      "Training on 229/233 positive/total out of 778 1-step experiences with actions distribution [ 41 112  66  14]\n",
      "average reward per episode = 1.3\n",
      "Training on 222/223 positive/total out of 639 1-step experiences with actions distribution [ 21 134  46  22]\n",
      "average reward per episode = 2.6\n",
      "Training on 235/236 positive/total out of 804 1-step experiences with actions distribution [ 32 115  70  19]\n",
      "average reward per episode = 0.7\n",
      "Training on 212/217 positive/total out of 599 1-step experiences with actions distribution [38 91 52 36]\n",
      "average reward per episode = 0.8\n",
      "Training on 211/217 positive/total out of 600 1-step experiences with actions distribution [48 84 47 38]\n",
      "average reward per episode = 0.8\n",
      "Training on 216/218 positive/total out of 581 1-step experiences with actions distribution [ 19 121  51  27]\n",
      "average reward per episode = 0.6\n",
      "Training on 212/216 positive/total out of 564 1-step experiences with actions distribution [ 20 121  44  31]\n",
      "average reward per episode = 1.9\n",
      "Training on 228/229 positive/total out of 715 1-step experiences with actions distribution [22 99 77 31]\n",
      "average reward per episode = 1.6\n",
      "Training on 221/226 positive/total out of 710 1-step experiences with actions distribution [37 83 81 25]\n",
      "average reward per episode = 1.0\n",
      "Training on 217/220 positive/total out of 633 1-step experiences with actions distribution [43 96 52 29]\n",
      "average reward per episode = 0.9\n",
      "Training on 216/219 positive/total out of 608 1-step experiences with actions distribution [ 38 102  35  44]\n",
      "average reward per episode = 3.9\n",
      "Training on 247/249 positive/total out of 963 1-step experiences with actions distribution [ 24 114  78  33]\n",
      "average reward per episode = 2.4\n",
      "Training on 231/234 positive/total out of 791 1-step experiences with actions distribution [ 19 106  82  27]\n",
      "average reward per episode = 3.2\n",
      "Training on 240/242 positive/total out of 925 1-step experiences with actions distribution [ 30 110  87  15]\n",
      "average reward per episode = 2.5\n",
      "Training on 232/235 positive/total out of 776 1-step experiences with actions distribution [ 16 138  65  16]\n",
      "average reward per episode = 1.6\n",
      "Training on 222/226 positive/total out of 689 1-step experiences with actions distribution [ 39 119  49  19]\n",
      "average reward per episode = 2.5\n",
      "Training on 235/235 positive/total out of 782 1-step experiences with actions distribution [33 98 73 31]\n",
      "average reward per episode = 1.2\n",
      "Training on 221/222 positive/total out of 613 1-step experiences with actions distribution [ 29 119  47  27]\n",
      "average reward per episode = 2.0\n",
      "Training on 228/230 positive/total out of 690 1-step experiences with actions distribution [ 38 108  65  19]\n",
      "average reward per episode = 0.5\n",
      "Training on 210/215 positive/total out of 561 1-step experiences with actions distribution [ 29 110  45  31]\n",
      "average reward per episode = 0.4\n",
      "Training on 210/214 positive/total out of 583 1-step experiences with actions distribution [ 30 117  37  30]\n",
      "average reward per episode = 1.7\n",
      "Training on 225/227 positive/total out of 676 1-step experiences with actions distribution [28 96 69 34]\n",
      "average reward per episode = 0.9\n",
      "Training on 218/219 positive/total out of 587 1-step experiences with actions distribution [ 44 103  45  27]\n",
      "average reward per episode = 0.8\n",
      "Training on 215/218 positive/total out of 593 1-step experiences with actions distribution [ 52 103  30  33]\n",
      "average reward per episode = 0.7\n",
      "Training on 212/217 positive/total out of 594 1-step experiences with actions distribution [ 35 112  40  30]\n",
      "average reward per episode = 0.6\n",
      "Training on 214/216 positive/total out of 638 1-step experiences with actions distribution [ 27 103  51  35]\n",
      "average reward per episode = 1.1\n",
      "Training on 220/221 positive/total out of 682 1-step experiences with actions distribution [ 32 124  22  43]\n",
      "average reward per episode = 0.6\n",
      "Training on 212/215 positive/total out of 639 1-step experiences with actions distribution [ 45 110  28  32]\n",
      "average reward per episode = 0.7\n",
      "Training on 216/217 positive/total out of 629 1-step experiences with actions distribution [45 93 31 48]\n",
      "average reward per episode = 0.7\n",
      "Training on 212/217 positive/total out of 627 1-step experiences with actions distribution [ 31 117  32  37]\n",
      "average reward per episode = 1.1\n",
      "Training on 218/221 positive/total out of 618 1-step experiences with actions distribution [31 91 57 42]\n",
      "average reward per episode = 1.4\n",
      "Training on 218/224 positive/total out of 716 1-step experiences with actions distribution [ 26 102  36  60]\n",
      "average reward per episode = 0.5\n",
      "Training on 212/215 positive/total out of 579 1-step experiences with actions distribution [ 29 114  29  43]\n",
      "average reward per episode = 0.7\n",
      "Training on 215/217 positive/total out of 610 1-step experiences with actions distribution [ 22 117  45  33]\n",
      "average reward per episode = 0.8\n",
      "Training on 217/218 positive/total out of 650 1-step experiences with actions distribution [32 95 46 45]\n",
      "average reward per episode = 0.5\n",
      "Training on 212/215 positive/total out of 641 1-step experiences with actions distribution [ 23 103  41  48]\n",
      "average reward per episode = 0.5\n",
      "Training on 214/215 positive/total out of 618 1-step experiences with actions distribution [50 84 35 46]\n",
      "average reward per episode = 0.8\n",
      "Training on 216/218 positive/total out of 678 1-step experiences with actions distribution [ 15 118  37  48]\n",
      "average reward per episode = 1.1\n",
      "Training on 219/221 positive/total out of 678 1-step experiences with actions distribution [ 29 113  39  40]\n",
      "average reward per episode = 1.2\n",
      "Training on 220/222 positive/total out of 684 1-step experiences with actions distribution [ 36 129  39  18]\n",
      "average reward per episode = 0.8\n",
      "Training on 214/218 positive/total out of 661 1-step experiences with actions distribution [ 30 132  35  21]\n",
      "average reward per episode = 2.0\n",
      "Training on 230/230 positive/total out of 682 1-step experiences with actions distribution [ 25 130  47  28]\n",
      "average reward per episode = 1.9\n",
      "Training on 227/229 positive/total out of 679 1-step experiences with actions distribution [ 36 125  44  24]\n",
      "average reward per episode = 0.9\n",
      "Training on 214/219 positive/total out of 588 1-step experiences with actions distribution [ 27 101  51  40]\n",
      "average reward per episode = 1.2\n",
      "Training on 219/222 positive/total out of 686 1-step experiences with actions distribution [18 96 59 49]\n",
      "average reward per episode = 0.8\n",
      "Training on 215/218 positive/total out of 619 1-step experiences with actions distribution [26 85 64 43]\n",
      "average reward per episode = 1.5\n",
      "Training on 223/224 positive/total out of 676 1-step experiences with actions distribution [23 89 75 37]\n",
      "average reward per episode = 0.8\n",
      "Training on 214/218 positive/total out of 600 1-step experiences with actions distribution [ 28 104  39  47]\n",
      "average reward per episode = 2.7\n",
      "Training on 234/237 positive/total out of 739 1-step experiences with actions distribution [ 34 114  55  34]\n",
      "average reward per episode = 0.6\n",
      "Training on 213/216 positive/total out of 619 1-step experiences with actions distribution [ 21 125  42  28]\n",
      "average reward per episode = 0.7\n",
      "Training on 215/217 positive/total out of 625 1-step experiences with actions distribution [ 19 114  39  45]\n",
      "average reward per episode = 1.2\n",
      "Training on 221/221 positive/total out of 621 1-step experiences with actions distribution [ 19 130  42  30]\n",
      "average reward per episode = 1.2\n",
      "Training on 220/222 positive/total out of 643 1-step experiences with actions distribution [ 34 101  68  19]\n",
      "average reward per episode = 1.4\n",
      "Training on 224/224 positive/total out of 643 1-step experiences with actions distribution [ 23 129  40  32]\n",
      "average reward per episode = 0.9\n",
      "Training on 214/218 positive/total out of 583 1-step experiences with actions distribution [ 15 115  54  34]\n",
      "average reward per episode = 0.4\n",
      "Training on 212/214 positive/total out of 594 1-step experiences with actions distribution [ 29 117  30  38]\n",
      "average reward per episode = 1.1\n",
      "Training on 218/221 positive/total out of 648 1-step experiences with actions distribution [ 18 105  57  41]\n",
      "average reward per episode = 1.0\n",
      "Training on 218/220 positive/total out of 615 1-step experiences with actions distribution [ 13 116  41  50]\n",
      "average reward per episode = 1.3\n",
      "Training on 219/222 positive/total out of 636 1-step experiences with actions distribution [ 15 114  53  40]\n",
      "average reward per episode = 1.6\n",
      "Training on 225/226 positive/total out of 682 1-step experiences with actions distribution [ 14 150  45  17]\n",
      "average reward per episode = 1.8\n",
      "Training on 224/228 positive/total out of 679 1-step experiences with actions distribution [ 10 120  73  25]\n",
      "average reward per episode = 1.9\n",
      "Training on 227/228 positive/total out of 680 1-step experiences with actions distribution [  9 139  59  21]\n",
      "average reward per episode = 1.9\n",
      "Training on 227/228 positive/total out of 722 1-step experiences with actions distribution [ 12 163  43  10]\n",
      "average reward per episode = 2.1\n",
      "Training on 227/231 positive/total out of 668 1-step experiences with actions distribution [  5 132  70  24]\n",
      "average reward per episode = 1.8\n",
      "Training on 228/228 positive/total out of 721 1-step experiences with actions distribution [  9 115  76  28]\n",
      "average reward per episode = 2.1\n",
      "Training on 223/231 positive/total out of 705 1-step experiences with actions distribution [  9 151  59  12]\n",
      "average reward per episode = 4.6\n",
      "Training on 254/256 positive/total out of 1009 1-step experiences with actions distribution [ 11 145  91   9]\n",
      "average reward per episode = 2.8\n",
      "Training on 237/238 positive/total out of 844 1-step experiences with actions distribution [ 11 130  90   7]\n",
      "average reward per episode = 1.9\n",
      "Training on 227/228 positive/total out of 669 1-step experiences with actions distribution [  4 124  85  15]\n",
      "average reward per episode = 3.4\n",
      "Training on 243/244 positive/total out of 896 1-step experiences with actions distribution [ 14 122  96  12]\n",
      "average reward per episode = 3.7\n",
      "Training on 245/247 positive/total out of 869 1-step experiences with actions distribution [  5 111 123   8]\n",
      "average reward per episode = 2.1\n",
      "Training on 227/230 positive/total out of 677 1-step experiences with actions distribution [  3 131  78  18]\n",
      "average reward per episode = 3.0\n",
      "Training on 234/240 positive/total out of 786 1-step experiences with actions distribution [  4 119 102  15]\n",
      "average reward per episode = 2.2\n",
      "Training on 229/232 positive/total out of 678 1-step experiences with actions distribution [ 18 101  82  31]\n",
      "average reward per episode = 2.5\n",
      "Training on 233/235 positive/total out of 716 1-step experiences with actions distribution [ 10 131  75  19]\n",
      "average reward per episode = 3.3\n",
      "Training on 242/242 positive/total out of 875 1-step experiences with actions distribution [13 88 96 45]\n",
      "average reward per episode = 4.1\n",
      "Training on 250/251 positive/total out of 967 1-step experiences with actions distribution [ 13  97 116  25]\n",
      "average reward per episode = 4.8\n",
      "Training on 257/258 positive/total out of 1120 1-step experiences with actions distribution [ 14 113 112  19]\n",
      "average reward per episode = 3.9\n",
      "Training on 244/248 positive/total out of 887 1-step experiences with actions distribution [ 26 100 106  16]\n",
      "average reward per episode = 3.8\n",
      "Training on 245/246 positive/total out of 904 1-step experiences with actions distribution [  9 112  95  30]\n",
      "average reward per episode = 1.6\n",
      "Training on 223/226 positive/total out of 649 1-step experiences with actions distribution [ 9 75 97 45]\n",
      "average reward per episode = 2.1\n",
      "Training on 229/231 positive/total out of 685 1-step experiences with actions distribution [  7 121  84  19]\n",
      "average reward per episode = 1.1\n",
      "Training on 220/221 positive/total out of 641 1-step experiences with actions distribution [ 13 115  65  28]\n",
      "average reward per episode = 3.6\n",
      "Training on 243/246 positive/total out of 911 1-step experiences with actions distribution [ 17  96 119  14]\n",
      "average reward per episode = 1.4\n",
      "Training on 221/224 positive/total out of 650 1-step experiences with actions distribution [ 26 121  54  23]\n",
      "average reward per episode = 0.8\n",
      "Training on 215/218 positive/total out of 628 1-step experiences with actions distribution [ 31 100  71  16]\n",
      "average reward per episode = 2.0\n",
      "Training on 227/230 positive/total out of 777 1-step experiences with actions distribution [20 92 86 32]\n",
      "average reward per episode = 1.7\n",
      "Training on 226/227 positive/total out of 672 1-step experiences with actions distribution [15 98 89 25]\n",
      "average reward per episode = 1.7\n",
      "Training on 222/227 positive/total out of 696 1-step experiences with actions distribution [38 80 88 21]\n",
      "average reward per episode = 0.4\n",
      "Training on 211/214 positive/total out of 607 1-step experiences with actions distribution [46 92 49 27]\n",
      "average reward per episode = 1.5\n",
      "Training on 224/225 positive/total out of 643 1-step experiences with actions distribution [ 28 101  69  27]\n",
      "average reward per episode = 0.7\n",
      "Training on 217/217 positive/total out of 622 1-step experiences with actions distribution [61 80 47 29]\n",
      "average reward per episode = 0.7\n",
      "Training on 212/217 positive/total out of 611 1-step experiences with actions distribution [ 52 104  29  32]\n",
      "average reward per episode = 1.8\n",
      "Training on 227/228 positive/total out of 726 1-step experiences with actions distribution [53 67 76 32]\n",
      "average reward per episode = 1.3\n",
      "Training on 221/223 positive/total out of 658 1-step experiences with actions distribution [65 57 71 30]\n",
      "average reward per episode = 0.2\n",
      "Training on 207/212 positive/total out of 574 1-step experiences with actions distribution [59 48 51 54]\n",
      "average reward per episode = 0.5\n",
      "Training on 212/215 positive/total out of 585 1-step experiences with actions distribution [36 75 53 51]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 20:30:41,221] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/DoomDefendLine_q_learning/openaigym.video.0.52850.video005000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = 0.7\n",
      "Training on 214/217 positive/total out of 619 1-step experiences with actions distribution [ 57 112  27  21]\n",
      "average reward per episode = 0.9\n",
      "Training on 218/219 positive/total out of 633 1-step experiences with actions distribution [23 88 62 46]\n",
      "average reward per episode = 0.6\n",
      "Training on 213/216 positive/total out of 617 1-step experiences with actions distribution [35 87 57 37]\n",
      "average reward per episode = 0.3\n",
      "Training on 208/213 positive/total out of 568 1-step experiences with actions distribution [35 96 38 44]\n",
      "average reward per episode = 1.9\n",
      "Training on 225/229 positive/total out of 719 1-step experiences with actions distribution [30 91 66 42]\n",
      "average reward per episode = 2.4\n",
      "Training on 230/234 positive/total out of 815 1-step experiences with actions distribution [29 95 81 29]\n",
      "average reward per episode = 0.4\n",
      "Training on 210/214 positive/total out of 576 1-step experiences with actions distribution [43 60 66 45]\n",
      "average reward per episode = 0.7\n",
      "Training on 214/217 positive/total out of 603 1-step experiences with actions distribution [64 47 53 53]\n",
      "average reward per episode = 1.6\n",
      "Training on 224/226 positive/total out of 704 1-step experiences with actions distribution [38 74 89 25]\n",
      "average reward per episode = 2.4\n",
      "Training on 232/234 positive/total out of 705 1-step experiences with actions distribution [45 98 67 24]\n",
      "average reward per episode = 0.7\n",
      "Training on 212/217 positive/total out of 584 1-step experiences with actions distribution [ 26 119  42  30]\n",
      "average reward per episode = 1.1\n",
      "Training on 218/221 positive/total out of 656 1-step experiences with actions distribution [ 22 125  43  31]\n",
      "average reward per episode = 1.6\n",
      "Training on 224/226 positive/total out of 721 1-step experiences with actions distribution [77 81 50 18]\n",
      "average reward per episode = 0.8\n",
      "Training on 217/217 positive/total out of 593 1-step experiences with actions distribution [47 99 51 20]\n",
      "average reward per episode = 1.2\n",
      "Training on 219/222 positive/total out of 669 1-step experiences with actions distribution [63 89 51 19]\n",
      "average reward per episode = 1.8\n",
      "Training on 225/228 positive/total out of 787 1-step experiences with actions distribution [ 49 100  50  29]\n",
      "average reward per episode = 1.4\n",
      "Training on 222/224 positive/total out of 641 1-step experiences with actions distribution [67 81 54 22]\n",
      "average reward per episode = 1.8\n",
      "Training on 225/228 positive/total out of 779 1-step experiences with actions distribution [63 84 55 26]\n",
      "average reward per episode = 0.8\n",
      "Training on 216/218 positive/total out of 639 1-step experiences with actions distribution [71 87 39 21]\n",
      "average reward per episode = 1.0\n",
      "Training on 214/220 positive/total out of 639 1-step experiences with actions distribution [65 75 59 21]\n",
      "average reward per episode = 1.1\n",
      "Training on 217/221 positive/total out of 643 1-step experiences with actions distribution [ 49 104  37  31]\n",
      "average reward per episode = 0.7\n",
      "Training on 216/217 positive/total out of 637 1-step experiences with actions distribution [53 83 44 37]\n",
      "average reward per episode = 1.5\n",
      "Training on 222/225 positive/total out of 708 1-step experiences with actions distribution [52 92 52 29]\n",
      "average reward per episode = 0.1\n",
      "Training on 204/211 positive/total out of 553 1-step experiences with actions distribution [49 96 38 28]\n",
      "average reward per episode = 2.1\n",
      "Training on 222/230 positive/total out of 702 1-step experiences with actions distribution [31 93 84 22]\n",
      "average reward per episode = 2.9\n",
      "Training on 236/239 positive/total out of 853 1-step experiences with actions distribution [42 92 76 29]\n",
      "average reward per episode = 3.0\n",
      "Training on 240/240 positive/total out of 814 1-step experiences with actions distribution [ 39 117  66  18]\n",
      "average reward per episode = 2.5\n",
      "Training on 233/235 positive/total out of 769 1-step experiences with actions distribution [ 30 111  78  16]\n",
      "average reward per episode = 1.6\n",
      "Training on 226/226 positive/total out of 670 1-step experiences with actions distribution [80 81 53 12]\n",
      "average reward per episode = 2.3\n",
      "Training on 231/233 positive/total out of 775 1-step experiences with actions distribution [ 32 115  68  18]\n",
      "average reward per episode = 1.3\n",
      "Training on 220/223 positive/total out of 692 1-step experiences with actions distribution [ 63 106  35  19]\n",
      "average reward per episode = 2.9\n",
      "Training on 234/239 positive/total out of 824 1-step experiences with actions distribution [ 35 102  68  34]\n",
      "average reward per episode = 1.1\n",
      "Training on 218/221 positive/total out of 623 1-step experiences with actions distribution [ 60 104  34  23]\n",
      "average reward per episode = 1.7\n",
      "Training on 225/227 positive/total out of 687 1-step experiences with actions distribution [ 54 104  51  18]\n",
      "average reward per episode = 1.5\n",
      "Training on 224/225 positive/total out of 672 1-step experiences with actions distribution [ 42 101  61  21]\n",
      "average reward per episode = 1.0\n",
      "Training on 218/219 positive/total out of 586 1-step experiences with actions distribution [75 95 37 12]\n",
      "average reward per episode = 0.4\n",
      "Training on 211/214 positive/total out of 608 1-step experiences with actions distribution [68 68 41 37]\n",
      "average reward per episode = 2.4\n",
      "Training on 233/234 positive/total out of 768 1-step experiences with actions distribution [38 82 79 35]\n",
      "average reward per episode = 1.8\n",
      "Training on 226/228 positive/total out of 707 1-step experiences with actions distribution [ 53 116  40  19]\n",
      "average reward per episode = 1.4\n",
      "Training on 222/224 positive/total out of 634 1-step experiences with actions distribution [56 97 51 20]\n",
      "average reward per episode = 1.1\n",
      "Training on 219/221 positive/total out of 684 1-step experiences with actions distribution [ 41 118  34  28]\n",
      "average reward per episode = 0.6\n",
      "Training on 213/216 positive/total out of 616 1-step experiences with actions distribution [ 41 109  41  25]\n",
      "average reward per episode = 1.2\n",
      "Training on 220/222 positive/total out of 665 1-step experiences with actions distribution [61 91 59 11]\n",
      "average reward per episode = 0.9\n",
      "Training on 217/219 positive/total out of 641 1-step experiences with actions distribution [56 80 61 22]\n",
      "average reward per episode = 1.6\n",
      "Training on 222/226 positive/total out of 690 1-step experiences with actions distribution [36 98 65 27]\n",
      "average reward per episode = 1.2\n",
      "Training on 221/222 positive/total out of 709 1-step experiences with actions distribution [47 92 63 20]\n",
      "average reward per episode = 1.4\n",
      "Training on 222/224 positive/total out of 635 1-step experiences with actions distribution [34 86 63 41]\n",
      "average reward per episode = 2.2\n",
      "Training on 231/232 positive/total out of 782 1-step experiences with actions distribution [ 31 102  86  13]\n",
      "average reward per episode = 1.6\n",
      "Training on 221/226 positive/total out of 675 1-step experiences with actions distribution [ 14 131  60  21]\n",
      "average reward per episode = 1.0\n",
      "Training on 217/220 positive/total out of 567 1-step experiences with actions distribution [ 24 122  48  26]\n",
      "average reward per episode = 1.6\n",
      "Training on 222/226 positive/total out of 650 1-step experiences with actions distribution [ 33 111  59  23]\n",
      "average reward per episode = 1.3\n",
      "Training on 220/223 positive/total out of 714 1-step experiences with actions distribution [ 55 110  34  24]\n",
      "average reward per episode = 2.1\n",
      "Training on 229/231 positive/total out of 668 1-step experiences with actions distribution [ 29 102  76  24]\n",
      "average reward per episode = 2.0\n",
      "Training on 229/230 positive/total out of 755 1-step experiences with actions distribution [ 27 104  78  21]\n",
      "average reward per episode = 1.3\n",
      "Training on 222/223 positive/total out of 665 1-step experiences with actions distribution [46 75 73 29]\n",
      "average reward per episode = 2.4\n",
      "Training on 231/234 positive/total out of 785 1-step experiences with actions distribution [43 92 80 19]\n",
      "average reward per episode = 0.8\n",
      "Training on 217/218 positive/total out of 634 1-step experiences with actions distribution [72 67 48 31]\n",
      "average reward per episode = 1.0\n",
      "Training on 217/220 positive/total out of 636 1-step experiences with actions distribution [37 66 83 34]\n",
      "average reward per episode = 0.5\n",
      "Training on 211/215 positive/total out of 572 1-step experiences with actions distribution [56 93 39 27]\n",
      "average reward per episode = 0.7\n",
      "Training on 214/217 positive/total out of 606 1-step experiences with actions distribution [69 94 35 19]\n",
      "average reward per episode = 1.4\n",
      "Training on 224/224 positive/total out of 700 1-step experiences with actions distribution [36 83 67 38]\n",
      "average reward per episode = 1.7\n",
      "Training on 224/227 positive/total out of 726 1-step experiences with actions distribution [26 97 70 34]\n",
      "average reward per episode = 1.8\n",
      "Training on 221/228 positive/total out of 734 1-step experiences with actions distribution [55 86 59 28]\n",
      "average reward per episode = 1.8\n",
      "Training on 227/228 positive/total out of 684 1-step experiences with actions distribution [34 86 88 20]\n",
      "average reward per episode = 1.3\n",
      "Training on 220/223 positive/total out of 652 1-step experiences with actions distribution [73 66 67 17]\n",
      "average reward per episode = 2.9\n",
      "Training on 233/239 positive/total out of 761 1-step experiences with actions distribution [ 39 115  71  14]\n",
      "average reward per episode = 1.6\n",
      "Training on 224/226 positive/total out of 655 1-step experiences with actions distribution [ 45 102  53  26]\n",
      "average reward per episode = 1.3\n",
      "Training on 219/223 positive/total out of 654 1-step experiences with actions distribution [ 53 110  47  13]\n",
      "average reward per episode = 3.0\n",
      "Training on 234/240 positive/total out of 894 1-step experiences with actions distribution [ 58 104  69   9]\n",
      "average reward per episode = 4.2\n",
      "Training on 251/252 positive/total out of 951 1-step experiences with actions distribution [ 36 104 103   9]\n",
      "average reward per episode = 1.7\n",
      "Training on 227/227 positive/total out of 729 1-step experiences with actions distribution [49 85 71 22]\n",
      "average reward per episode = 1.2\n",
      "Training on 218/222 positive/total out of 665 1-step experiences with actions distribution [68 90 53 11]\n",
      "average reward per episode = 1.6\n",
      "Training on 226/226 positive/total out of 677 1-step experiences with actions distribution [69 94 50 13]\n",
      "average reward per episode = 1.9\n",
      "Training on 226/229 positive/total out of 680 1-step experiences with actions distribution [60 96 62 11]\n",
      "average reward per episode = 1.2\n",
      "Training on 221/222 positive/total out of 736 1-step experiences with actions distribution [49 89 70 14]\n",
      "average reward per episode = 2.9\n",
      "Training on 237/239 positive/total out of 853 1-step experiences with actions distribution [56 96 70 17]\n",
      "average reward per episode = 1.5\n",
      "Training on 222/224 positive/total out of 685 1-step experiences with actions distribution [ 45 115  52  12]\n",
      "average reward per episode = 2.5\n",
      "Training on 234/235 positive/total out of 798 1-step experiences with actions distribution [ 45 112  73   5]\n",
      "average reward per episode = 1.7\n",
      "Training on 223/227 positive/total out of 724 1-step experiences with actions distribution [ 41 100  67  19]\n",
      "average reward per episode = 3.4\n",
      "Training on 242/244 positive/total out of 827 1-step experiences with actions distribution [ 29 108  91  16]\n",
      "average reward per episode = 1.1\n",
      "Training on 216/221 positive/total out of 657 1-step experiences with actions distribution [ 34 113  48  26]\n",
      "average reward per episode = 4.1\n",
      "Training on 249/251 positive/total out of 970 1-step experiences with actions distribution [ 27 109 105  10]\n",
      "average reward per episode = 1.7\n",
      "Training on 226/227 positive/total out of 713 1-step experiences with actions distribution [ 25 116  53  33]\n",
      "average reward per episode = 1.0\n",
      "Training on 214/220 positive/total out of 625 1-step experiences with actions distribution [ 33 107  57  23]\n",
      "average reward per episode = 1.7\n",
      "Training on 226/227 positive/total out of 669 1-step experiences with actions distribution [43 99 63 22]\n",
      "average reward per episode = 2.8\n",
      "Training on 236/238 positive/total out of 844 1-step experiences with actions distribution [41 92 88 17]\n",
      "average reward per episode = 0.9\n",
      "Training on 219/219 positive/total out of 594 1-step experiences with actions distribution [ 41 100  48  30]\n",
      "average reward per episode = 2.2\n",
      "Training on 229/232 positive/total out of 799 1-step experiences with actions distribution [49 95 65 23]\n",
      "average reward per episode = 0.9\n",
      "Training on 218/219 positive/total out of 596 1-step experiences with actions distribution [ 52 120  35  12]\n",
      "average reward per episode = 1.4\n",
      "Training on 224/224 positive/total out of 657 1-step experiences with actions distribution [70 71 59 24]\n",
      "average reward per episode = 1.2\n",
      "Training on 217/222 positive/total out of 695 1-step experiences with actions distribution [44 85 69 24]\n",
      "average reward per episode = 1.6\n",
      "Training on 223/226 positive/total out of 709 1-step experiences with actions distribution [34 93 59 40]\n",
      "average reward per episode = 1.2\n",
      "Training on 221/222 positive/total out of 600 1-step experiences with actions distribution [ 35 106  48  33]\n",
      "average reward per episode = 0.2\n",
      "Training on 210/212 positive/total out of 515 1-step experiences with actions distribution [40 75 44 53]\n",
      "average reward per episode = 2.2\n",
      "Training on 227/231 positive/total out of 710 1-step experiences with actions distribution [ 17 113  77  24]\n",
      "average reward per episode = 0.7\n",
      "Training on 213/217 positive/total out of 641 1-step experiences with actions distribution [30 84 56 47]\n",
      "average reward per episode = 1.5\n",
      "Training on 221/225 positive/total out of 657 1-step experiences with actions distribution [31 81 70 43]\n",
      "average reward per episode = 1.1\n",
      "Training on 215/221 positive/total out of 609 1-step experiences with actions distribution [ 50 102  47  22]\n",
      "average reward per episode = 1.7\n",
      "Training on 224/227 positive/total out of 674 1-step experiences with actions distribution [53 89 49 36]\n",
      "average reward per episode = 1.1\n",
      "Training on 217/221 positive/total out of 632 1-step experiences with actions distribution [42 76 56 47]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 21:01:22,878] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/DoomDefendLine_q_learning/openaigym.video.0.52850.video006000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = 1.0\n",
      "Training on 219/220 positive/total out of 655 1-step experiences with actions distribution [35 94 57 34]\n",
      "average reward per episode = 0.6\n",
      "Training on 210/216 positive/total out of 589 1-step experiences with actions distribution [26 88 65 37]\n",
      "average reward per episode = 0.7\n",
      "Training on 214/217 positive/total out of 620 1-step experiences with actions distribution [ 39 101  39  38]\n",
      "average reward per episode = 1.5\n",
      "Training on 221/225 positive/total out of 670 1-step experiences with actions distribution [ 33 113  49  30]\n",
      "average reward per episode = 0.9\n",
      "Training on 217/219 positive/total out of 624 1-step experiences with actions distribution [27 86 60 46]\n",
      "average reward per episode = 1.3\n",
      "Training on 221/223 positive/total out of 709 1-step experiences with actions distribution [29 65 76 53]\n",
      "average reward per episode = 0.5\n",
      "Training on 209/215 positive/total out of 603 1-step experiences with actions distribution [ 31 126  37  21]\n",
      "average reward per episode = 1.0\n",
      "Training on 219/220 positive/total out of 611 1-step experiences with actions distribution [ 29 109  58  24]\n",
      "average reward per episode = 0.8\n",
      "Training on 215/218 positive/total out of 587 1-step experiences with actions distribution [44 78 55 41]\n",
      "average reward per episode = 1.1\n",
      "Training on 217/221 positive/total out of 682 1-step experiences with actions distribution [ 34 106  45  36]\n",
      "average reward per episode = 2.0\n",
      "Training on 228/230 positive/total out of 701 1-step experiences with actions distribution [73 90 49 18]\n",
      "average reward per episode = 0.8\n",
      "Training on 216/218 positive/total out of 637 1-step experiences with actions distribution [44 87 62 25]\n",
      "average reward per episode = 2.5\n",
      "Training on 232/235 positive/total out of 740 1-step experiences with actions distribution [ 32 116  73  14]\n",
      "average reward per episode = 1.7\n",
      "Training on 223/227 positive/total out of 690 1-step experiences with actions distribution [41 86 81 19]\n",
      "average reward per episode = 2.4\n",
      "Training on 232/234 positive/total out of 713 1-step experiences with actions distribution [64 91 67 12]\n",
      "average reward per episode = 0.9\n",
      "Training on 217/219 positive/total out of 604 1-step experiences with actions distribution [90 67 46 16]\n",
      "average reward per episode = 1.8\n",
      "Training on 226/228 positive/total out of 735 1-step experiences with actions distribution [90 72 47 19]\n",
      "average reward per episode = 1.2\n",
      "Training on 217/221 positive/total out of 691 1-step experiences with actions distribution [81 70 57 13]\n",
      "average reward per episode = 0.4\n",
      "Training on 212/214 positive/total out of 572 1-step experiences with actions distribution [100  78  21  15]\n",
      "average reward per episode = 0.8\n",
      "Training on 218/218 positive/total out of 610 1-step experiences with actions distribution [76 78 43 21]\n",
      "average reward per episode = 1.0\n",
      "Training on 217/220 positive/total out of 638 1-step experiences with actions distribution [79 79 46 16]\n",
      "average reward per episode = 1.5\n",
      "Training on 220/225 positive/total out of 670 1-step experiences with actions distribution [ 65 105  42  13]\n",
      "average reward per episode = 1.5\n",
      "Training on 222/225 positive/total out of 648 1-step experiences with actions distribution [105  49  55  16]\n",
      "average reward per episode = 1.0\n",
      "Training on 215/220 positive/total out of 617 1-step experiences with actions distribution [ 6 89 76 49]\n",
      "average reward per episode = 2.0\n",
      "Training on 226/230 positive/total out of 785 1-step experiences with actions distribution [  2  70 144  14]\n",
      "average reward per episode = 3.4\n",
      "Training on 242/244 positive/total out of 889 1-step experiences with actions distribution [35 95 94 20]\n",
      "average reward per episode = 2.4\n",
      "Training on 233/234 positive/total out of 866 1-step experiences with actions distribution [  6  59 148  21]\n",
      "average reward per episode = 2.8\n",
      "Training on 236/237 positive/total out of 872 1-step experiences with actions distribution [ 14  85 130   8]\n",
      "average reward per episode = 3.5\n",
      "Training on 244/245 positive/total out of 906 1-step experiences with actions distribution [  7  95 124  19]\n",
      "average reward per episode = 4.2\n",
      "Training on 251/251 positive/total out of 980 1-step experiences with actions distribution [  7  98 129  17]\n",
      "average reward per episode = 3.8\n",
      "Training on 247/248 positive/total out of 955 1-step experiences with actions distribution [  6 123 105  14]\n",
      "average reward per episode = 4.2\n",
      "Training on 251/252 positive/total out of 973 1-step experiences with actions distribution [  8 119 113  12]\n",
      "average reward per episode = 4.5\n",
      "Training on 252/255 positive/total out of 1063 1-step experiences with actions distribution [  7 112 130   6]\n",
      "average reward per episode = 4.8\n",
      "Training on 255/257 positive/total out of 983 1-step experiences with actions distribution [ 13 132 101  11]\n",
      "average reward per episode = 4.1\n",
      "Training on 251/251 positive/total out of 1024 1-step experiences with actions distribution [  4 134 108   5]\n",
      "average reward per episode = 4.4\n",
      "Training on 251/254 positive/total out of 1018 1-step experiences with actions distribution [  7 107 133   7]\n",
      "average reward per episode = 4.1\n",
      "Training on 251/251 positive/total out of 953 1-step experiences with actions distribution [  4 133 108   6]\n",
      "average reward per episode = 5.0\n",
      "Training on 256/260 positive/total out of 1027 1-step experiences with actions distribution [  3 129 123   5]\n",
      "average reward per episode = 3.5\n",
      "Training on 245/245 positive/total out of 879 1-step experiences with actions distribution [ 14 128  97   6]\n",
      "average reward per episode = 4.0\n",
      "Training on 247/250 positive/total out of 1002 1-step experiences with actions distribution [  6 123 116   5]\n",
      "average reward per episode = 3.6\n",
      "Training on 243/246 positive/total out of 956 1-step experiences with actions distribution [  8 122 111   5]\n",
      "average reward per episode = 5.7\n",
      "Training on 265/266 positive/total out of 1149 1-step experiences with actions distribution [ 10 125 129   2]\n",
      "average reward per episode = 3.6\n",
      "Training on 246/246 positive/total out of 910 1-step experiences with actions distribution [  8 131 102   5]\n",
      "average reward per episode = 3.6\n",
      "Training on 244/246 positive/total out of 890 1-step experiences with actions distribution [  6 137  99   4]\n",
      "average reward per episode = 4.0\n",
      "Training on 247/250 positive/total out of 962 1-step experiences with actions distribution [  5 143  97   5]\n",
      "average reward per episode = 4.6\n",
      "Training on 254/255 positive/total out of 1011 1-step experiences with actions distribution [ 13 136 101   5]\n",
      "average reward per episode = 4.4\n",
      "Training on 253/254 positive/total out of 986 1-step experiences with actions distribution [  9 143  94   8]\n",
      "average reward per episode = 2.9\n",
      "Training on 237/238 positive/total out of 785 1-step experiences with actions distribution [  7 143  81   7]\n",
      "average reward per episode = 4.2\n",
      "Training on 250/252 positive/total out of 938 1-step experiences with actions distribution [  6 132 106   8]\n",
      "average reward per episode = 4.0\n",
      "Training on 247/250 positive/total out of 968 1-step experiences with actions distribution [  8 129 107   6]\n",
      "average reward per episode = 4.3\n",
      "Training on 251/253 positive/total out of 998 1-step experiences with actions distribution [  7 136 101   9]\n",
      "average reward per episode = 5.3\n",
      "Training on 262/263 positive/total out of 1024 1-step experiences with actions distribution [ 11 141 107   4]\n",
      "average reward per episode = 3.6\n",
      "Training on 244/245 positive/total out of 874 1-step experiences with actions distribution [ 21 133  89   2]\n",
      "average reward per episode = 3.0\n",
      "Training on 238/240 positive/total out of 845 1-step experiences with actions distribution [  9 160  66   5]\n",
      "average reward per episode = 3.8\n",
      "Training on 248/248 positive/total out of 877 1-step experiences with actions distribution [ 12 135  93   8]\n",
      "average reward per episode = 3.6\n",
      "Training on 245/246 positive/total out of 867 1-step experiences with actions distribution [ 29 139  73   5]\n",
      "average reward per episode = 3.2\n",
      "Training on 238/242 positive/total out of 860 1-step experiences with actions distribution [ 12 137  91   2]\n",
      "average reward per episode = 4.3\n",
      "Training on 252/253 positive/total out of 979 1-step experiences with actions distribution [ 14 131 103   5]\n",
      "average reward per episode = 3.2\n",
      "Training on 240/242 positive/total out of 830 1-step experiences with actions distribution [ 15 149  71   7]\n",
      "average reward per episode = 2.3\n",
      "Training on 229/233 positive/total out of 760 1-step experiences with actions distribution [ 48 110  68   7]\n",
      "average reward per episode = 3.3\n",
      "Training on 242/243 positive/total out of 842 1-step experiences with actions distribution [ 26 120  88   9]\n",
      "average reward per episode = 3.9\n",
      "Training on 248/249 positive/total out of 947 1-step experiences with actions distribution [ 19 128  98   4]\n",
      "average reward per episode = 4.8\n",
      "Training on 256/258 positive/total out of 985 1-step experiences with actions distribution [  3 144 107   4]\n",
      "average reward per episode = 3.6\n",
      "Training on 243/245 positive/total out of 852 1-step experiences with actions distribution [  5 146  90   4]\n",
      "average reward per episode = 3.6\n",
      "Training on 243/246 positive/total out of 896 1-step experiences with actions distribution [ 20 123  96   7]\n",
      "average reward per episode = 3.1\n",
      "Training on 237/240 positive/total out of 872 1-step experiences with actions distribution [ 14 129  91   6]\n",
      "average reward per episode = 3.4\n",
      "Training on 244/244 positive/total out of 839 1-step experiences with actions distribution [  7 149  79   9]\n",
      "average reward per episode = 5.1\n",
      "Training on 257/261 positive/total out of 1017 1-step experiences with actions distribution [  9 154  88  10]\n",
      "average reward per episode = 3.4\n",
      "Training on 241/244 positive/total out of 870 1-step experiences with actions distribution [ 15 155  72   2]\n",
      "average reward per episode = 4.3\n",
      "Training on 252/253 positive/total out of 1004 1-step experiences with actions distribution [ 12 154  82   5]\n",
      "average reward per episode = 2.7\n",
      "Training on 234/236 positive/total out of 810 1-step experiences with actions distribution [  9 139  81   7]\n",
      "average reward per episode = 3.3\n",
      "Training on 242/243 positive/total out of 859 1-step experiences with actions distribution [ 13 140  86   4]\n",
      "average reward per episode = 2.7\n",
      "Training on 236/237 positive/total out of 791 1-step experiences with actions distribution [  8 142  81   6]\n",
      "average reward per episode = 4.8\n",
      "Training on 256/258 positive/total out of 1021 1-step experiences with actions distribution [  9 141 104   4]\n",
      "average reward per episode = 2.0\n",
      "Training on 226/230 positive/total out of 638 1-step experiences with actions distribution [ 13 144  69   4]\n",
      "average reward per episode = 3.5\n",
      "Training on 244/245 positive/total out of 866 1-step experiences with actions distribution [ 11 141  86   7]\n",
      "average reward per episode = 2.7\n",
      "Training on 235/237 positive/total out of 763 1-step experiences with actions distribution [ 16 156  63   2]\n",
      "average reward per episode = 3.7\n",
      "Training on 244/247 positive/total out of 836 1-step experiences with actions distribution [ 15 169  56   7]\n",
      "average reward per episode = 2.4\n",
      "Training on 232/234 positive/total out of 784 1-step experiences with actions distribution [ 17 137  68  12]\n",
      "average reward per episode = 3.8\n",
      "Training on 248/248 positive/total out of 904 1-step experiences with actions distribution [ 11 122 108   7]\n",
      "average reward per episode = 3.6\n",
      "Training on 245/246 positive/total out of 839 1-step experiences with actions distribution [  5 147  92   2]\n",
      "average reward per episode = 2.7\n",
      "Training on 232/237 positive/total out of 775 1-step experiences with actions distribution [  5 141  86   5]\n",
      "average reward per episode = 4.2\n",
      "Training on 252/252 positive/total out of 992 1-step experiences with actions distribution [ 11 138  95   8]\n",
      "average reward per episode = 4.3\n",
      "Training on 252/253 positive/total out of 1001 1-step experiences with actions distribution [  7 141 102   3]\n",
      "average reward per episode = 4.2\n",
      "Training on 251/252 positive/total out of 913 1-step experiences with actions distribution [  8 139  95  10]\n",
      "average reward per episode = 4.4\n",
      "Training on 253/254 positive/total out of 977 1-step experiences with actions distribution [  5 147  96   6]\n",
      "average reward per episode = 4.5\n",
      "Training on 254/255 positive/total out of 1029 1-step experiences with actions distribution [  9 131 110   5]\n",
      "average reward per episode = 2.3\n",
      "Training on 228/233 positive/total out of 719 1-step experiences with actions distribution [  9 143  74   7]\n",
      "average reward per episode = 3.6\n",
      "Training on 245/246 positive/total out of 905 1-step experiences with actions distribution [  7 150  82   7]\n",
      "average reward per episode = 1.8\n",
      "Training on 225/226 positive/total out of 660 1-step experiences with actions distribution [  8 151  60   7]\n",
      "average reward per episode = 5.1\n",
      "Training on 258/261 positive/total out of 1049 1-step experiences with actions distribution [ 18 139  94  10]\n",
      "average reward per episode = 3.4\n",
      "Training on 243/244 positive/total out of 876 1-step experiences with actions distribution [  3 137  99   5]\n",
      "average reward per episode = 5.1\n",
      "Training on 260/261 positive/total out of 1090 1-step experiences with actions distribution [  4 129 117  11]\n",
      "average reward per episode = 3.9\n",
      "Training on 249/249 positive/total out of 945 1-step experiences with actions distribution [  4 103 135   7]\n",
      "average reward per episode = 1.4\n",
      "Training on 221/224 positive/total out of 603 1-step experiences with actions distribution [ 11 139  65   9]\n",
      "average reward per episode = 3.4\n",
      "Training on 242/244 positive/total out of 854 1-step experiences with actions distribution [  9 162  68   5]\n",
      "average reward per episode = 3.3\n",
      "Training on 242/243 positive/total out of 858 1-step experiences with actions distribution [ 12 130  93   8]\n",
      "average reward per episode = 3.6\n",
      "Training on 245/246 positive/total out of 891 1-step experiences with actions distribution [ 12 141  87   6]\n",
      "average reward per episode = 3.0\n",
      "Training on 238/240 positive/total out of 798 1-step experiences with actions distribution [  9 143  79   9]\n",
      "average reward per episode = 4.0\n",
      "Training on 246/250 positive/total out of 973 1-step experiences with actions distribution [ 10 140  96   4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 21:37:25,269] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/DoomDefendLine_q_learning/openaigym.video.0.52850.video007000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = 3.6\n",
      "Training on 245/245 positive/total out of 924 1-step experiences with actions distribution [ 20 130  87   8]\n",
      "average reward per episode = 3.6\n",
      "Training on 244/246 positive/total out of 917 1-step experiences with actions distribution [ 11 125  98  12]\n",
      "average reward per episode = 4.8\n",
      "Training on 256/258 positive/total out of 991 1-step experiences with actions distribution [  7 166  82   3]\n",
      "average reward per episode = 3.3\n",
      "Training on 242/243 positive/total out of 878 1-step experiences with actions distribution [  7 166  62   8]\n",
      "average reward per episode = 4.3\n",
      "Training on 249/253 positive/total out of 958 1-step experiences with actions distribution [  4 148  98   3]\n",
      "average reward per episode = 3.6\n",
      "Training on 241/246 positive/total out of 863 1-step experiences with actions distribution [  7 133  98   8]\n",
      "average reward per episode = 4.3\n",
      "Training on 253/253 positive/total out of 968 1-step experiences with actions distribution [  6 154  86   7]\n",
      "average reward per episode = 4.5\n",
      "Training on 253/255 positive/total out of 1010 1-step experiences with actions distribution [  4 138 104   9]\n",
      "average reward per episode = 3.2\n",
      "Training on 241/242 positive/total out of 802 1-step experiences with actions distribution [ 11 142  75  14]\n",
      "average reward per episode = 4.3\n",
      "Training on 249/253 positive/total out of 954 1-step experiences with actions distribution [  7 164  79   3]\n",
      "average reward per episode = 3.3\n",
      "Training on 241/243 positive/total out of 940 1-step experiences with actions distribution [  6 161  64  12]\n",
      "average reward per episode = 3.3\n",
      "Training on 242/243 positive/total out of 912 1-step experiences with actions distribution [  9 142  82  10]\n",
      "average reward per episode = 2.3\n",
      "Training on 229/232 positive/total out of 743 1-step experiences with actions distribution [ 18 156  49   9]\n",
      "average reward per episode = 1.9\n",
      "Training on 224/227 positive/total out of 630 1-step experiences with actions distribution [  6 161  57   3]\n",
      "average reward per episode = 3.6\n",
      "Training on 244/246 positive/total out of 851 1-step experiences with actions distribution [ 18 146  77   5]\n",
      "average reward per episode = 3.1\n",
      "Training on 239/240 positive/total out of 841 1-step experiences with actions distribution [ 11 131  92   6]\n",
      "average reward per episode = 2.9\n",
      "Training on 238/239 positive/total out of 787 1-step experiences with actions distribution [ 10 166  59   4]\n",
      "average reward per episode = 2.8\n",
      "Training on 236/238 positive/total out of 794 1-step experiences with actions distribution [ 13 158  61   6]\n",
      "average reward per episode = 2.7\n",
      "Training on 237/237 positive/total out of 775 1-step experiences with actions distribution [ 36 128  56  17]\n",
      "average reward per episode = 3.2\n",
      "Training on 240/242 positive/total out of 787 1-step experiences with actions distribution [ 10 141  87   4]\n",
      "average reward per episode = 1.7\n",
      "Training on 224/227 positive/total out of 702 1-step experiences with actions distribution [ 11 150  58   8]\n",
      "average reward per episode = 2.5\n",
      "Training on 233/234 positive/total out of 713 1-step experiences with actions distribution [  9 153  63   9]\n",
      "average reward per episode = 3.4\n",
      "Training on 241/244 positive/total out of 879 1-step experiences with actions distribution [ 15 139  76  14]\n",
      "average reward per episode = 2.1\n",
      "Training on 224/230 positive/total out of 683 1-step experiences with actions distribution [ 30 119  67  14]\n",
      "average reward per episode = 3.5\n",
      "Training on 244/245 positive/total out of 832 1-step experiences with actions distribution [ 17 138  79  11]\n",
      "average reward per episode = 3.3\n",
      "Training on 242/243 positive/total out of 862 1-step experiences with actions distribution [ 12 132  81  18]\n",
      "average reward per episode = 2.4\n",
      "Training on 233/234 positive/total out of 676 1-step experiences with actions distribution [  8 152  63  11]\n",
      "average reward per episode = 4.6\n",
      "Training on 253/256 positive/total out of 986 1-step experiences with actions distribution [ 11 150  87   8]\n",
      "average reward per episode = 2.8\n",
      "Training on 235/237 positive/total out of 782 1-step experiences with actions distribution [ 13 159  61   4]\n",
      "average reward per episode = 2.9\n",
      "Training on 237/239 positive/total out of 822 1-step experiences with actions distribution [ 11 144  78   6]\n",
      "average reward per episode = 2.5\n",
      "Training on 230/235 positive/total out of 661 1-step experiences with actions distribution [  9 154  59  13]\n",
      "average reward per episode = 1.7\n",
      "Training on 224/227 positive/total out of 691 1-step experiences with actions distribution [ 11 119  75  22]\n",
      "average reward per episode = 2.2\n",
      "Training on 230/231 positive/total out of 690 1-step experiences with actions distribution [ 14 143  66   8]\n",
      "average reward per episode = 2.7\n",
      "Training on 234/237 positive/total out of 746 1-step experiences with actions distribution [ 18 122  88   9]\n",
      "average reward per episode = 1.9\n",
      "Training on 228/229 positive/total out of 716 1-step experiences with actions distribution [ 10 116  77  26]\n",
      "average reward per episode = 1.3\n",
      "Training on 221/223 positive/total out of 642 1-step experiences with actions distribution [ 19 132  61  11]\n",
      "average reward per episode = 1.6\n",
      "Training on 224/226 positive/total out of 698 1-step experiences with actions distribution [ 6 95 93 32]\n",
      "average reward per episode = 1.6\n",
      "Training on 225/226 positive/total out of 671 1-step experiences with actions distribution [14 87 94 31]\n",
      "average reward per episode = 1.4\n",
      "Training on 223/224 positive/total out of 646 1-step experiences with actions distribution [12 87 79 46]\n",
      "average reward per episode = 2.1\n",
      "Training on 226/231 positive/total out of 704 1-step experiences with actions distribution [  8 118  70  35]\n",
      "average reward per episode = 1.2\n",
      "Training on 222/222 positive/total out of 647 1-step experiences with actions distribution [10 86 76 50]\n",
      "average reward per episode = 0.7\n",
      "Training on 215/217 positive/total out of 604 1-step experiences with actions distribution [21 68 64 64]\n",
      "average reward per episode = 1.0\n",
      "Training on 216/220 positive/total out of 615 1-step experiences with actions distribution [ 7 99 67 47]\n",
      "average reward per episode = 0.9\n",
      "Training on 214/219 positive/total out of 636 1-step experiences with actions distribution [ 7 66 80 66]\n",
      "average reward per episode = 2.5\n",
      "Training on 230/235 positive/total out of 816 1-step experiences with actions distribution [  6  96 101  32]\n",
      "average reward per episode = 0.7\n",
      "Training on 212/217 positive/total out of 618 1-step experiences with actions distribution [ 13 101  70  33]\n",
      "average reward per episode = 1.9\n",
      "Training on 226/229 positive/total out of 792 1-step experiences with actions distribution [ 6 91 74 58]\n",
      "average reward per episode = 2.5\n",
      "Training on 234/235 positive/total out of 822 1-step experiences with actions distribution [ 10 103  81  41]\n",
      "average reward per episode = 1.9\n",
      "Training on 227/229 positive/total out of 718 1-step experiences with actions distribution [ 7 87 89 46]\n",
      "average reward per episode = 0.9\n",
      "Training on 216/218 positive/total out of 612 1-step experiences with actions distribution [ 5 69 84 60]\n",
      "average reward per episode = 1.8\n",
      "Training on 226/228 positive/total out of 667 1-step experiences with actions distribution [ 4 90 83 51]\n",
      "average reward per episode = 1.4\n",
      "Training on 220/224 positive/total out of 661 1-step experiences with actions distribution [ 6 84 96 38]\n",
      "average reward per episode = 2.4\n",
      "Training on 232/234 positive/total out of 774 1-step experiences with actions distribution [ 14 112  63  45]\n",
      "average reward per episode = 0.7\n",
      "Training on 216/217 positive/total out of 600 1-step experiences with actions distribution [ 20 108  54  35]\n",
      "average reward per episode = 2.0\n",
      "Training on 228/230 positive/total out of 704 1-step experiences with actions distribution [ 11 102  77  40]\n",
      "average reward per episode = 1.3\n",
      "Training on 221/223 positive/total out of 685 1-step experiences with actions distribution [12 75 87 49]\n",
      "average reward per episode = 2.2\n",
      "Training on 228/232 positive/total out of 816 1-step experiences with actions distribution [  7 110  75  40]\n",
      "average reward per episode = 1.8\n",
      "Training on 228/228 positive/total out of 705 1-step experiences with actions distribution [18 90 83 37]\n",
      "average reward per episode = 2.3\n",
      "Training on 231/233 positive/total out of 762 1-step experiences with actions distribution [ 10 116  87  20]\n",
      "average reward per episode = 1.9\n",
      "Training on 226/229 positive/total out of 682 1-step experiences with actions distribution [ 10 110  67  42]\n",
      "average reward per episode = 2.2\n",
      "Training on 225/232 positive/total out of 699 1-step experiences with actions distribution [ 19 109  78  26]\n",
      "average reward per episode = 2.1\n",
      "Training on 231/231 positive/total out of 712 1-step experiences with actions distribution [23 99 85 24]\n",
      "average reward per episode = 1.5\n",
      "Training on 224/225 positive/total out of 660 1-step experiences with actions distribution [27 93 69 36]\n",
      "average reward per episode = 3.4\n",
      "Training on 243/244 positive/total out of 819 1-step experiences with actions distribution [ 21 127  74  22]\n",
      "average reward per episode = 1.9\n",
      "Training on 226/229 positive/total out of 727 1-step experiences with actions distribution [ 25 107  73  24]\n",
      "average reward per episode = 1.0\n",
      "Training on 215/220 positive/total out of 619 1-step experiences with actions distribution [ 36 101  61  22]\n",
      "average reward per episode = 1.2\n",
      "Training on 222/222 positive/total out of 640 1-step experiences with actions distribution [28 89 76 29]\n",
      "average reward per episode = 1.4\n",
      "Training on 218/224 positive/total out of 631 1-step experiences with actions distribution [30 76 79 39]\n",
      "average reward per episode = 1.2\n",
      "Training on 219/222 positive/total out of 618 1-step experiences with actions distribution [ 25 133  41  23]\n",
      "average reward per episode = 1.4\n",
      "Training on 221/224 positive/total out of 676 1-step experiences with actions distribution [ 18 107  63  36]\n",
      "average reward per episode = 1.1\n",
      "Training on 220/221 positive/total out of 661 1-step experiences with actions distribution [ 28 116  56  21]\n",
      "average reward per episode = 0.9\n",
      "Training on 210/219 positive/total out of 588 1-step experiences with actions distribution [ 23 118  61  17]\n",
      "average reward per episode = 1.3\n",
      "Training on 222/223 positive/total out of 664 1-step experiences with actions distribution [ 22 115  52  34]\n",
      "average reward per episode = 1.8\n",
      "Training on 225/228 positive/total out of 732 1-step experiences with actions distribution [ 16 115  67  30]\n",
      "average reward per episode = 1.3\n",
      "Training on 221/223 positive/total out of 741 1-step experiences with actions distribution [ 33 107  55  28]\n",
      "average reward per episode = 1.6\n",
      "Training on 224/226 positive/total out of 679 1-step experiences with actions distribution [ 31 106  58  31]\n",
      "average reward per episode = 1.4\n",
      "Training on 218/224 positive/total out of 706 1-step experiences with actions distribution [13 89 75 47]\n",
      "average reward per episode = 1.9\n",
      "Training on 226/229 positive/total out of 788 1-step experiences with actions distribution [15 79 73 62]\n",
      "average reward per episode = 0.9\n",
      "Training on 216/219 positive/total out of 593 1-step experiences with actions distribution [ 17 111  68  23]\n",
      "average reward per episode = 1.6\n",
      "Training on 224/226 positive/total out of 681 1-step experiences with actions distribution [  7 107  72  40]\n",
      "average reward per episode = 0.8\n",
      "Training on 214/218 positive/total out of 558 1-step experiences with actions distribution [ 12 108  57  41]\n",
      "average reward per episode = 0.8\n",
      "Training on 217/218 positive/total out of 612 1-step experiences with actions distribution [ 33 126  47  12]\n",
      "average reward per episode = 1.3\n",
      "Training on 218/223 positive/total out of 645 1-step experiences with actions distribution [23 88 57 55]\n",
      "average reward per episode = 0.9\n",
      "Training on 215/219 positive/total out of 654 1-step experiences with actions distribution [19 82 67 51]\n",
      "average reward per episode = 1.2\n",
      "Training on 222/222 positive/total out of 671 1-step experiences with actions distribution [20 87 80 35]\n",
      "average reward per episode = 2.5\n",
      "Training on 232/235 positive/total out of 740 1-step experiences with actions distribution [ 32 110  75  18]\n",
      "average reward per episode = 2.1\n",
      "Training on 229/231 positive/total out of 730 1-step experiences with actions distribution [ 22 103  73  33]\n",
      "average reward per episode = 1.6\n",
      "Training on 222/225 positive/total out of 664 1-step experiences with actions distribution [24 76 79 46]\n",
      "average reward per episode = 1.4\n",
      "Training on 224/224 positive/total out of 659 1-step experiences with actions distribution [ 18 125  61  20]\n",
      "average reward per episode = 1.7\n",
      "Training on 226/227 positive/total out of 732 1-step experiences with actions distribution [ 43 123  43  18]\n",
      "average reward per episode = 1.2\n",
      "Training on 220/222 positive/total out of 634 1-step experiences with actions distribution [20 88 64 50]\n",
      "average reward per episode = 2.2\n",
      "Training on 230/232 positive/total out of 750 1-step experiences with actions distribution [ 9 94 86 43]\n",
      "average reward per episode = 1.7\n",
      "Training on 226/227 positive/total out of 698 1-step experiences with actions distribution [12 94 90 31]\n",
      "average reward per episode = 1.8\n",
      "Training on 226/228 positive/total out of 688 1-step experiences with actions distribution [  3 107  73  45]\n",
      "average reward per episode = 2.1\n",
      "Training on 227/231 positive/total out of 731 1-step experiences with actions distribution [ 10 105  88  28]\n",
      "average reward per episode = 1.9\n",
      "Training on 225/229 positive/total out of 694 1-step experiences with actions distribution [ 11 113  81  24]\n",
      "average reward per episode = 1.4\n",
      "Training on 220/224 positive/total out of 652 1-step experiences with actions distribution [24 90 62 48]\n",
      "average reward per episode = 2.0\n",
      "Training on 228/230 positive/total out of 708 1-step experiences with actions distribution [ 23 112  69  26]\n",
      "average reward per episode = 1.8\n",
      "Training on 224/228 positive/total out of 660 1-step experiences with actions distribution [18 65 81 64]\n",
      "average reward per episode = 2.1\n",
      "Training on 230/231 positive/total out of 708 1-step experiences with actions distribution [ 11 104  70  46]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 22:14:09,238] Starting new video recorder writing to /Users/gui/Dev/rl-study/tmp/DoomDefendLine_q_learning/openaigym.video.0.52850.video008000.mp4\n",
      "[2017-03-04 22:27:49,702] Finished writing results. You can upload them to the scoreboard via gym.upload('/Users/gui/Dev/rl-study/tmp/DoomDefendLine_q_learning')\n",
      "[2017-03-04 22:27:49,730] [DoomDefendCenter-v0] Uploading 9000 episodes of training data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average reward per episode = 1.469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-04 22:27:56,246] [DoomDefendCenter-v0] Uploading videos of 18 training episodes (1313267 bytes)\n",
      "[2017-03-04 22:28:09,448] [DoomDefendCenter-v0] Creating evaluation object from tmp/DoomDefendLine_q_learning with learning curve and training video\n",
      "[2017-03-04 22:28:09,966] \n",
      "****************************************************\n",
      "You successfully uploaded your evaluation on DoomDefendCenter-v0 to\n",
      "OpenAI Gym! You can find it at:\n",
      "\n",
      "    https://gym.openai.com/evaluations/eval_eiiH0J8yS1yx4NHrJt0wqQ\n",
      "\n",
      "****************************************************\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "class EpsilonGreedyQAgent(object):\n",
    "    def __init__(self, model, epsilon=.1):\n",
    "        self.model = model\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def act(self, observation, reward, done):\n",
    "        if random.uniform(0, 1) <= self.epsilon:\n",
    "            return random.choice([NOOP, SHOOT, LEFT, RIGHT])\n",
    "        else:\n",
    "            return self.model.predict(observation[np.newaxis])[0].argmax()\n",
    "\n",
    "\n",
    "N_BATCHES = 400\n",
    "N_BATCHED_EPISODES = 10\n",
    "UPDATE_TARGET_EVERY_N_BACTHES = 2\n",
    "MINI_BATCH_SIZE = 32\n",
    "REWARD_CLIP = 5\n",
    "ONLY_N_MISSES = 200\n",
    "\n",
    "env = create_env()\n",
    "env = wrappers.Monitor(env, directory='tmp/DoomDefendLine_q_learning', force=True, mode='training')\n",
    "\n",
    "for _ in range(N_BATCHES):\n",
    "    for _ in range(UPDATE_TARGET_EVERY_N_BACTHES):\n",
    "        sares = episode_sares(env, EpsilonGreedyQAgent(acting_model, epsilon=.1), N_BATCHED_EPISODES)\n",
    "        prev_frames, target_action_rewards = sares_to_input_targets(target_model, sares, reward_clip=REWARD_CLIP, only_n_misses=ONLY_N_MISSES)\n",
    "        acting_model.fit(x=prev_frames, y=target_action_rewards, batch_size=MINI_BATCH_SIZE, nb_epoch=1, verbose=0)\n",
    "    \n",
    "    target_model = copy_model(acting_model)\n",
    "\n",
    "\n",
    "# final greedy episodes\n",
    "sares = episode_sares(env, EpsilonGreedyQAgent(acting_model, epsilon=0), episode_count=1000)\n",
    "\n",
    "env.close()\n",
    "gym.upload('tmp/DoomDefendLine_q_learning', api_key='sk_bNZUvCfkTfabQCoKoKbjFA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
