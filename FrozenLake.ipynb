{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frozen Lake\n",
    "\n",
    " * https://gym.openai.com/envs/FrozenLake-v0\n",
    " * Inspired by https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0#.91x58km60\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-03-02 09:28:46,429] Making new env: FrozenLake-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "0 0.0 False {'prob': 0.3333333333333333}\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "1 0.0 False {'prob': 0.3333333333333333}\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "5 0.0 True {'prob': 0.3333333333333333}\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "5 0 True {'prob': 1.0}\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ipykernel.iostream.OutStream at 0x108e14240>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# slipperly lake, so actions not deterministic!\n",
    "env = gym.make('FrozenLake-v0')\n",
    "\n",
    "env.render()\n",
    "\n",
    "next_s, r, is_end, inf = env.step(action=0)\n",
    "print(next_s, r, is_end, inf)\n",
    "env.render()\n",
    "\n",
    "next_s, r, is_end, inf = env.step(action=1)\n",
    "print(next_s, r, is_end, inf)\n",
    "env.render()\n",
    "\n",
    "next_s, r, is_end, inf = env.step(action=2)\n",
    "print(next_s, r, is_end, inf)\n",
    "env.render()\n",
    "\n",
    "next_s, r, is_end, inf = env.step(action=3)\n",
    "print(next_s, r, is_end, inf)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabular Q learning\n",
    "\n",
    "See https://webdocs.cs.ualberta.ca/~sutton/book/ebook/node65.html\n",
    "\n",
    "![alt text](https://webdocs.cs.ualberta.ca/~sutton/book/ebook/numeqtmp31.png \"Q learning update rule\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "def one_step_update(Q, s, a, r, next_s, alpha, gamma):\n",
    "    Q[s, a] += alpha * (r + gamma * Q[next_s, :].max() - Q[s, a])\n",
    "    return Q\n",
    "\n",
    "def eps_greedy_action(Q, s, epsilon):\n",
    "    return random.choice(np.arange(Q.shape[1])) if (random.uniform(0, 1) <= epsilon) else Q[s, :].argmax()\n",
    "\n",
    "\n",
    "class QLearning:\n",
    "    def __init__(self, Q, epsilon, alpha=0.1, gamma=.99):\n",
    "        self.Q = Q\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.n_episodes = 0\n",
    "        \n",
    "    def take_step(self, env, s):\n",
    "        # decayed and first\n",
    "        epsilon = (self.epsilon / (1 + self.n_episodes)) if (self.n_episodes <= 5000) else 0\n",
    "        a = eps_greedy_action(self.Q, s, epsilon)\n",
    "        next_s, r, is_end, _ = env.step(a)\n",
    "        self.Q = one_step_update(self.Q, s, a, r, next_s, self.alpha, self.gamma)\n",
    "        \n",
    "        if is_end:\n",
    "            self.n_episodes += 1\n",
    "\n",
    "        return next_s, r, is_end\n",
    "    \n",
    "    @classmethod\n",
    "    def create(cls, epsilon=10, alpha=0.1, gamma=.99):\n",
    "        Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "        return cls(Q, epsilon, alpha, gamma)\n",
    "\n",
    "def play_new_episode(env, act_func):\n",
    "    s = env.reset()\n",
    "    is_end = False\n",
    "    i = 0\n",
    "    while (not is_end) and i < 100:\n",
    "        i += 1\n",
    "        s, r, is_end = act_func(env, s)\n",
    "        yield r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\n",
      "SFF\u001b[41mF\u001b[0m\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Right)\n",
      "\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Down)\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Right)\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Right)\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Up)\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def capture_new_episode(env, act_func):\n",
    "    rewards, debugs = zip(*[(r, env.render(mode='ansi')) for r in play_new_episode(env, act_func)])\n",
    "    return sum(rewards), '\\n'.join(d.getvalue() for d in debugs)\n",
    "\n",
    "total_reward, debug = capture_new_episode(env, QLearning.create().take_step)\n",
    "\n",
    "print(debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAO4AAADuCAYAAAA+7jsiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x108f19860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "n_episodes = 500\n",
    "qlearn = QLearning.create(epsilon=100)\n",
    "\n",
    "simulated_episodes = (play_new_episode(env, qlearn.take_step) for _ in range(n_episodes))\n",
    "average_reward = sum(sum(rs) for rs in simulated_episodes) / n_episodes\n",
    "print(average_reward)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def showQ(Q):\n",
    "    actions_grid = Q.argmax(1).reshape((4, 4))\n",
    "    values_grid = Q.max(1).reshape((4, 4))\n",
    "\n",
    "    actions_xys = np.array([\n",
    "        [-1, 0], #L\n",
    "        [0, -1], #D\n",
    "        [1, 0], #R\n",
    "        [0, 1], #U\n",
    "    ])\n",
    "\n",
    "    actions_x_grid = actions_xys[actions_grid, 0]\n",
    "    actions_y_grid = actions_xys[actions_grid, 1]\n",
    "\n",
    "    plt.imshow(values_grid, interpolation='nearest')\n",
    "\n",
    "    plt.quiver(actions_x_grid, actions_y_grid)\n",
    "\n",
    "    plt.xticks([]); plt.yticks([]);\n",
    "\n",
    "showQ(qlearn.Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 100\n",
    "\n",
    "simulated_episodes = (capture_new_episode(env, QLearning(qlearn.Q, epsilon=0).take_step) for _ in range(n_episodes))\n",
    "\n",
    "average_reward = sum(total_reward for total_reward, _ in simulated_episodes) / n_episodes\n",
    "print(average_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "\u001b[41mF\u001b[0mFFH\n",
      "HFFG\n",
      "  (Left)\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "F\u001b[41mF\u001b[0mFH\n",
      "HFFG\n",
      "  (Up)\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "  (Down)\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "  (Left)\n",
      "\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "  (Down)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "simulated_episodes = (capture_new_episode(env, QLearning(qlearn.Q, epsilon=0.).take_step) for _ in range(n_episodes))\n",
    "first_winning_debug = next(debug for total_reward, debug in simulated_episodes \n",
    "                           if (total_reward > 0 and len(debug) < 300))\n",
    "print(first_winning_debug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-19 00:46:50,628] Making new env: FrozenLake-v0\n",
      "[2017-02-19 00:46:50,636] DEPRECATION WARNING: env.spec.timestep_limit has been deprecated. Replace your call to `env.spec.timestep_limit` with `env.spec.tags.get('wrapper_config.TimeLimit.max_episode_steps')`. This change was made 12/28/2016 and is included in version 0.7.0\n",
      "[2017-02-19 00:46:50,638] Clearing 26 monitor files from previous run (because force=True was provided)\n",
      "[2017-02-19 00:46:50,641] Starting new video recorder writing to /tmp/FrozenLake-v0/openaigym.video.7.47835.video000000.json\n",
      "[2017-02-19 00:46:50,644] Starting new video recorder writing to /tmp/FrozenLake-v0/openaigym.video.7.47835.video000001.json\n",
      "[2017-02-19 00:46:50,648] Starting new video recorder writing to /tmp/FrozenLake-v0/openaigym.video.7.47835.video000008.json\n",
      "[2017-02-19 00:46:50,653] Starting new video recorder writing to /tmp/FrozenLake-v0/openaigym.video.7.47835.video000027.json\n",
      "[2017-02-19 00:46:50,666] Starting new video recorder writing to /tmp/FrozenLake-v0/openaigym.video.7.47835.video000064.json\n",
      "[2017-02-19 00:46:50,680] Starting new video recorder writing to /tmp/FrozenLake-v0/openaigym.video.7.47835.video000125.json\n",
      "[2017-02-19 00:46:50,701] Starting new video recorder writing to /tmp/FrozenLake-v0/openaigym.video.7.47835.video000216.json\n",
      "[2017-02-19 00:46:50,732] Starting new video recorder writing to /tmp/FrozenLake-v0/openaigym.video.7.47835.video000343.json\n",
      "[2017-02-19 00:46:50,783] Starting new video recorder writing to /tmp/FrozenLake-v0/openaigym.video.7.47835.video000512.json\n",
      "[2017-02-19 00:46:50,884] Starting new video recorder writing to /tmp/FrozenLake-v0/openaigym.video.7.47835.video000729.json\n",
      "[2017-02-19 00:46:51,043] Starting new video recorder writing to /tmp/FrozenLake-v0/openaigym.video.7.47835.video001000.json\n",
      "[2017-02-19 00:46:51,793] Starting new video recorder writing to /tmp/FrozenLake-v0/openaigym.video.7.47835.video002000.json\n",
      "[2017-02-19 00:46:52,640] Starting new video recorder writing to /tmp/FrozenLake-v0/openaigym.video.7.47835.video003000.json\n",
      "[2017-02-19 00:46:53,444] Starting new video recorder writing to /tmp/FrozenLake-v0/openaigym.video.7.47835.video004000.json\n",
      "[2017-02-19 00:46:54,319] Starting new video recorder writing to /tmp/FrozenLake-v0/openaigym.video.7.47835.video005000.json\n",
      "[2017-02-19 00:46:55,253] Starting new video recorder writing to /tmp/FrozenLake-v0/openaigym.video.7.47835.video006000.json\n",
      "[2017-02-19 00:46:56,296] Starting new video recorder writing to /tmp/FrozenLake-v0/openaigym.video.7.47835.video007000.json\n",
      "[2017-02-19 00:46:57,306] Starting new video recorder writing to /tmp/FrozenLake-v0/openaigym.video.7.47835.video008000.json\n",
      "[2017-02-19 00:46:58,304] Starting new video recorder writing to /tmp/FrozenLake-v0/openaigym.video.7.47835.video009000.json\n",
      "[2017-02-19 00:46:59,449] Finished writing results. You can upload them to the scoreboard via gym.upload('/tmp/FrozenLake-v0')\n"
     ]
    }
   ],
   "source": [
    "qlearn = QLearning.create(epsilon=100)\n",
    "\n",
    "import gym\n",
    "from gym import wrappers\n",
    "\n",
    "env = wrappers.Monitor(gym.make('FrozenLake-v0'), '/tmp/FrozenLake-v0', force=True)\n",
    "\n",
    "for i_episode in range(10000):\n",
    "    r = sum(play_new_episode(env, qlearn.take_step))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-02-19 00:47:00,333] [FrozenLake-v0] Uploading 10000 episodes of training data\n",
      "[2017-02-19 00:47:12,078] [FrozenLake-v0] Uploading videos of 19 training episodes (2124 bytes)\n",
      "[2017-02-19 00:47:12,595] [FrozenLake-v0] Creating evaluation object from /tmp/FrozenLake-v0 with learning curve and training video\n",
      "[2017-02-19 00:47:12,926] \n",
      "****************************************************\n",
      "You successfully uploaded your evaluation on FrozenLake-v0 to\n",
      "OpenAI Gym! You can find it at:\n",
      "\n",
      "    https://gym.openai.com/evaluations/eval_sYk0FCES1iR4JPQJxfpmA\n",
      "\n",
      "****************************************************\n"
     ]
    }
   ],
   "source": [
    "gym.upload('/tmp/FrozenLake-v0', api_key='sk_bNZUvCfkTfabQCoKoKbjFA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
